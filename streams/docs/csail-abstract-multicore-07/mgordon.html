<html><head><!-- Which section should this abstract be included in?   	1.) Systems   	-->

</head><body><div align="center">
<h3>StreamIt: A Common Machine Language for Parallel Architectures</h3>
   
<h4>Michael I. Gordon, William Thies, Qiuyuan J. Li, Phil Sung, David Zhang, 
Weng Fai Wong, &amp; Saman Amarasinghe</h4>
</div>

<h5>Introduction</h5>

<p>To continue performance scalability and to manage design
complexity, processor architectures are now relying on coarse-grained
parallelism.  This form of parallelism is achieved by the replication
of simple processing elements.  However, easing the programming burden
for these explicitly parallel machines is still a largely unsolved
problem. We are developing high-level programming idioms, compiler
technologies, and runtime systems for parallel machines.  We seek to
enable high programmer productivity coupled with high application
performance, portability, and scalability.  The algorithms we develop
will be applicable across the spectrum of parallel architectures, from
general purpose multicores to emerging flexible graphics hardware
(GPUs).</p>

<p>Unfortunately, parallel programming is notoriously
difficult. Application developers have long hoped for new high-level
programming models, with associated compiler and runtime technologies,
in order to produce efficient, portable, and scalable code for
coarse-grained parallel architectures. The stream programming paradigm
offers a promising high-level approach for application development
targeting parallel machines.  Our research focuses on language,
compiler, and runtime technologies for the StreamIt programming
language [5], a streaming language we are developing in the Compilers
Group at MIT (COMMIT).</p>

<h5>Background</h5>

<p>The common properties across parallel machines are multiple flows
of control and multiple local memories. StreamIt directly exposes
these common properties. In the StreamIt language, a program is
represented as a set of autonomous filters that communicate through
FIFO data channels.  During program execution, filters fire repeatedly
in a periodic schedule.  As each filter has a separate program counter
and an independent address space, all dependences between filters are
made explicit by the communication channels.  Compilers or runtime
systems can leverage this dependence information to orchestrate
parallel execution.<p>

<p>
<center>
<img src="mgordon1.jpg" alt="3 types of parallelism: example." width=800>
</center>
</p>
<p>
<center>
<b>Figure 1</b>: Example stream graph with parallelism axes. Filters
are denoted by green boxes and data-reorganization nodes are denoted
by blue ovals. Pipeline parallelism is present between filters in the
horizontal dimension, task parallelism in the vertical dimension, and
data parallelism in the perceived depth dimension.  Data parallelism
is exploited only in Filter B.  Filter B is stateless and can be
data-parallelized by duplicating the filter (duplicates denoted by the
lighter boxes). 
</center>
</p>

<p>StreamIt directly exposes three types of coarse-grained
parallelism: task, data, and pipeline. Conceptually, each type of
parallelism can be expressed on an independent axis of a stream
graph (see Figure 1).  Task parallelism refers to pairs of filters
that are on different parallel branches of the original stream graph,
as written by the programmer (Filters C, D, and E in Figure 1). That
is, the output of each filter never reaches the input of the other. In
stream programs, task parallelism reflects logical parallelism in the
underlying algorithm. Data parallelism refers to any filter that has
no dependences between one execution and the next. Such "stateless"
filters offer unlimited data parallelism, as different instances of
the filter can be spread across any number of computation units
(Filter B in Figure 1).  StreamIt, unlike many other streaming
languages, naturally supports pipeline parallelism [1].  Pipeline
parallelism applies to chains of producers and consumers that are
directly connected in the stream graph (e.g., Filters A and B in
Figure 1). Each type of parallelism has advantages and
disadvantages. For example, exploiting data parallelism is
straightforward and produces a load-balanced mapping. However, by
itself, data parallelism requires global communication and
synchronization. A stream-aware system can determine the correct
granularity and combination of parallelism for high efficiency.<p>

<h5>Compiler Techniques</h5> 

<p>In our most recent work, we present automatic compiler techniques
for bridging the gap between the granularity of a stream program and
the underlying granularity of the target architecture [2].  We present
a unified static approach that leverages the correct combination of
task, data, and pipeline parallelism.  To reduce the communication
overhead typical of data-parallelism, we first increase the
granularity of the stream graph by fusing filters so long as the
result is data-parallel.  To further reduce communication costs, we
data-parallelize the application while maintaining task parallelism
being careful to load-balance each data-parallel stage.  To
parallelize filters with loop-carried dependencies, we employ
software-pipelining techniques to execute filters from different
iterations in parallel, increasing scheduling flexibility.</p>

<p>Targeting the 16-core Raw Architecture [4], we achieve a mean
speedup of 11.2x over a single core baseline; 7 out of 12 benchmarks
speedup by over 12x. In comparison, a traditional data-parallel
approach that does not attempt to match the granularity achieves a
speedup of only 1.4x over the sequential baseline. Our research
suggests that two specific features of StreamIt are a good match for
parallel architectures: exposing producer-consumer relationships and
exposing the outer loop around the entire stream graph.</p>

<h5>Current and Future Work</h5>

<p>We wish to demonstrate that, for the class of streaming
applications, StreamIt programs are portable, scalable and efficient
across parallel architectures.  Toward this end, we are in the process
of developing backends and runtime systems for emerging industry and
research parallel architectures. Each separate architecture tests the
portability of the language, generality of our current mapping
algorithms, and reveals further opportunities for research.</p>

<p>For example, we are currently working on a compiler backend with an
associated runtime system for the Cell processor [3]. This development
effort is interesting for multiple reasons. For high-performance, Cell
requires the exploitation of fine-grained data-parallelism in the form
of SIMD instructions.  We have developed an infrastructure that will
map a "copy" of a suitable stateless filter onto a SIMD
lane. Furthermore, Cell is an example of a heterogeneous parallel
architecture.  We must extend our mapping algorithms to make informed
mapping decisions given the differing capabilities of the processing
cores. Finally, we are developing a system for Cell where we can vary
the mapping and scheduling of the stream program between static
(compiler) and dynamic (runtime) control.  We wish to investigate
points on the continuum between static and dynamic mappings, e.g.,
whether it is useful to compute the mapping and schedule at
compile time.</p>

<p>Data-parallelism often requires global communication and data
reorganization.  Another question we seek to investigate is whether
data parallelism scales indefinitely.  We wish to determine the points
at which data-parallelism ceases to scale as we vary the architectural
parameters (e.g., number and capability of processing elements,
network parameters, memory bandwidth)?  When data-parallelism no
longer scales, what architectural, language, and compiler techniques
will be necessary to continue performance scaling?</p>

<p>Finally, we plan on demonstrating that StreamIt can effectively target
GPUs. GPUs have massive data parallelism and are specialized for
rendering data-types and operations. Coarse-grained data parallelism
can be discovered in StreamIt and should be relatively easy to
exploit. A more difficult task is to generate efficient code for the
processing elements themselves, code that exploits the dense
instruction encoding of the processing elements. As GPUs become more
programmable and more tightly coupled with the CPU, one can think of
the coupling as a heterogeneous parallel processor where segments of
an application are offloaded to the GPU for processing. With StreamIt,
one can write an application in a single high-level programming
language and let the compiler decide the partitioning between CPU and
GPU.  We plan to extend our mapping algorithms for this problem.</p> 

<h5>References:</h5>

<p>[1] Michael Gordon, William Thies, Michal Karczmarek, Jasper Lin,
Ali S. Meli, Andrew A. Lamb, Chris Leger, Jeremy Wong, Henry Hoffmann,
David Maze, and Saman Amarasinghe. A Stream Compiler for
Communication-Exposed Architectures. In <em>ASPLOS</em>, 2002.</p>

<p>[2] Michael Gordon, William Thies, and Saman
Amarasinghe. Exploiting Coarse-Grained Task, Data, and Pipeline
Parallelism in Stream Programs. In <em>ASPLOS</em>, 2006.</p>

<p>[3] H. Peter Hofstee. Power Efficient Processor Architecture and
The Cell Processor. In <em>HPCA</em>, 2005.

<p>[4] Michael Taylor, Walter Lee, Jason Miller, David Wentzlaff,
et. al. Evaluation of the Raw Microprocessor: An Exposed-Wire-Delay
Architecture for ILP and Streams. In <em>ISCA</em>, 2004.</p>

<p>[5] William Thies, Michal Karczmarek, and Saman
Amarasinghe. StreamIt: A Language for Streaming Applications. In
<em>Proc. of the International Conference of Compiler
Construction</em>, 2002.</p>


</body></html>
