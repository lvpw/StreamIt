\section{Streaming Virtual Machine API}

The Streaming Virtual Machine API is a set of data structures and functions used to express the streaming portion of an application mapped to a specific VM by the high-level compiler. It includes streams and blocks which specify how data is mapped to memory, kernels which specify how computation and data-movement functions are mapped to processors, and control functions which are used by threaded code to control kernel execution. 

We describe the API for streams and blocks in Section~\ref{sec:memory}, the API for kernels in Section~\ref{sec:kernel}, and the API for control in Section~\ref{sec:control}.

\subsection{Memory}
\label{sec:memory}

The SVM API specifies how data is mapped to memory using stream and block declarations in the control code.  Streams are used for sequential production and consumption of elements, while blocks provide random access to a fixed set of elements.  Streams are mapped to random-access memory or hardware FIFOs. Blocks are only mapped to random-access memory. We describe streams in Section~\ref{sec:streams} and blocks in Section~\ref{sec:blocks}.

\subsubsection{Streams}
\label{sec:streams}

A {\it stream} is a data type that represents a sequence of elements of a given type. 

Streams come in two basic flavors: ordered and unordered. Ordered streams have the semantics of a FIFO queue: popping elements from the stream will yield the elements in the same order the elements were pushed. Unordered streams relax these semantics: popping elements from the stream may not yield the elements in the same order the elements were pushed. Unordered streams are used when data can be processed in any order, {\it e.g.}, for network packets. \footnote{Unordered streams offer an opportunity for optimization to the low-level compiler, but can be implemented as ordered streams. }

From the perspective of a given kernel, a stream is used for input (the kernel pops elements from the stream), output (the kernel pushes elements onto the stream), or both.

All six combinations of these attributes represent a valid interface
from a kernel to a stream, and we define a separate stream type for
each (see Figure~\ref{fig:declgrid}).  For the sake of clarifying the
specification and the implementation of the runtime system, this
proposal uses inheritance to describe these types\footnote{However,
nothing in the specification relies on the inheritance relationships
for correctness, so these could be expanded into six stand-alone types
if desired.}(see Figure~\ref{fig:inherit}).  Note that from the
control code, all streams will be instantiated using the {\tt Stream}
classes at the bottom of the hierarchy.  The other classes are strictly
for the purpose of allowing kernels to specify their relationship to a
given stream.

%% \begin{figure}[t]
%% \begin{center}
%% \psfig{figure=inherit.eps,width=6in}
%% \end{center}
%% \vspace{-12pt}
%% \caption{Class hierarchy diagram for streams and blocks.\protect\label{fig:inherit}}
%% \end{figure}

\sss{BaseStorage}

The inheritance tree for streams starts with {\tt BaseStorage}, which
represents part of a random access memory or a hardware FIFO.  {\small
\begin{verbatim}
  template<class T>
  class BaseStorage {
  public:
    // defines a portion of random-access memory node <memLocation>
    // allocated at <address> that can hold at most <capacity> elements
    BaseStorage(VM_NODE_TYPE_MEM_RANDOM memLocation, int address, int capacity);

    // defines a fifo node <memLocation>
    BaseStorage(VM_NODE_TYPE_MEM_FIFO memLocation);
  }
\end{verbatim}}

Each {\tt BaseStorage} object is based on a template type {\tt T} to
allow type-safe access to its data elements.  This type must have a
fixed size.  Variable sized records can be supported at the
language-level and compiled into the fixed size scheme using a data
encoding.

\input{declgrid}
\clearpage

\sss{BaseStream}

The base type of all streams is {\tt BaseStream}:
{\small
\begin{verbatim}
  template<class T>
  class BaseStream : public BaseStorage {
  public:
    // make a stream mapped to memory node <memLocation>, 
    // allocated at <address> that can hold at most <capacity> elements,
    // with <initLength> items already in place.  
    // The last two arguments are optional hints to he low-level compiler:  
    // <aliased> is true if some other stream might overlap this in memory,
    // <wraps> is true if more than <capacity> elements might be pushed to 
    // the stream during its lifetime.
    BaseStream(VM_NODE_TYPE_MEM_RANDOM memLocation, int address, int capacity, 
               int initLength=0, boolean aliased=true, boolean wraps=true);

    BaseStream(VM_NODE_TYPE_MEM_FIFO memLocation);
  }
\end{verbatim}}

Each stream mapped to random access memory makes an important guarantee on its
data layout: it is implemented as a circular buffer of length {\tt
capacity} that starts at the specified {\tt address} and wraps around
to the beginning when more than {\tt capacity} elements have been
pushed.  This guarantee is important for allowing reuse of memory
space between streams, as well as for transfering data between
threaded code and streaming code.  However, there is no contract on
data layout at any time if the {\tt aliased} parameter is false in the
{\tt BaseStream}'s constructor, as this represents a case where the
high-level compiler guarantees that no operation will access the
locations allocated to the stream.  This gives the low-level compiler
the freedom to implement streams with an architecture-specific
representation if their contents are never aliased.

The constructor's {\tt initLength} parameter indicates how many items
are already in place at {\tt address} that should be pushed onto the
stream initially. Thus, the stream should increment its own {\tt
end} by {\tt initLength} during construction.

\newpage
\sss{Interface to Streams}

We now turn our attention to the other stream functions, which are
implemented in derived classes (see Figure~\ref{fig:declgrid}).

\ssss{constructors} Each stream provides two constructors, which have
the same signatures as those in {\tt BaseStream}.

\ssss{push} For output streams, the {\tt push} method enqueues a value
onto the end of the stream.  If the stream cannot hold any more items 
then push stalls until there is space available.

\ssss{pop} For input streams, the {\tt pop} method dequeues a value
from the front of the stream.  If there are no items in the stream,
then pop stalls until an item is available. 

\ssss{peek} For ordered input streams, the {\tt peek} method returns
the element at position {\it index}, where {\it index} is zero-indexed
(such that {\tt peek(0)} gives the same value as {\tt pop()}).  If
there are fewer than $\mt{index}+1$ items in the stream,
then peek stalls until $\mt{index}+1$ items are available.
If $\mt{index}+1$ exceeds the capacity of the stream, then the 
return value is undefined.

\ssss{push EOS and checkEOS} In some cases the number of elements which can 
be popped from a stream is variable and determined by the producing kernel. 
The producing kernel can encode this information directly in the data stream, 
but doing so often ignores hardware support and incurs overhead for each element.
For this reason, the Stream API provides an optional boolean argument 
{\tt EOS} to the {\tt push} method and a {\tt checkEOS} method. When pushing
the last element onto a stream, the producing kernel may call push
with {\tt EOS} set to true. {\tt checkEos}, called with argument  
{\it index}, returns true if the {\it index}th or less element is the last element
in the stream. The EOS functions are used only when necessary (when there is 
not another way to determine the end of a stream). \footnote{EOS and checkEOS can be 
implemented using several methods, such as a EOS bit attached to each element 
in a FIFO or a pointer to the last item in a random-access memory.}

\subsubsection{Blocks}
\label{sec:blocks}

For kernels that require random access to a fixed set of elements, the
API provides the {\it block} abstraction.  A block is simply a region
of memory that can be read and written to.  Since some kernels use a
given block only for input or only for output, we divide the block
classes into {\tt IBlock} and {\tt OBlock}, with {\tt Block} extending
both (see Figures~\ref{fig:declgrid} and~\ref{fig:inherit}).

\ssss{write} For output blocks, the {\tt write} function stores an
item into memory.  It requires that the {\tt index} written to is less
than the block's {\tt capacity}.  

\ssss{read} For input blocks, the {\tt read} function reads a location
in memory.  It requires that the {\tt index} read is less than the
block's {\tt capacity}.

\subsection{Kernels}
\label{sec:kernel}

Kernels are objects used to map computation to processors. 
The API has support for three kinds of kernels.
Section~\ref{sec:kernelhlc} describes general user-defined kernels
whose behavior is explicitly described by the high-level compiler;
Section~\ref{sec:kernelsvm} describes pre-defined kernels that are
built into the SVM and must be supported on every architecture; and
Section~\ref{sec:kernelllc} describes library kernels that appear as a
black box to the high-level compiler. 

All three types of kernels share a common structure, base class, and execution model.

\sss{Kernel Structure}

Kernels have the following components:

\begin{enumerate}

\item A constructor, which receives the following:

\begin{itemize}
\item The processor resource where the kernel will execute.
\item (Optional) A block of memory for spilling local variables. This
block is always of type {\tt Block<byte>}. 
\item The inputs and outputs for the kernel.
\item Any other kernel-specific initialization data.
\end{itemize}

\item A {\it work} function that defines the execution of the kernel.

\item (Optional) Data members, which represent kernel data that is
accessible to the control thread before, after, and at certain times
during kernel execution.

\item A {\it status} field that reflects the current state of the kernel.

\item Control methods, called from threaded code or within the kernel
to control its execution.

\end{enumerate}

\sss{Kernel Base Class}

The status and control methods are all inherited from the kernel base class:

{\small
\begin{verbatim}
  class Kernel {

  public:      
    // status codes
    enum STATUS {
      UNSTARTED,
      WAITING,
      RUNNING,
      PAUSED,
      FINISHED
    }
  private:
    // status of kernel
    STATUS status;

  public:
    // non-blocking method that indicates that the kernel should not execute the
    // work function until kernel <k> is FINISHED.  This method can only be called
    // when the kernel is in the UNSTARTED state.
    void addDependence(Kernel& k);

    // non-blocking method that starts or resumes execution of this kernel.  The  
    // kernel continues to run until its status is PAUSED or FINISHED.
    void run();

    // non-blocking method that interrupts execution of a kernel on a best-effort basis,
    // and then sets the kernel's status to PAUSED.  This should be followed by a call to 
    // wait() if the control thread wants to ensure that the kernel is paused.
    void pause();

    // non-blocking method that interrupts execution of a kernel on a best-effort basis,
    // and then sets the kernel's status to FINISHED.  This should be followed by a call to 
    // wait() if the control thread wants to ensure that the kernel is finished.
    void end();

    // waits for the status of the kernel to be PAUSED or FINISHED.
    void wait();

    // non-blocking method that returns the current status of the kernel
    STATUS getStatus();
  }  
\end{verbatim}}

\sss{Kernel Execution Model}

\begin{figure}[t]
\begin{center}
\begin{verbatim}
UNSTARTED -(run)-> WAITING -(deps finished)-> RUNNING -(end)-> FINISHED
                                              |    ^              ^
                                          (pause)(run)          (end)
                                              v    |              |
                                              PAUSED -------------/
\end{verbatim}
\end{center}
\caption{Legal transitions of a kernel's {\tt status}}
\end{figure}

Kernels have a simple execution model. 
A kernel is initially {\tt UNSTARTED}. 
The control thread calls {\tt run()}, the kernel becomes {\tt WAITING}.
When all kernels it depends on are {\tt FINISHED}, the kernel becomes {\tt RUNNING}.
The kernel executes its {\tt work()} function. 
If either the control thread or the work function calls {\tt pause()},
the kernel pauses in executing the work function and becomes {\tt PAUSED}.
It remains {\tt PAUSED} until the control thread calls {\tt run()} again.
If either the control thread or the work function calls {\tt end()},
the kernel stops executing the work function and becomes {\tt FINISHED}.
The kernel implicitly calls {\tt end()} when the work function returns.
A transition
diagram for the legal states of a kernel appears in
Figure~\ref{fig:kernel-status}.

\subsubsection{User-Defined Kernels}
\label{sec:kernelhlc}

User-defined kernels have user-defined constructors, work functions,
and data members. For example, a kernel for a simple amplifier could be as follows:
{\small
\begin{verbatim}
  class AmplifierKernel : public Kernel {
    IStream<float> in;
    OStream<float> out;
    int N;

  public:
    AmplifierKernel(int _N, VM_NODE location, Block<byte> scratch, 
                    IStream<float> _in, OStream<float> _out) : 
                    public Kernel (location, scratch, _in, _out) {
      in = _in;
      out = _out;
      N = _N;
    }

  protected:
    work() {
      while(!in.checkEOS(0)) {
        out.push(in.pop() * N);
      }
    }
  }  
\end{verbatim}}

\sss{User-defined Kernel Restrictions}

The constructor must store the the input and output streams
with a one-to-one mapping from arguments to streams, and no surrounding
control flow.  This allows {\tt work} to access the streams.

The {\tt work()} function only supports a subset of C++; restrictions
are listed in Figure~\ref{fig:restrict}.  

\begin{figure}[t]
\framebox[6.5in]{
\begin{minipage}{6in}
\begin{enumerate}

\item No pointers.

\item No dynamic memory allocation.

\item No accesses to global memory.

\item No GOTO statements (all control flow is structured).

\item No recursive functions (all function calls have inline
semantics).

\item No calls to functions that violate any of these restrictions.

\item No references to templates or objects besides the classes
described in this document.  Further, no creation or casting of
objects within kernels.

\item All kernel functions (including the constructor) must receive
their arguments by value (not by reference.)  Also, the kernel
constructor cannot invoke any member functions of a stream object.

\item Supported opcodes are only the logical, arithmetic, and boolean
operations found in C (no special-purpose DSP operations at this
time\footnote{DSP operations may be added at a future date pending
further discussion by the forum.}).

\item Supported types include 64-bit {\tt double}, 64-bit signed and
unsigned {\tt long}, 32-bit {\tt float}, 32-bit signed and unsigned
{\tt int}, 16-bit signed and unsigned {\tt short}, 8-bit {\tt byte},
{\tt boolean}, arrays with a fixed (int literal) length, and {\tt
struct}'s containing members of any other type.

\end{enumerate}

\caption{Restrictions on C++ code within kernels.\protect\label{fig:restrict}}
\end{minipage}}
\end{figure}

\subsubsection{Pre-Defined Kernels}
\label{sec:kernelsvm}

\label{sec:predef}

Special pre-defined kernels are used to move streams and blocks between memories.
These kernels are typically executed by DMA processors, but may be executed by
stream processors on some architectures.

\subsubsection*{Memory Management Kernels}

\ssss{Copy} The primary use of this kernel is for copying items
between streams in different memory banks, though it can also be used
for copying between streams in a single memory.
{\small
\begin{verbatim}
  template <class T>
  class Copy : public Kernel {
  public:
    Copy(VM_NODE location, IStream<T> srcStr, OStream<T> destStr, int length = ENTIRE_STREAM);
  }
\end{verbatim}}
The {\tt length} indicates how many items should be transferred; by
default, all elements in the {\tt srcStr} are copied.

\ssss{Scatter/Gather} A set of scatter and gather kernels allow copies
between non-contiguous elements of streams.  The strided kernels are
for copying regularly spaced chunks into or out of a block:
{\small
\begin{verbatim}
  template <class T>
  class StridedScatter : public Kernel {
  public:
    StridedScatter(VM_NODE location, IStream<T> srcStr, OBlock<T> destBlock, int length,
                   int destStride = 1, int itemsPerChunk = 1);
  }

  template <class T>
  class StridedGather : public Kernel {
  public:
    StridedGather(VM_NODE location, IBlock<T> srcBlock, OStream<T> destStr, int length,
                  int srcStride = 1, int itemsPerChunk = 1);
  }  
\end{verbatim}}
The above kernels copy items in segments of {\tt itemsPerChunk} items.
The {\tt srcStride} or {\tt destStride} indicates the number of items
between the start of adjacent chunks, while {\tt length} represents
the number of items that should be copied.

The indexed scatter and gather kernels allow irregular accesses to a
source or destination block:
{\small
\begin{verbatim}
  template <class T>
  class IndexedScatter : public Kernel {
  public:
    IndexedScatter(VM_NODE location, IStream<T> srcStr, IStream<int> indexStr, OBlock<T> destBlock, 
                   int length, int itemsPerChunk = 1);
  }

  template <class T>
  class IndexedGather : public Kernel {
  public:
    IndexedGather(VM_NODE location, IBlock<T> srcBlock, IStream<int> indexStr, OStream<T> destStr, 
                  int length, int itemsPerChunk = 1);
  }  
\end{verbatim}}
In the {\tt IndexedScatter} kernel, the {\tt indexStr} indicates the
positions in the output stream at which the records should be written;
in the {\tt IndexedGather} kernel, the {\tt indexStr} indicates the
positions in the input stream at which the records should be read.
The {\tt length} argument indicates the number of items that should be
copied.

\subsubsection{Black-Box Library Kernels}
\label{sec:kernelllc}

The API supports library routines via black-box kernels that are
defined in the metadata and then instantiated by name in the stream
control code.  These kernels are identical to user-defined kernels,
except that they have no {\tt work} functions; instead, they are 
recognized by the low-level compiler for the architecture and are 
translated into hand-optimized library code.

To ensure that the low-level compiler can recognize a black-box
kernel, the high-level compiler guarantees that it will not merge
black-box kernels with other kernels, or rename the kernel so that it
would become unrecognizable to the low-level compiler.

\subsection{Control}
\label{sec:control}

Threaded code is used to declare streams, blocks, and kernels and control
the execution of kernels.

An example follows:
{\small
\begin{verbatim}
  // read file into memory location of s0 (not part of API)
  int length1 = readFile("input.dat", MEM1, 0x1000, 1024);

  Stream<float> s0(MEM1, 0x1000, 1024, length1),
                s1(SRF1, 0x200, 1024),
                s2(SRF1, 0x600, 128), 
  Block<byte> scratch2(SRF1, 0x680, 32);

  // copy data from  main memory to local memory
  Copy<float> copy01(DMA1, s0, s1, length1);

  RunLengthEncode rle(PROC1, scratch2, s1, s2);

  // run the kernels
  copy01.run();
  rle.run();
  rle.wait();

  // if the output is still too large, run additional compression, 
  // overwriting s2 in place
  int length2 = rle.outputLength;
  if (length2 > SIZE_THRESHOLD) {
    Stream<float> s3(SRF1, 0x600, 128);
    Block<byte> scratch3(SRF1, 0x680, 32);
    
    Kernel zip = new ZipCompress(PROC2, scratch3, s2, s3)
    zip.run();
    zip.wait();
    length2 = zip.outputLength;
  }
  
  // copy data from  local memory to main memory
  Stream<float> s4(MEM1, 0x2000, 1024),
  
  Copy<float> copy24(DMA1, s2, s4);
  
  copy24.run();

  // store the result from memory to "output.dat" (not part of API)
  writeFile("output.dat", MEM1, 0x2000, length2);
\end{verbatim}}

The above code fragment illustrates several aspects of the stream
control API.  In the rest of this section, we desribe the 
capabilities and limitations of the stream control API.

\sss{Transfering Data Between Kernels}

There are two ways to transfer the outputs of one kernel to the
inputs of another.  Perhaps the most natural way is to reuse the same
stream, thereby carrying over the results; in our
example, stream {\tt s2} is used by multiple kernels.  The other way to
transfer items is by allocating a new input stream that overlaps with
the output stream in memory.  In this case,
the control code should indicate that the kernel reading the new
stream is dependent on the kernel that wrote the original stream; this
is done by use of the kernel's {\tt addDependence} method, thereby
saving the low-level compiler from doing location-based memory
analysis to discover dependences between kernels.

\sss{Transfering Data Between Threaded Code and Kernels}

Before kernel execution, streams can be initialized with (possibly
non-streaming) data from general-purpose threaded code.  Likewise, the
results of a streaming computation can be used in the threaded code
following the kernel's execution.  Both of these transfers are done
through memory, either using memory management kernels as in the example
 or by directly accessing the memory assigned to a
stream.  The control thread can rely on
the sequential data layout guaranteed by streams when managing this
communication (see Section~\ref{sec:streams}).  Also note that most
device I/O (such as file handling and terminal interaction) is done 
by threaded code, and then made available to streams through memory.  
For direct interaction with streams, I/O devices can be described as 
processors that support a single black-box kernel.

It is also possible for the control processor to inspect and modify
the data members of a kernel when it is not in the 
{\tt WAITING} or {\tt RUNNING}
states. Accesses to kernel fields are useful for passing parameters to a 
kernel or retrieving reduction values from
a kernel.  For example: 

{\small
\begin{verbatim}
  // --- kernel code ---
  class SumKernel : public Kernel {
  public:
    IStreamUnordered<int> in;
    int sum;

    SumKernel(VM_NODE location, Block<byte> scratch, IStreamUnordered<int> _in) : 
              Kernel (_in, location, scratch) {
      in = _in;
      sum = 0;
    }

  protected:
    void work() {
        while(!in.checkEOS(0)) {
            sum += in.pop();
        }
    }
  }

  // --- control code ---
  Stream<int> s1(SRF, 0x100, 128);
  Block<byte> scratch1(SRF, 0x180, 16);

  SumKernel sk(PROC1, scratch1, s1);
  sk.run();
  sk.wait();
  int finalSum = sk.sum;
\end{verbatim}}

\sss{Managing Kernel Instruction Memory}

The high-level compiler might anticipate upcoming kernel executions.
The following method allows this information to be transfered to the
low-level compiler so that the kernel can be loaded into IMEM ahead of
time:
\begin{verbatim}
  // gives a hint to the low-level compiler that it should be loading the instructions 
  // for <kernel> at the time of this call, as it is likely to be used in the near future
  void loadKernel(Kernel k);
\end{verbatim}

\subsubsection{Restrictions on Stream Control}

There are restrictions on the stream control code to ensure a valid hardware
mapping and allow simple analysis by the low-level compiler.
However, there are no restrictions on non-streaming statements, which
can be finely interleaved with the stream statements; these statements
can be arbitrarily complex threaded code (although, in accordance with
the threaded API, they must adhere to C instead of C++).

In order to represent a valid hardware mapping, there are several restrictions:

\begin{enumerate}

\item More than one kernel may read or write a stream over the course of a program
but only one {\tt RUNNING} kernel may be reading a stream at any given time and only one
{\tt RUNNING} kernel may be writing a stream at any given time.

\item A kernel may only read from or write to streams that are mapped
to memories directly connected to the processor executing the kernel
(there must exist a series of connections with no intervening 
processors or memories that connects the processor running the kernel to 
the memory that contains the stream.)

\item Only one kernel may be {\tt RUNNING} on a given processor at any given time.

\end{enumerate}

In order to facilitate static analysis in the
low-level compiler, there are a number of restrictions:

\begin{enumerate}

\item All stream, block, and kernel variables have a static
single assignment in the program.  This exposes exactly which
constructor is invoked for a given variable name.

\item Architecture resources must be specified by literals when they
are passed to stream, block, or kernel functions.  That is,
constants denoting processor nodes, memory nodes, connections, and
channels must be passed to API functions directly, rather than passing
the value in a variable.  Similarly, identifiers of objects can only
be used in three contexts: 1) in their own declaration, 2) as the
direct target of a method call, and 3) as an argument to a function.
This ensures that all references to streams, blocks, and kernels are
resolvable by the low-level compiler.

\item There are no references to templates or objects besides the
classes described in this document.

\item Any function that contains a reference to a stream, block, or kernel, or
object must NOT be recursive.

\end{enumerate}

\subsubsection{Example}
\label{sec:example}

WARNING: EXAMPLE IS WAY OUT OF DATE

We consider one more example in order to illustrate graphs with
pre-defined kernels.  In this example, there are two processors that
each contain their own memory:

\begin{figure}[h]
\begin{center}
\psfig{figure=ex1.eps,width=1.8in}
\end{center}
\vspace{-12pt}
\end{figure}

The application does audio segmentation on a series of 10 input files
and plays a summary of each file on a speaker.  The first processor
does the segmentation itself, while the second processor filters the
extracted segments to provide a smooth transition between them.

{\small
\begin{verbatim}
  // --- code for PROC1 ---
  for (int i=0; i<10; i++) {
    // read file and put in memory (not part of API)
    int fileLength = readFile(filename[i], 10000, MEM1, 0x1000);

    // allocate overlapping stream in memory for raw data
    Stream<float> rawStr(fileLength, MEM1, 0x1000, fileLength), 
    // also allocate block for gather operation
    Block<float> rawBlock(fileLength, MEM1, 0x1000), 
    
    // allocate other streams
    Stream<float> segIndices(PROC1), sumData(10, PROC1);
    Block<byte> scratch1(128, MEM1, 0x4000);

    Kernel k1 = new ExtractSegments(PROC1, scratch1,               // make indices of summary segments
                                    rawStr, segIndices);
    Kernel k2 = new Gather<float>(DMA1, rawBlock, segIndices, sumData),  
                                                                   // gather summary audio in sumData
    k1.run();                                                      // run for whole length of file
    k2.run();                                                      // run for whole length of file
    k2.wait();
  }

  // --- code for PROC2 ---
  for (int i=0; i<10; i++) {
    stream<float> sumData(128, MEM2, 0x2000), smoothData(100, MEM2, 0x2080);
    stream<byte> scratch2(128, MEM2, 0x2160);

    Kernel k2 = new FIRFilter(PROC2, scratch,                      // filter summaries
                              sumData, smoothData);
    Kernel k3 = new Speaker(smoothData);                           // send to speaker

    k1.run();
    k2.addDependence(k1);
    k2.run();
    k3.addDependence(k2);
    k3.run();
    k3.wait();
  }   
\end{verbatim}}

In processor 1, a {\tt Gather} kernel is used to load the audio file
at the indices where the summary segments appear.  The {\tt Gather}
kernel is directly connected to a {\tt Send} kernel which sends the
summary segments across virtual channel 3 of connection {\tt c1}.
Processor 2 uses a {\tt Receive} kernel to receive the summary
segments before filtering them and sending them to a speaker.  Note
that there are 10 sessions of data transfer between the processors,
and the amount of data transferred during each session depends on the
length of the audio file; a session is finished when processor 1
finishes executing its stream graph.
