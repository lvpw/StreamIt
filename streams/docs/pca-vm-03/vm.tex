\section{Streaming Virtual Machine API}

The Streaming Virtual Machine (SVM) describes logical and physical
entities recognized by the High Level Compiler (HLC) and Low Level
Compiler (LLC) as a description of a streaming PCA program.  The SVM
describes the valid operations on each entity and the constraints for
mapping logical entities onto physical entities.  During compilation,
the HLC creates a structure or set of structures containing the
logical entities and their mapping onto the physical entities.  The
available physical entities are described in the PCA SVM metadata.
The logical structure represents the function and performance of the
HLC source program.  The mapping to physical entities reflects the HLC
resource mapping.  The LLC translates the logical structures to
executable machine code for a specific PCA architecture.

\sss{Logical Entitites}

The SVM contains the following logical entities:

\begin{itemize}

\item {\it Streams}.  A stream is a data type that represents a
sequence of data elements of a given type.  Streams are used for
sequential production and consumption of elements.

\item {\it Blocks}. Blocks provide indexed (random) access to a fixed
set of data elements.

\item {\it Kernels (slaves)}. Kernels are a logical entity that
describes a locus of computation consuming 0 or more input streams and
producing 0 or more output streams.  Kernels contain an execution
state (unstarted, waiting, paused, running, or finished).  Kernels are
described with a subset of C code and SVM API commands.

\item {\it Controls (masters)}. Controls are a logical entity that
describes the locus of computation within a processor.  Control code
can initiate, monitor, and terminate the execution of kernels.
Controls are described with a subset of C code and SVM API commands.

\item {\it Data elements}. Data elements of streams and blocks must
have a fixed size.  Variable sized elements can be supported at the
language-level and compiled into the fixed size scheme using a data
encoding.  Supported types include 64-bit {\tt double}, 64-bit signed
and unsigned {\tt long}, 32-bit {\tt float}, 32-bit signed and
unsigned {\tt int}, 16-bit signed and unsigned {\tt short}, 8-bit {\tt
byte}, {\tt boolean}, arrays with a fixed (int literal) length, and
{\tt struct}'s containing members of any other type.

\end{itemize}

\sss{Physical Entities}

The following physical entities are contained within the metadata and referenced by
the SVM:

\begin{itemize}

\item {\it FIFOs}.  A FIFO is a first-in first-out buffer.  The FIFO
state includes the data elements stored in the FIFO and the number of
items stored in the FIFO which includes the empty state.  The valid
FIFO operations are Peek(n), Pop, and Push.

\item {\it Memories}. A memory is random access storage with an index
pointing to the data element to be read or written.

\item {\it Processors}. A processor is logic with the ability to carry
out an independent execution.  It has internal state.  It can be
independently started or stopped.  A given processor can support a
certain set of black box kernels, and may support user-defined
kernels.

\item {\it Links}. Links pass data elements between the physical
entities.  Links connect processors to memories, processors to FIFOs,
and links to other links.

\end{itemize}

\sss{Processor Properties}

The following processor properties in the metadata are relevant to the
operation of the SVM:

\begin{itemize}

\item {\it MASTER}.  This property indicates one or more processors
that control a given processor.  A MASTER is capable of controlling a
kernel's execution on a corresponding slave processor, as well as
issuing a morph command for the slave.  A processor may be its own
MASTER.

\item {\it SUPPORTED\_KERNELS}.  This property indicates a list of
kernels that can execute on a given processor.  This list can contain
the names of black-box kernels, as well as a USER\_DEFINED\_KERNEL
entry, which indicates the ability to execute any user-defined kernel.
The black box kernels can be architecture-specific or architecture
independent (such as the Move and Scatter/Gather kernels described in
this document.)

\end{itemize}

\sss{Logical to Physical Binding}

The SVM addresses the mapping of logical to physical entities
described above:

\begin{itemize}

\item A Stream is mapped to a FIFO or a memory.

\item A Block is mapped to a memory.

\item A Kernel is mapped to a processor that supports the given kernel.

\item Control code is mapped to processors that are their own master.

\end{itemize}

\sss{SVM Overview}

The Streaming Virtual Machine API is a set of data types and functions
used to express the streaming portion of an application mapped to a
specific VM by the high-level compiler. It includes streams and blocks
which specify how data is mapped to memory, kernels which specify how
computation and data-movement functions are mapped to processors, and
control functions which are used by threaded code to control kernel
execution.

The Streaming Virtual Machine API is a strict C subset, and can be
compiled by a standard C compiler with a simple library for testing
and debugging purposes.  We describe the API for streams in
Section~\ref{sec:streams}, the API for blocks in
Section~\ref{sec:blocks}, the API for kernels in
Section~\ref{sec:kernel}, and the API for control in
Section~\ref{sec:control}.

\subsection{Streams}
\label{sec:streams}

A {\it stream} is a data type that represents a sequence of elements
of a given type.  The elements must have a fixed size. Variable sized
elements can be supported at the language-level and compiled into the
fixed size scheme using a data encoding. The stream type and
supporting enumerations and functions are declared as follows:

{\small
\begin{verbatim}
  typedef struct {
    // not exposed
  } Stream;

  // aliases used by high-level compiler to indicate how a kernel uses a stream
  // to the low-level compiler: for input only, output only, or both
  typedef Stream IStream;
  typedef Stream OStream;
  typedef Stream IOStream;
 
  //
  // stream initialization functions:
  //
  
  // make a stream mapped to random access memory <ramLocation>, 
  // allocated at <address> that can hold at most <capacity> elements
  // of size <elementSize>
  void streamInitRAM(Stream* s, VM_NODE_MEM ramLocation, int address, 
                     int capacity, int elementSize, int flags); 
 


  // as above with <initLength> elements already in place.  If 
  // <initEOS> is true, then the last item is tagged with an EOS flag.
  void streamInitWithDataRAM(Stream* s, VM_NODE_MEM ramLocation, int address, 
                             int capacity, int elementSize, int initLength, 
                             int initEOS, int flags);

  // make a stream mapped to hardware FIFO <fifoLocation> that contains
  // elements of size <elementSize>
  void streamInitFIFO(Stream* s, VM_NODE_MEM fifoLocation, int elementSize, int flags);

  // flags used by high level-compiler to pass performance hints to the low-level compiler
  typedef enum /* STREAM_FLAGS */ {
    // True if FIFO semantics may be relaxed such that popping does not return   
    // elements in the same order that the elements were pushed. Peek cannot be
    // used if this flag is set.
    STREAM_UNORDERED = 0x1,

    // True if other streams/blocks not aliased with this stream and stream 
    // can have arbitrary layout in memory
    STREAM_UNALIASED_RAM = 0x2,
    
    // True if more than capacity elements will never be pushed
    STREAM_NEVER_WRAPS = 0x4,

    // True if at most one element in the stream will ever have an EOS
    // tag at any given time
    STREAM_ONE_EOS = 0x8
  } STREAM_FLAGS;  

  //
  // stream io functions, described in more detail below:
  //
  
  // pushes element onto stream, stalls if full
  void streamPush(Stream* s, void* element);
  
  // pushes element onto streams, stalls if any is full
  void streamPushMulticast(void* element, Stream* s0, Stream* s1, ...);
  
  // pops element off of stream, stalls if empty
  void streamPop(Stream* s, void* element);
  
  // peeks element n from the front of the stream
  // stalls if less than n+1 elements are available
  void streamPeek(Stream* s, int n, void* element);

  // pushes element onto stream with an EOS tag attached to the
  // element; stalls if full
  void streamPushWithEOS(Stream* s, void* element);

  // pushes element onto streams with an EOS tag attached to the
  // element; stalls if any is full
  void streamPushMulticastWithEOS(void* element, Stream* s0, Stream* s1, ...);

  // returns whether or not there is an EOS tag on element n from the
  // front of the stream; stalls if less than n+1 elements available.
  boolean streamPeekEOS(Stream* s, int n)
\end{verbatim}}

\ssss{Stream types} The IStream, OStream, and IOStream aliases for the
Stream type are intended to make the code more readable, as they
indicate the relationship between a kernel and a stream.  Note that
IOStreams are used by a single kernel to both pop and push elements.
The IOStream is not used between kernels since such use would be
output for one kernel and input for the other. The primary use of the
IOStream is to share a single physical resource for both an output and
input stream.  Data pushed onto the stream can be popped.  When the
resource is full, {\tt push} stalls.  When the resource is empty, {\tt
pop} stalls.  IOStreams are a first-in and first-out structure.

\ssss{Initialization functions} The stream initialization functions
declare the location and size of the stream, and specify flags which
give performance hints to the low level compiler.  Note that an
address to a memory is the full address required by the architecture
metadata description; it may be the node, offset or other.  Address
and capacity are not static at compile time; they may be an expression
determined at run time.  Element size is static at compile time.  Note
also that the flags can be ignored (assumed to have a value of zero)
without sacrificing correctness; they are only for the sake of
optimization.

Each stream mapped to random access memory for which the
STREAM\_UNALIASED\_RAM flag is not set makes an important guarantee on
its data layout: it is implemented as a circular buffer of length {\tt
capacity} that starts at the specified {\tt address} and wraps around
to the beginning when more than {\tt capacity} elements have been
pushed.  This guarantee is important for allowing reuse of memory
space between streams, as well as for transferring data between
threaded code and streaming code.  However, there is no contract on
data layout at any time if the STREAM\_UNALIASED\_RAM flag is
set. This gives the low-level compiler the freedom to implement
streams with an architecture-specific representation if their contents
are never aliased.

The {\tt streamInitRAMWithData} initialization function is used to
initialize a stream which aliases RAM that already contains data.  The
{\tt initLength} parameter indicates how many elements are already in
place at {\tt address} as though they were pushed onto the stream
initially. The STREAM\_UNALIASED\_RAM flag must NOT be set with this
initializer.

\sss{Stream I/O Functions}

We now turn our attention to the other stream functions, which are
used to write and read the stream.

\ssss{streamPush} For output streams, the {\tt streamPush} function
enqueues a value onto the end of the stream.  If the stream cannot
hold any more elements, then {\tt streamPush} stalls until there is
space available.

\ssss{streamPushMulticast} The {\tt streamPushMulticast} function
pushes the same data item onto multiple streams, and is equivalent to
a series of {\tt streamPush} operations with the same argument but
provides an opportunity for optimization by the low-level compiler.
Note: this also has the effect of synchronizing the streams when one
or more of the stream buffers is unable to accept more inputs.

\ssss{streamPop} For input streams, the {\tt streamPop} function
dequeues a value from the front of the stream.  If there are no
elements in the stream, then {\tt streamPop} stalls until an element
is available.

\ssss{streamPeek} For ordered input streams, the {\tt streamPeek}
function returns the element at position {\it index}, where {\it
index} is zero-indexed (such that {\tt streamPeek(0)} gives the same
value as {\tt streamPop()}).  If there are fewer than $\mt{index}+1$
elements in the stream, then {\tt streamPeek} stalls until
$\mt{index}+1$ elements are available. If $\mt{index}+1$ exceeds the
capacity of the stream, then the return value is undefined.

\ssss{streamPushWithEOS, streamPushMulticastWithEOS, and
streamPeekEOS} In some cases the number of elements which can be
popped from a stream is variable and determined by the producing
kernel.  The producing kernel can encode this information directly in
the data stream, but doing so often ignores hardware support and
incurs overhead for each element.  For this reason, the Stream API
provides the {\tt streamPushWithEOS} method to attach an EOS tag to a
data item, and a {\tt streamPeekEOS} method to see whether or not
there is an EOS tag for a given item.  A producing kernel should use
{\tt streamPushWithEOS} for the last element that it produces onto a
given stream.  The consuming kernel can call {\tt streamPeekEOS} at
any time; if called with argument {\tt index}, it will stall until
{\tt index + 1} elements are available (just like {\tt streamPeek}).
Note that the {\tt streamPop} function will pop both a data item and
any associated EOS tag.  Also, multiple items in a given stream can
have an EOS tag (unless the STREAM\_ONE\_EOS flag is passed to the
stream initializer, indicating that only a single EOS will appear at a
given time.)

%% \subsection{Memory}
%% \label{sec:memory}

%% The SVM API specifies how data is mapped to memory using stream and
%% block declarations in the control code.  Streams are used for
%% sequential production and consumption of elements, while blocks
%% provide random access to a fixed set of elements.  Streams are mapped
%% to random-access memory or hardware FIFOs. Blocks are only mapped to
%% random-access memory.

\subsection{Blocks}
\label{sec:blocks}

For kernels that require random access to a fixed set of elements, the
API provides the {\it block} abstraction.  A block is simply a region
of memory that can be read and written to.  The block type and
supporting functions are declared as follows:

{\small
\begin{verbatim}
  typedef struct {
    // not exposed
  } Block;

  // aliases used by high-level compiler to indicate how a kernel uses a block
  // to the low-level compiler
  typedef Block IBlock;
  typedef Block OBlock;

  // make a block mapped to memory node <ramLocation>, allocated at <address> 
  // that can hold at most <capacity> elements of size <elementSize>
  void blockInit(Block* b, VM_NODE_MEM ramLocation, int address, 
                 int capacity, int elementSize);

  //
  // block I/O functions, described in more detail below:
  //

  // writes element to block
  void blockWrite(Block* b, int index, void* element);

  // reads element from block
  void blockRead(Block* b, int index, void* element); \end{verbatim}}

\ssss{initialization functions} The block initialization function
declares the location and size of the block.

\sss{Block I/O Functions}

\ssss{blockWrite} For output blocks, the {\tt write} function stores
an element into memory.  It requires that the {\tt index} written to
is less than the block's {\tt capacity}.

\ssss{blockRead} For input blocks, the {\tt read} function reads a
location in memory.  It requires that the {\tt index} read is less
than the block's {\tt capacity}.

\subsection{Kernels}
\label{sec:kernel}

Kernels can be used to map computation to processors.  The API has
support for three kinds of kernels. Section~\ref{sec:kernelhlc}
describes general user-defined kernels whose behavior is explicitly
described by the high-level compiler; Section~\ref{sec:kernelsvm}
describes pre-defined kernels that are built into the SVM and must be
supported on every architecture; and Section~\ref{sec:kernelllc}
describes library kernels that appear as a black box to the high-level
compiler.

All three types of kernels share a common execution model, as well as
a base data type that specifies the location for kernel execution and
is passed to functions that control kernel execution.

Kernels have the following components:

\begin{enumerate}

\item An initialization function, which receives the following: 
\begin{itemize}
\item The processor resource where the kernel will execute. 
\item (Optional) A block of memory for spilling local variables. 
\item The input and output streams and blocks for the kernel. 
\item Any other kernel-specific initialization data. 
\end{itemize}

\item A {\it work} function that defines the execution of the kernel.

\item (Optional) A data type which represents kernel data that is
accessible to the control thread before, after, and at certain times
during kernel execution.

\item A {\it status} that reflects the current state of the kernel.

\item Control functions used to execute the kernel.

\end{enumerate}

\sss{Kernel Base Data Type}

All kernels share a base data type which is extended to create
specific kernels. The kernel type and supporting functions are
declared as follows:

{\small
\begin{verbatim}
  typedef struct {
    // not exposed
  } Kernel;

  // function pointer to work function
  typedef void (*ExtKernelWork) (void* extKernelData);

  // makes a kernel that executes on processor <procLocation>,
  // spills to memory <scratch> and is extended with the encompassing
  // data type <extKernelData> of size <extKernelDataSize> and work
  // function <extKernelWork>
  void kernelInit(Kernel* k, VM_NODE_PROC procLocation, Block* scratch,
    void* extKernelData, int extKernelDataSize, ExtKernelWork extKernelWork);

  // kernel status codes
  typedef enum {
      KERNEL_UNSTARTED,
      KERNEL_WAITING,
      KERNEL_RUNNING,
      KERNEL_PAUSED,
      KERNEL_FINISHED
  } KERNEL_STATUS;

  //
  // kernel control functions, described in more detail below:
  //

  // non-blocking function that indicates that kernel <k> should not 
  // execute the work function until kernel <dependsOnKernel> is FINISHED.  
  // This function can only be called when <k> is in the UNSTARTED state,
  // and is ignored if dependsOnKernel is in the UNSTARTED state.
  void kernelAddDependence(Kernel* k, Kernel* dependsOnKernel);

  // hint to low-level compiler to ready the kernel for execution.
  void kernelReady(Kernel* k);

  // non-blocking function that starts or resumes execution of this kernel. 
  // The kernel continues to run until its status is PAUSED or FINISHED.
  void kernelRun(Kernel* k);

  // non-blocking function that interrupts execution of a kernel on a
  // best-effort basis, and then sets the kernel's status to PAUSED.  This 
  // should be followed by a call to kernelWait() if the control thread wants to
  // ensure that the kernel is paused.
  void kernelPause(Kernel* k);

  // non-blocking function that interrupts execution of a kernel on a 
  // best-effort basis, and then sets the kernel's status to FINISHED.  This 
  // should be followed by a call to kernelWait() if the control thread wants to
  // ensure that the kernel is finished.
  void kernelEnd(Kernel* k);

  // waits for the status of the kernel to be PAUSED or FINISHED.
  // ignored for UNSTARTED kernels.
  void kernelWait(Kernel* k);

  // waits for at least one kernel to be PAUSED or all kernels to be FINISHED.
  // ignores UNSTARTED kernels.
  void kernelWaitMultiple(Kernel* k0, Kernel* k1, ...);

  // returns the status of the kernel as best known by the control thread
  KERNEL_STATUS kernelGetStatus(Kernel* k);
\end{verbatim}}

\sss{Kernel Execution Model}

\begin{figure}[t]
\framebox[6.5in]{
\begin{minipage}{6in}
\begin{center}
\psfig{figure=State.eps,width=5in}
\end{center}
\caption{Legal transitions of a kernel's {\tt status} \protect\label{fig:kernel-status}}
\end{minipage}}
\end{figure}

Kernels have a simple execution model. A transition diagram for the
legal states of a kernel appears in Figure~\ref{fig:kernel-status}.

A kernel is initially {\tt UNSTARTED}.  The control thread calls {\tt
kernelRun()}, the kernel becomes {\tt WAITING}.  When all kernels it
depends on (as explicitly indicated by the {\tt kernelAddDependence}
function) are {\tt FINISHED}, the kernel becomes {\tt RUNNING}. The
kernel executes its {\tt work()} function.  If either the control
thread or the work function calls {\tt kernelPause()}, the kernel
pauses in executing the work function and becomes {\tt PAUSED}. It
remains {\tt PAUSED} until the control thread calls {\tt kernelRun()}
again. If either the control thread or the work function calls {\tt
kernelEnd()} at any time when the kernel is not {\tt FINISHED}, the
kernel stops executing the work function and becomes {\tt
FINISHED}. The kernel implicitly calls {\tt kernelEnd()} when the work
function returns. If control flow causes a kernel to be initialized
again, it becomes {\tt UNSTARTED}.

Section~\ref{sec:control} gives a detailed explanation of how kernels
should be controlled.  However, we also give some brief examples here
to clarify the model of computation for kernels.  The most common
control sequence for initializing and running a kernel is as follows:

{\small
\begin{verbatim}
  static MyKernel k;

  kernelInit(&k.kernel, ...);
  kernelRun(&k.kernel);
  kernelWait(&k.kernel);
\end{verbatim}}

In addition, multiple kernels can be run in parallel, with items being
transfered from one to the other using streams.  In this case, no
explicit dependences are needed.  For example:

{\small
\begin{verbatim}
  static Stream s;
  static MyKernel1 k1;   // k1 will write to s
  static MyKernel2 k2;   // k2 will read from s

  streamInitRAM(&s, ...);

  kernelInit(&k1.kernel, ...);  
  kernelRun(&k1.kernel);

  kernelInit(&k2.kernel, ...);
  kernelRun(&k2.kernel);

  kernelWait(&k2.kernel);
\end{verbatim}}

Thus, the {\tt kernelAddDependence} function does NOT need to be used
to synchronize the transfer of data elements through a stream object.
Instead, it is intended for cases in which there is a memory
dependence that is not captured by the stream semantics.  This can
occur in two cases: 1) a kernel writes to a block that a subsequent
kernel reads from, and 2) two kernels are reading/writing to streams
or blocks that point to overlapping regions in memory.  It would be
possible to use {\tt kernelWait} to enforce these dependences from the
control thread, but the dependence tracking mechanism is added for
performance reasons; with stream processing, a round-trip
communication to the control thread is often too expensive.  A simple
use of dependences is as follows:

{\small
\begin{verbatim}
  static Block b;
  static MyKernel1 k1;   // k1 will write to b
  static MyKernel2 k2;   // k2 will read from b

  blockInit(&b, ...);

  kernelInit(&k1.kernel, ...);  
  kernelRun(&k1.kernel);

  kernelInit(&k2.kernel, ...);
  kernelAddDependence(&k2.kernel, &k1.kernel);
  kernelRun(&k2.kernel);

  kernelWait(&k2.kernel);
\end{verbatim}}

Finally, there is a subtle point: the specification supports the use
of dependent kernels in a loop.  In this scenario, each kernel is run
multiple times, and each call to {\tt kernelAddDependence(k2, k1)}
makes {\tt k2} dependent on the last execution of {\tt k1}, whenever
it occured. If {\tt k1} is {\tt UNSTARTED}, the {\tt
kernelAddDependence} call is ignored. The following code illustates
these concepts:

{\small
\begin{verbatim}
  while (...) {
    static K1 k1;
    static K2 k2;

    kernelWait(&k1);
    kernelInit(&k1.kernel, ...);
    kernelAddDependence(&k1.kernel, &k2.kernel);
    kernelRun(&k1.kernel);

    kernelWait(&k2);
    kernelInit(&k2.kernel, ...);
    kernelAddDependence(&k2.kernel, &k1.kernel);
    kernelRun(&k2.kernel);
  }
\end{verbatim}}

\subsubsection{User-Defined Kernels}
\label{sec:kernelhlc}

The kernel base data type is extended to different kinds of kernels by
declaring a new data type for each kernel which includes the kernel
base data type, an initialization function that calls the kernel
initialization function, and a work function that operates on the new
data type.

For example, a kernel for a simple amplifier could be as follows:
 
{\small
\begin{verbatim}
  typedef struct {
    Kernel kernel;
    IStream* in;
    OStream* out;
    int N;
  } Amplifier;

  void amplifierWork(Amplifier* amp);

  void amplifierInit(Amplifier* amp, VM_NODE_PROC procLocation, Block* scratch, 
                     IStream* _in, OStream* _out, int _N) {
      kernelInit(&amp->kernel, procLocation, scratch, 
        amp, sizeof(Amplifier), (ExtKernelWork)&amplifierWork);
      amp->in = _in;
      amp->out = _out;
      amp->N = _N;
  }
 
  void amplifierWork(Amplifier* amp) {
      float x;
      while (!streamPeekEOS(amp->in, 0)) {
        streamPop(amp->in, &x);
        x = x * amp->N;
        streamPush(amp->out, &x);
      }
      streamPop(amp->in, &x);
      x = x * amp->N;
      streamPushWithEOS(amp->out, &x);
  }
\end{verbatim}}

\noindent If the input stream is known to be infinite, then the work
function can be simplified as follows:

{\small
\begin{verbatim}
  void amplifierWork(Amplifier* amp) {
      while(true) {
        float x;
        streamPop(amp->in, &x);
        x = x * amp->N;
        streamPush(amp->out, &x);
      }
  }
\end{verbatim}}

\sss{User-defined Kernel Restrictions}

The initialization function must statically associate the input and
output streams with a one-to-one mapping from arguments to streams,
and no surrounding control flow or condition statements that can
change the assignment.  This ensures that the low-level compiler can
statically calculate which streams are accessed from the work
function.

Also, the work function supports only a subset of C; restrictions are
listed in Figure~\ref{fig:restrict}.

\begin{figure}[t]
\framebox[6.5in]{
\begin{minipage}{6in}
\begin{enumerate}

\item No pointers, except for those directly passed to stream, block,
and kernel functions as described in this document.

\item No dynamic memory allocation.

\item No accesses to global variables.

\item No GOTO statements (all control flow is structured).

\item No recursive functions (all function calls have inline semantics).

\item No calls to functions that violate any of these restrictions.

\item Supported opcodes are only the logical, arithmetic, and boolean
operations found in C (no special-purpose DSP operations at this
time\footnote{DSP operations may be added at a future date pending
further discussion by the forum.}).

\item Supported types include 64-bit {\tt double}, 64-bit signed and
unsigned {\tt long}, 32-bit {\tt float}, 32-bit signed and unsigned
{\tt int}, 16-bit signed and unsigned {\tt short}, 8-bit {\tt byte},
{\tt boolean}, arrays with a fixed (int literal) length, and {\tt
struct}'s containing members of any other type.

\end{enumerate}

\caption{Restrictions on C code within kernels.\protect\label{fig:restrict}}
\end{minipage}}
\end{figure}

\subsubsection{Pre-Defined Kernels}
\label{sec:kernelsvm}

\label{sec:predef}

Special pre-defined kernels are used to move data between streams and
blocks. These kernels are typically executed by DMA processors, but
may be executed by stream processors on some architectures.

\ssss{Move} The primary use of this kernel is for moving elements
between streams in different memory banks, though it can also be used
for moving between streams in a single memory.

{\small \begin{verbatim}
  typedef struct {
    Kernel kernel;
    // not exposed
  } Move;
  
  const int STREAM_LENGTH_ALL = -1;
  
  void moveInit(Move* move, VM_NODE_PROC location, IStream* srcStr, 
                OStream* destStr, int length, boolean setEOS);
\end{verbatim}}

MoveInit is only the initialization routine (as in setting up a DMA
processor); it does not do the data move itself.  The {\tt length}
argument indicates how many elements should be popped from {\tt
srcStr} and pushed to {\tt destStr}. Passing a length of {\tt
STREAM\_LENGTH\_ALL} indicates that elements from {\tt srcStr} are
moved up through the first one that has an EOS tag.  If {\tt setEOS}
is true, then the last element pushed onto {\tt destStr} will have an
EOS tag.

\ssss{Scatter/Gather} A set of scatter and gather kernels allow copies
between non-contiguous elements within blocks and streams.  The
strided kernels are for moving regularly spaced chunks into or out of
a block:
{\small \begin{verbatim}
  typedef struct {
    Kernel kernel;
    // not exposed
  } StridedScatter;
  
  void stridedScatterInit(StridedScatter* scatter, VM_NODE_PROC location, 
                          IStream* srcStr, OBlock* destBlock, int length,
                          int destStride, int elementsPerStride, boolean setEOS);

  typedef struct {
    Kernel kernel;
    // not exposed
  } StridedGather;
  
  void stridedGatherInit(StridedGather* gather, VM_NODE_PROC location, 
                         IBlock* srcBlock, OStream* destStr, int length,
                         int srcStride, int elementsPerStride, boolean setEOS);
\end{verbatim}}

The above kernels move elements in segments of {\tt elementsPerStride}
elements. The {\tt srcStride} or {\tt destStride} indicates the number
of elements between the start of adjacent segments, while {\tt length}
represents the total number of elements that should be moved ({\tt
STREAM\_LENGTH\_ALL} may be used as with Move, indicates up to end of
block for gather).  If {\tt setEOS} is true, then the last element
pushed onto {\tt destStr} will have an EOS tag.

The indexed scatter and gather kernels allow irregular accesses to a
source or destination block:

{\small \begin{verbatim}
  typedef struct {
    Kernel kernel;
    // not exposed
  } IndexedScatter;

  void indexedScatterInit(IndexedScatter* scatter, VM_NODE_PROC location,
                          IStream* srcStr, IStream* indexStr, OBlock* destBlock, 
                          int length, int elementsPerIndex, boolean setEOS);
                     
  typedef struct {
    Kernel kernel;
    // not exposed
  } IndexedGather;

  void indexedGatherInit(IndexedGather* gather, VM_NODE_PROC location, 
                         IBlock* srcBlock, IStream* indexStr, OStream* destStr, 
                         int length, int elementsPerIndex, boolean setEOS);
\end{verbatim}}

In the {\tt IndexedScatter} kernel, the {\tt indexStr} indicates the
positions in the output block at which the records should be written;
in the {\tt IndexedGather} kernel, the {\tt indexStr} indicates the
positions in the input block at which the records should be read. The
number of data elements must be equal to the number of indices times
{\tt elementsPerIndex}. The {\tt length} argument indicates the number
of elements that should be moved ({\tt STREAM\_LENGTH\_ALL} may be
used as with Move, indicates up to end of indices for gather).  If
{\tt setEOS} is true, then the last element pushed onto {\tt destStr}
will have an EOS tag.

\subsubsection{Black-Box Library Kernels}
\label{sec:kernelllc}

The API supports library routines via black-box kernels that are
defined in the metadata and then instantiated by name in the stream
control code.  These kernels are identical to user-defined kernels,
except that their {\tt work} functions are not defined; instead, they
are recognized by the low-level compiler for the architecture and are
translated into hand-optimized library code.

To ensure that the low-level compiler can recognize a black-box
kernel, the high-level compiler guarantees that it will not merge
black-box kernels with other kernels, or rename a black-box kernel so
that it would become unrecognizable to the low-level compiler.

\subsection{Control}
\label{sec:control}

Threaded code is used to declare streams, blocks, and kernels and to
control the execution of kernels. All stream, kernel, and block
variables must be declared as static variables.

An example follows:
{\small
\begin{verbatim}
  int length1, length2;
  static Stream s0, s1, s2, s4;
  static Block scratch;
  static Move move01;
  static RLE rle;
  static Move move24;

  // read file into memory location of s0 (not part of API)
  length1 = readFile("input.dat", GLOBALMEM1, 0x1000, 1024);

  streamInitWithDataRAM(&s0, GLOBALMEM1, 0x1000, 1024, 4, length1, 1, STREAM_NEVER_WRAPS);
  streamInitRAM(&s1, LOCALMEM1, 0x0, 256, 4, STREAM_UNALIASED_RAM);
  streamInitRAM(&s2, LOCALMEM1, 0x100, 128, 4, STREAM_NEVER_WRAPS);
  blockInit(&scratch, LOCALMEM1, 0x180, 32, 1);

  // move from memory
  moveInit(&move01, DMA1, &s0, &s1, STREAM_LENGTH_ALL, TRUE);
  kernelRun(&move01.kernel);
  
  // run rle
  rleInit(&rle, PROC1, &scratch, &s1, &s2);
  kernelRun(&rle.kernel);

  // get output length
  kernelWait(&rle.kernel);
  length2 = rle.outputLength;
  
  // if the output is still too large, run additional compression, overwriting s2 in place
  if (length2 > SIZE_THRESHOLD) {
    static Stream s3;
    static Block scratch2;
    static Zip zip;

    streamInitRAM(&s3, LOCALMEM1, 0x100, 128, 4, STREAM_NEVER_WRAPS);
    blockInit(&scratch2, LOCALMEM1, 0x180, 32, 1);

    // run zip kernel
    zipInit(&zip, PROC1, &scratch2, &s2, &s3);
    kernelRun(&zip.kernel);
    
    // get output length
    kernelWait(&zip.kernel);
    length2 = zip.outputLength;

    streamInitWithDataRAM(&s2, LOCALMEM1, 0x100, 128, 4, length2, 1, STREAM_NEVER_WRAPS);
  }
  
  streamInitRAM(&s4, GLOBALMEM1, 0x2000, 1024, 4, STREAM_NEVER_WRAPS),
  
  // move to memory
  moveInit(&move24, DMA1, &s2, &s4, STREAM_LENGTH_ALL, TRUE);
  kernelRun(&move24.kernel);
  kernelWait(&move24.kernel);

  // store the result from memory to "output.dat" (not part of API)
  writeFile("output.dat", GLOBALMEM1, 0x2000, length2); \end{verbatim}}

\clearpage
The above code fragment illustrates several aspects of the stream
control API.  In the rest of this section, we describe the
capabilities and limitations of the stream control API.

\sss{Transferring Data Between Kernels}

There are two ways to transfer the outputs of one kernel to the inputs
of another.  Perhaps the most natural way is to reuse the same stream,
thereby carrying over the results; in our example, stream {\tt s2} is
used by multiple kernels.

The other way to transfer elements is by allocating a new input stream
that overlaps with the output stream in memory.  In this case, the
control code should indicate that the kernel reading the new stream is
dependent on the kernel that wrote the original stream; this is done
by use of the kernel's {\tt kernelAddDependence} function, thereby
saving the low-level compiler from doing location-based memory
analysis to discover dependences between kernels.

\sss{Transferring Data Between Threaded Code and Kernels}

Before kernel execution, streams can be initialized with (possibly
non-streaming) data from general-purpose threaded code.  Likewise, the
results of a streaming computation can be used in the threaded code
following the kernel's execution.  Both of these transfers are done
through memory, either by directly accessing the memory assigned to a
stream then using memory management kernels (as in the example), or by
calling stream I/O functions from the control code.  The control
thread can rely on the sequential data layout guaranteed by streams
when managing this communication (see Section~\ref{sec:streams}).
Also note that most device I/O (such as file handling and terminal
interaction) is done by threaded code, and then made available to
streams through memory.

For direct interaction with streams, I/O devices can be described as
processors that support a single black-box kernel and hence I/O data
does not need to move through memories.

It is also possible for the control processor to inspect and modify
the data members of a kernel when it is not in the {\tt WAITING} or
{\tt RUNNING} states. Accesses to kernel fields are useful for passing
parameters to a kernel or retrieving reduction values from a kernel.
For example:

{\small
\begin{verbatim}
  // --- kernel code ---
  typedef struct {
    Kernel kernel;
    IStream* in;
    int sum;
  } SumKernel;

  void sumKernelWork();
  
  sumInit(SumKernel* k, VM_NODE_PROC procLocation, Block* scratch, IStream* _in) {
    kernelInit(&k->kernel, procLocation, scratch,
      k, sizeof(SumKernel), (ExtKernelWork)&sumKernelWork); 
    k->in = _in;
    k->sum = 0;
  }

  void sumKernelWork(SumKernel* k) {
      int x;
      while (!streamGetEOS(k->in, 0)) {
          streamPop(k->in, &x);
          k->sum += x;
      }
      streamPop(k->in, &x);
      k->sum += x;
  }

  // --- control code ---
  static Stream s1;
  static Block scratch1;
  static SumKernel sk;
  int finalSum;

  // assumes prior code initializes data in RAM
  streamInitWithDataRAM(&s1, LOCALMEM1, 0x100, 128, 4, 128, 1, 0);

  // run sum kernel
  sumInit(&sk, PROC1, &scratch1, &s1);
  blockInit(&scratch1, LOCALMEM1, 0x180, 16, 1);
  kernelRun(&sk.kernel);

  // get output sum
  kernelWait(&sk.kernel);
  finalSum = sk.sum; 
}\end{verbatim}}

\sss{Managing Kernel Instruction Memory}

The high-level compiler might anticipate upcoming kernel
executions. The {\tt kernelReady} function allows this information to
be transferred to the low-level compiler so that the kernel can be
loaded into IMEM ahead of time.

\subsubsection{Restrictions on Stream Control}

There are restrictions on the stream control code to ensure a valid
hardware mapping and allow simple analysis by the low-level
compiler. However, there are no restrictions on non-streaming
statements, which can be freely mixed with the stream statements;
these statements can be arbitrarily complex threaded code.

In order to represent a valid hardware mapping, there are several restrictions:

\renewcommand{\labelenumi}{A\theenumi.}

\begin{enumerate}

\item An initialization function may not be called on an instance of a
{\tt WAITING}, {\tt RUNNING}, or {\tt PAUSED} kernel.

\item An initialization function may not be called on a stream or
block that is being read/written by a {\tt WAITING}, {\tt RUNNING}, or
{\tt PAUSED} kernel.

\item Initialization functions, and all kernel control functions
except kernelPause() may only be called in control code.

\item Only one user-defined kernel may be {\tt RUNNING} or {\tt
PAUSED} on a stream processor at any given time.

\item A number of pre-defined memory management kernels may be {\tt
RUNNING} or {\tt PAUSED} on a DMA processor at any given time.  The
maximum number is defined in the metadata.

\item A kernel may only read from or write to streams that are mapped
to memories or FIFOs directly connected to the processor executing the
kernel (there must exist a series of links with no intervening
processors or memories that connects the processor running the kernel
to the memory or FIFO that contains the stream.)

\item More than one kernel may read or write a stream over the course
of a program but only one {\tt WAITING}, {\tt RUNNING}, or {\tt
PAUSED} kernel may be reading a stream at any given time and only one
{\tt WAITING}, {\tt RUNNING}, or {\tt PAUSED} kernel may be writing a
stream at any given time.

\item Only one stream mapped to a hardware FIFO may retain state at
any given time. The SVM code should ensure that the state is empty
before calling an initialization function that maps any stream to the
FIFO.  The state is empty if there are no data elements in the FIFO.

\item Kernel A may only be dependent on kernel B if A can read from a
memory that B can write to.  That is, the processor executing A and
the processor executing B must both be connected to some common
memory.  This allows kernel B to notify kernel A when the dependence
is satisfied.

\end{enumerate}

\noindent In order to facilitate static analysis in the low-level
compiler, there are a number of restrictions:

\renewcommand{\labelenumi}{B\theenumi.}

\begin{enumerate}

\item A call to an initialization function must precede any use of the
kernel, block, or stream variable.

\item Hardware resource locations (e.g. processor nodes and memory
nodes), element sizes, and stream performance hint flags, must be
specified by literals when they are passed to the initialization
functions for a stream, block, or kernel variable. Those literals must
be the same for all initialization functions for a given stream,
block, or kernel.

\item If there are re-initializations of a given kernel instance, then
the same stream and block arguments must be passed to each invocation
of the initialization function.

\item It is illegal to take the address of a stream, block, or kernel
variable, except to pass it directly to a function described in this
document. Thus, these variables cannot be aliased other than streams
and blocks within extensions of the kernel base data type.

\item Stream, block, and kernel variables may not be part of
structures except for extensions of the kernel base data type.

\item Any function that contains a stream, block, or kernel variable
must NOT be called recursively or through a function pointer, even
indirectly (e.g. cannot be called from a recursive function, or called
from a function called from a recursive function, etc.).

\end{enumerate}

\subsection{Error Handling}

If any of the above restrictions are violated, or if there is an
operation that is disallowed by the API (e.g., peeking beyond the
capacity of a stream) then the behavior is undefined.  All error
detection, error handling, and error recovery are defined by the
implementation of the low-level compiler.  It is recommended that each
low-level compiler provides a mode where it checks assertions on the
restrictions above and fails cleanly (with an assertion error) if a
restriction is violated.  However, these assertions could be turned
off to obtain high performance if the low-level compiler trusts that
the high-level compiler generated safe code.

Note that the potential for undefined behavior is introduced for the
sake of performance.  If each architecture was required to verify the
restrictions above, the overhead would be prohibitive.

\subsection{Concept of Operations}

The following is an incomplete list of SVM/HLC/LLC expectations:

\renewcommand{\labelenumi}{\theenumi.}

\begin{enumerate}

\item One or more Thread processors may be working cooperatively on a
single program.

\item Streaming operations are performed by kernels that accept zero
or more input streams and produce zero or more output streams.  Stream
functions can also be called from control code.

\item Streams are sequences of data items, that may be stored in
memories or FIFOs.

\item Multiple kernels can be assigned to a streaming processor, but
only one RUNNING kernel can be assigned.  The rest must be WAITING for
a previous kernel to complete.

\item At any given time, Streams have a single producer and a single
consumer.  However, the contents of a Stream stored in memory can be
aliased such that several different Streams can read the same values.

\item The metadata file describes the available resource pool that the
HLC can work with.  This may not be the hardware's full capability -
some of the resources may be assigned to other programs outside the
compiler's immediate view.  The metadata file may describe a multichip
environment.

\item Multiple programs may be running on the PCA hardware,
concurrently.

\item The PCA elements that control the morphing of the processor both
set the metadata file resource environment for a compiled program and
apply directions from the HLC as to how the morphed assets are to be
used (i.e., which morph configuration was used for the specific
compiled program)

\item The HLC assesses the alternative morph configurations as defined
in the metadata file to choose the preferred set of resources to use
for the compilation.

\item A cost function could be provided for each of the SVM APIs as
for each resource instance - thus allowing the HLC to assess which
logical entities to assign to each resource.  The details of this cost
estimation will be addressed in the future.

\end{enumerate}

