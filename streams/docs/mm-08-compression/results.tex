\section{Experimental Evaluation}

To demonstrate the potential benefits of mapping into the compressed
domain, we implemented a few of our transformations as part of the
StreamIt compiler.  Our current implementation supports two
computational patterns: 1) transforming each individual element of a
stream (via a pop-1, push-1 filter), and 2) combining the elements of
two streams (via a roundrobin(1,1) joiner and a pop-2, push-1 filter).
The program can contain any number of filters that perform arbitrary
computations, so long as the I/O rates match these patterns.  While we
look forward to performing a broader implementation in future work,
these two building blocks are sufficient to express a number of useful
programs and to characterize the performance of the technique.

Our evaluation focuses on applications in digital video editing.
Given StreamIt source code that operates on pixels from each frame of
a video, the StreamIt compiler maps the computation into the
compressed domain and emits executable plugins for two popular video
editing tools, MEncoder and Blender.  The plugins are written for the
Apple Animation format (see Section~\ref{sec:formats-good}).

Our benchmarks fall into two categories: 1) pixel transformations,
such as brightness, contrast, and color inversion, which adjust pixels
within a single video, and 2) video compositing, in which one video is
combined with another as an overlay or mask.

The main results of our evaluation are:
\begin{itemize}

\item Operating directly on compressed data offers a speedup roughly
proportional to the compression factor in the resulting video.

\item For pixel transformations, speedups range from 2.5x to 471x,
  with a median of 17x.  Output sizes are within 0.1\% of input sizes
  and about 5\% larger (median) than a full re-compression.

\item For video compositing, speedups range from 1.1x to 32x, with a
  median of 6.6x.  Output files retain a sizable compression ratio
  (1.0x to 44x) and are about 52\% larger (median) than a full
  re-compression.

\end{itemize}
The following sections provide more details on our video workloads,
the evaluation of pixel transformations, and the evaluation of video
compositing.

\subsection{Video Workloads}

Our evaluation utilizes a suite of 12 video workloads that are
described in Table~\ref{tab:videos}; some of the videos are also
pictured in Figure~\ref{fig:videos}.  The suite represents three
common usage scenarios for lossless video formats: Internet
screencasts, computer animation, and digital television production.
While videos in each area are often rendered to a lossy format for
final distribution, lossless codecs are preferred during the editing
process to avoid accumulating compression artifacts.  All of our
source videos are in the Apple Animation format (described in
Section~\ref{sec:formats-good}), which is widely used by video editing
professionals~\cite[p.~106]{adobe-anim} \cite[p.~284]{harrington-anim}
\cite[p.~367]{long-anim} \cite[p.~280]{pogue-anim}.  The Apple
Animation format is also popular for capturing video from the screen
or camera, as the encoder is relatively fast.

Our suite of videos is assembled from a variety of realistic and
industry-standard sources.  The first screencast is an online demo of
an authentication generator for rails~\cite{auth-demo}; the second is
a PowerPoint presentation (including animations), captured using
Camtasia Studio.  As Internet content is often watermarked with a logo
or advertisement, we include two animated logos in the ``Internet
video'' category.  These logos are taken from Digital
Juice~\cite{digital-juice}, a standard source for professional
animations, and rendered to Apple Animation format using their
software.  The animated logos are rendered full-frame (with the logo
in the corner) because compositing operations in our testbed (Blender)
are done on equal-sized videos.

The computer animation clips are derived from Elephant's Dream, a
short film with entirely open-source content~\cite{elephants-dream};
our videos are rendered from source using Blender.  Finally, the
digital television content is also taken from a Digital Juice
library~\cite{digital-juice}.  The backgrounds represent
high-resolution, rotating backdrops as might appear in the
introduction to a program.  The mattes are black-and-white animations
that can be used to synthesize a smaller overlay (such as a frame or a
``lower third'', often used for text) from a full animated background
(see Figure~\ref{fig:videos}b for an example).

The videos exhibit a wide range of compression factors.  The
screencasts have very high compression ($\sim$400x-700x) because only
a small part of the screen (e.g., a mouse, menu, or PowerPoint bullet)
is changing on any given frame; the Apple Animation format compresses
the inter-frame redundancy.  The compression for {\tt anim-scene1} is
also in excess of 200x because motion is limited to a small animated
character.  The animated logos are the next most compressed
($\sim$50-70x), influenced largely by the constant blank region
outside the logo.  The computer animation content ($\sim$10-30x
compression) has a high level of detail but benefits from both
inter-frame and intra-frame redundancy, as some rendered regions have
constant color.  Next are the digital video mattes ($\sim$5-10x
compression), which have fine-grained motion in some sections.
Finally, the digital video backgrounds offer almost no compression
gains (1.0-1.1x) under Apple Animation, as they have pervasive motion
and detail across the entire frame.

The Apple Animation format supports various bit depths.  All of our
source videos use 32 bits per pixel, allocating a single byte for each
of the red, green, blue, and alpha channels.

\subsection{Pixel Transformations}

The pixel transformations adjust the color of each pixel in a uniform
way.  We evaluated three transformations:
\begin{itemize}
\item Brightness adjustment, which increases each RGB value by a value
of 20 (saturating at 255).
\item Contrast adjustment, which moves each RGB value away from the
center (128) by a factor of 1.2 (saturating at 0 and 255).

\item Color inversion, which subtracts each RGB value from 255 (useful
  for improving the readability of screencasts or for reversing the
  effect of video mattes).

\end{itemize}

We implemented each transformation as a single StreamIt filter that
transforms one pixel to another.  Because the filter has a pop rate of
one, it does not incur any alignment overhead.

\subsubsection{Setup}

The pixel transformations were compiled into plugins for MEncoder, a
popular command-line tool (bundled with MPlayer) for video decoding,
encoding, and filtering.  MEncoder relies on the FFMPEG library to
decode the Apple Animation format; as FFMPEG lacked an encoder for
this format, the authors implemented one.  Additionally, as MEncoder
lacks an interface for toggling only brightness or contrast, the
baseline configuration was implemented by the authors.

The baseline configuration performs decompression, pixel
transformations, then re-compression.  Because the main video frame is
updated incrementally by the decoder, the pixel transformations are
unable to modify the frame in place (otherwise pixels present across
frames would be transformed multiple times).  Thus, the baseline
transformation writes to a separate location in memory.  The optimized
configuration performs pixel transformations directly on the
compressed data, avoiding data expansion implied by decompression and
multiple frame buffers, before copying the data to the output file.

\begin{table*}[t]
\psfig{figure=table-pixel-speedup.eps,width=7.02in}
\caption{Results for pixel transformations.
\protect\label{tab:pixel-speedup}}
\end{table*}

Our evaluation platform is a dual-processor Intel Xeon (2.2 GHz) with
2 GB of RAM.  As all of our applications are single-threaded, the
second processor is not utilized.  For the timing measurements, we
execute each program five times and report the median user time.

\subsubsection{Results}

Detailed results for the pixel transformations appear in
Table~\ref{tab:pixel-speedup}.  Figure~\ref{fig:pixel-speedup}
illustrates the speedups, which range from 2.5x to 471x.  As
illustrated in Figure~\ref{fig:speedup-scatter}, the speedups are
closely correlated with the compression factor in the original video.
For the highly-compressed screencasts and {\tt anim-scene1}, speedups
range from 58x to 471x.  For the medium-compression computer
animations (including the animated logos), speedups range from 11x to
46x.  And for the low-compression digital television content, speedups
range from 2.5x to 8.9x.

There are two distinct reasons for the speedups observed.  First, by
avoiding the decompression stage, computing on compressed data reduces
the volume of data that needs to be stored, manipulated, and
transformed.  This savings is directly related to the compression
factor and is responsible for the upwards slope of the graph in
Figure~\ref{fig:speedup-scatter}.  Second, computing on compressed
data eliminates the algorithmic complexity of re-compression.  For the
Apple Animation format, the cost of compressing a given frame does not
increase with the compression factor (if anything, it decreases as
fewer pixels need a fine-grained encoding).  Thus, the baseline
devotes roughly constant runtime to re-compressing each video, which
explains the positive intercept in the graph of
Figure~\ref{fig:speedup-scatter}.

The impact of re-compression is especially evident in the digital
television examples.  Despite a compression factor of 1.0 on {\tt
digvid-background2}, our technique offers a 4.7x speedup on color
inversion.  Application profiling confirms that 
% NOTE: I profiled the hand-coded version rather than the StreamIt
% version.  It had a speedup of 5.5x rather than 4.7x
73\% of the baseline runtime is spent in the encoder; as this stage is
absent from the optimized version, it accounts for $1/(1-0.73) = 3.7$x
of the speedup.  The remaining speedup in this case is due to the
extra frame buffer (and associated memory operations) in the
decompression stage of the baseline configuration.
%
%(Don't really understand these profiling numbers yet)
%
%Note that videos with larger compression ratios spend
%a smaller fraction of time in the encoder; for example, color
%inversion on {\tt screencast1-demo} spends only 5\% of its time in the
%encoder (and 65\% of the time doing the transformation itself).

Another important aspect of the results is the size of the output
files produced.  Apart from the first frame of a video\footnote{In the
Apple Animation format, the first frame is encoded as if the previous
frame was black.  Thus, adjusting the color of black pixels in the
first frame may increase the size of the file, as it removes
inter-frame redundancy.}, performing pixel transformations directly on
compressed data will never increase the size of the file.  This is
illustrated in the middle columns of Table~\ref{tab:pixel-speedup}, in
which the output sizes are mostly equal to the input sizes (up to 2
decimal places).  The only exception is contrast adjustment on {\tt
anim-scene1}, in which the output is 2\% smaller than the input due to
variations in the first frame; for the same reason, some cases
experience a 0.1\% increase in size (not visisble in the table).

\begin{figure}[t]
\psfig{figure=graph-speedup-pixel.eps,width=3.35in}
\caption{Speedup on pixel transformations.
\protect\label{fig:pixel-speedup}}
\end{figure}

Though computing on compressed data has virtually no effect on the
file size, there are some cases in which the pixel transformation
increases the redundancy in the video and an additional re-compression
step could compress the output even further than the original input.
This potential benefit is illustrated in the last three columns of
Table~\ref{tab:pixel-speedup}, which track the output size of the
baseline configuration (including a re-compression stage) versus the
original input.  For the inverse transformation, no additional
compression is possible because inverse is a 1-to-1 transform: two
pixels have equal values in the output file if and only if they have
equal values in the input file.  However, the brightness and contrast
transformations may map distinct input values to the same output
value, due to the saturating arithmetic.  In such cases, the
re-compression stage can shrink the file to as low as 0.75x
(brightness) and 0.35x (contrast) its original size.  These are
extreme cases in which many pixels are close to the saturating point;
the median re-compression (across brightness and contrast) is only
10\%.

To achieve the minimal file size whenever possible, future work will
explore integrating a lightweight re-compression stage into the
compressed processing technique.  Because most of the compression is
already in place, it should be possible to improve the compression
ratio without running the full encoder (e.g., run-length encoded
regions can be extended without being rediscovered).  

% (The following comment didn't make sense -- to do the full
% re-compression, you would need to start from a decompressed stream,
% which has the full data volume.)
%
% Even running the full encoding algorithm (in place on the compressed
% data) may leave us with significant speedups, as much of the
% improvement comes from decreased data volume rather than decreased
% re-compression cost.

\subsection{Video Compositing}

In video compositing, two videos are combined using a specific
function to derive each output pixel from a pair of input pixels (see
Figure~\ref{fig:videos}).  In the case of subtitling, animated logos,
and computer graphics, an alpha-under transformation is common; it
overlays one video on top of another using the transparency
information in the alpha channel.  In applying an animated matte, the
videos are combined with a multiply operation, thereby masking the
output according to the brightness of the matte.  For our experiments,
we generated composites using each foreground/background pair within a
given application area, yielding a total of 12 composites.

In StreamIt, we implemented each compositing operation as a
roundrobin(1,1) joiner (to interleave the streams) followed by a
filter (to combine the pixel values).  The intuition of the
compressed-domain execution is that if both streams have the same kind
of repeat (inter-frame or intra-frame), then the repeat is copied
directly to the output.  If they have different kinds of repeats, or
if one stream is uncompressed, then both streams are uncompressed.

\subsubsection{Setup}

The compositing operations were compiled into plugins for Blender, a
popular tool for modeling, rendering, and post-processing 3-D
animations.  Blender has logged 1.8 million downloads in the last
year~\cite{blender-stats} and was used in the production of Spiderman
2~\cite{blender-wikipedia}.  Like MEncoder, Blender relies on the
FFMPEG library for video coding, so we utilize the same Apple
Animation decoder/encoder as in the pixel transformations.

As Blender already includes support for video compositing, we use its
implementation as our baseline.  The compositing operations have
already been hand-tuned for performance; the implementation of
alpha-under includes multiple shortcuts, unrolled loops, and the
following comment: ``this complex optimalisation is because the
'skybuf' can be crossed in''.  We further improved the baseline
performance by patching other parts of the Blender source base, which
were designed around 3-D rendering and are more general than needed
for video editing.  We removed two redundant vertical flips for each
frame, two redundant BGRA-RGBA conversions, and redundant memory
allocation/deallocation for each frame.
% did not mention removal of O(frames^2) linked list search, although 
% this fell out of removing the redundant memory deallocation

\begin{figure}[t]
\vspace{1.5pt}
\psfig{figure=graph-speedup-scatter.eps,width=3.47in}
\caption{Speedup vs. compression factor for all transformations.
\protect\label{fig:speedup-scatter}}
\vspace{-7.5pt}
\end{figure}

\begin{figure}[t]
\begin{minipage}{1.1in}
\vspace{-4pt}
\psfig{figure=blender-background1-frame5.eps,width=1.09in}
\vspace{2pt}
\end{minipage}
\begin{minipage}{1.1in}
\vspace{-4pt}
\psfig{figure=blender-foreground2-frame5.eps,width=1.09in}
\vspace{2pt}
\end{minipage}
\begin{minipage}{1.1in}
\vspace{-4pt}
\psfig{figure=blender-composite-frame5.eps,width=1.09in}
\vspace{2pt}
\end{minipage}

{\small ~~~~~~~~~~anim-scene1~~~~~~~~~+~~~~~~anim-character2~~~~~~=~~~~~~video composite}

\begin{center}
\vspace{-3pt}
(a) Computer animation composite (alpha-under)
\end{center} \vspace{3pt}

\begin{minipage}{1.1in}
\psfig{figure=digvid-background1.eps,width=1.09in}
\vspace{2pt}
\end{minipage}
\begin{minipage}{1.1in}
\psfig{figure=digvid-matte1-frame.eps,width=1.09in}
\vspace{2pt}
\end{minipage}
\begin{minipage}{1.1in}
\psfig{figure=digvid-composite.eps,width=1.09in}
\vspace{2pt}
\end{minipage}

{\small ~~~~digvid-background1~~+~~~~~digvid-matte-frame~~~=~~~~~video composite}

\begin{center}
\vspace{-3pt}
(b) Digital television composite (multiply)
\end{center}
\vspace{-3pt}
\caption{Examples of video compositing operations.\protect\label{fig:videos}}
\vspace{-3pt}
\end{figure}

\begin{table*}[t]
\vspace{-1.5\baselineskip}
\begin{minipage}{0.6in}
\mbox{~}
\end{minipage}
\psfig{figure=table-composite-speedup.eps,width=5.8in}
\caption{Results for composite transformations.
\protect\label{tab:composite-speedup}}
\end{table*}

Our optimized configuration operates in the compressed domain.
Outside of the auto-generated plugin, we patched three frame-copy
operations in
% actually four frame-copy operations, though one of them is a no-op
the Blender source code to copy only the compressed frame data rather
than the full frame dimensions.

\subsubsection{Results}

Full results for the compositing operations appear in
Table~\ref{tab:composite-speedup}.  Figure~\ref{fig:composite-speedup}
illustrates the speedups, which range from 1.1x to 32x.  As in the
case of the pixel transformations, the speedups are closely correlated
with the compression factor of the resulting videos, a relationship
depicted in Figure~\ref{fig:speedup-scatter}.  The highly-compressed
screencasts enjoy the largest speedups (20x-32x), the computer
animations have intermediate speedups (5x-9x), while the digital
television content has negligible speedups (1.1x-1.4x).  Overall, the
speedups on video compositing (median = 6.6x) are lower than the pixel
transformations (median = 17x); this is because the compression
achieved on composite videos is roughly proportional to the minimum
compression across the two input files.

\begin{figure}[t]
\psfig{figure=graph-speedup-composite.eps,width=3.35in}
\caption{Speedup on composite transformations.
\protect\label{fig:composite-speedup}}
\end{figure}

As for the pixel transformations, the composite videos produced by the
compressed processing technique would sometimes benefit from an
additional re-compression stage.  The last three columns in
Table~\ref{tab:composite-speedup} quantify this benefit by comparing
the compression factors achieved by compressed processing and normal
processing (including a re-compression step).  For screencasts and
computer animations, compressed processing preserves a sizable
compression factor (7.7x-44x), though the full re-compression can
further reduce file sizes by 1.2x to 1.6x.  For digital television,
the matting operations introduce a large amount of redundancy (black
regions), thereby enabling the re-compression stage to shrink the file
by 1.8x to 5.4x over the compressed processing technique.

Even if the composite transformation does not introduce any new
redundancy in the video, the compressed processing technique may
increase file sizes by ignoring a specific kind of redundancy in the
inputs.  Suppose that in the first frame, both inputs are 100\% black,
while in the second frame, one input is 100\% black and the other is
100\% white.  If the inputs are averaged, the second frame of output
will be 100\% gray and can be run-length encoded within the frame.
However, because the inputs have different kinds of redundancy on the
second frame (one is inter-frame, the other is intra-frame), the
technique is unable to detect the intra-frame redundancy in the output
and will instead produce N distinct pixels (all of them gray).  We
believe that this effect is small in practice, though we have yet to
quantify its impact in relation to the new redundancy introduced by a
transformation.  Future work will explore alternate data structures
for the compressed processing technique that may be able to preserve
this redundancy with low overhead.
