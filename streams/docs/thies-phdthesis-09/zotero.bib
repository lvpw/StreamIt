@article{thies_step_2007,
	title = {A step towards unifying schedule and storage optimization},
	volume = {29},
	url = {http://portal.acm.org/citation.cfm?id=1286821.1286825\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/1286821.1286825},
	abstract = {We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal (one-dimensional affine) schedules. We consider storage mappings that collapse one dimension of a multidimensional array, and programs that are in a single assignment form and accept a one-dimensional affine schedule. Our method combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.},
	number = {6},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {William Thies and Frédéric Vivien and Saman Amarasinghe},
	year = {2007},
	keywords = {affine recurrence equations,affine scheduling,automatic parallelization,occupancy vectors,polyhedral model,storage optimization},
	pages = {34}
},

@inproceedings{derenzi_e-imci:_2008,
	address = {Florence, Italy},
	title = {E-imci: improving pediatric health care in low-income countries},
	isbn = {978-1-60558-011-1},
	url = {http://portal.acm.org/citation.cfm?id=1357054.1357174\&jmp=cit\&coll=GUIDE\&dl=GUIDE},
	doi = {10.1145/1357054.1357174},
	abstract = {Every year almost 10 million children die before reaching the age of five despite the fact that two-thirds of these deaths could be prevented by effective low-cost interventions. To combat this, the World Health Organization (WHO) and UNICEF developed the Integrated Management of Childhood Illness (IMCI) treatment algorithms.},
	booktitle = {Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems},
	publisher = {ACM},
	author = {Brian DeRenzi and Neal Lesh and Tapan Parikh and Clayton Sims and Werner Maokla and Mwajuma Chemba and Yuna Hamisi and David S hellenberg and Marc Mitchell and Gaetano Borriello},
	year = {2008},
	keywords = {automation,child health,imci,pda,tanzania},
	pages = {753--762}
},

@misc{_graphviz_????,
	title = {Graphviz},
	url = {http://graphviz.org/},
	howpublished = {http://graphviz.org/}
},

@article{engebretsen_acceptance_2005,
	title = {Acceptance of Information Technology by Health Research Projects in Low-income Countries Intention to use and Acceptance of},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.4003},
	doi = {10.1.1.108.4003},
	author = {Terje Engebretsen},
	year = {2005}
},

@inproceedings{buck_heterogeneous_2000,
	title = {Heterogeneous modeling and simulation of embedded systems in El Greco},
	abstract = {This paper describes the functional specification and verification portions of El Greco, a system for high-level, heterogeneous functional specification, efficient compiled simulation, and software and hardware implementation. Specifications in the form of dataflow graphs, hierarchical finite state machines, or a mixture, are supported. These specifications can be arbitrarily nested, as in Ptolemy. When dataflow graphs are placed in a control context, the graph execution is fully controllable; its execution can be restarted or suspended and parameters can be changed. We describe system modeling and simulation generation in El Greco and compare to other approaches.},
	booktitle = {Hardware/Software Codesign, 2000. CODES 2000. Proceedings of the Eighth International Workshop on},
	author = {J. Buck and R. Vaidyanathan},
	year = {2000},
	keywords = {compiled simulation,data flow graphs,dataflow graphs,digital simulation,El Greco,embedded systems,finite state machines,formal specification,formal verification,functional specification,hardware-software codesign,simulation generation,verification},
	pages = {142--146}
},

@article{bilsen_cycle-static_1996,
	title = {Cycle-static dataflow},
	volume = {44},
	issn = {1053-587X},
	doi = {10.1109/78.485935},
	abstract = {We present cycle-static dataflow (CSDF), which is a new model for the specification and implementation of digital signal processing algorithms. The CSDF paradigm is an extension of synchronous dataflow that still allows for static scheduling and, thus, a very efficient implementation of an application. In comparison with synchronous dataflow, it is more versatile because it also supports algorithms with a cyclically changing, but predefined, behavior. Our examples show that this capability results in a higher degree of parallelism and, hence, a higher throughput, shorter delays, and less buffer memory. Moreover, they indicate that CSDF is essential for modelling prescheduled components, like application-specific integrated circuits. Besides introducing the CSDF paradigm, we also derive necessary and sufficient conditions for the schedulability of a CSDF graph. We present and compare two methods for checking the liveness of a graph. The first one checks the liveness of loops, and the second one constructs a single-processor schedule for one iteration of the graph. Once the schedulability is tested, a makespan optimal schedule on a multiprocessor can be constructed. We also introduce the heuristic scheduling method of our graphical rapid prototyping environment (GRAPE) },
	number = {2},
	journal = {Signal Processing, IEEE Transactions on},
	author = {G. Bilsen and M. Engels and R. Lauwereins and J. Peperstraete},
	year = {1996},
	keywords = {application specific integrated circuits,application-specific integrated circuits,buffer memory,CSDF graph,CSDF paradigm,cycle-static dataflow,data flow graphs,digital signal processing algorithms,graph liveness,graph schedulability,graphical rapid prototyping environment,heuristic scheduling method,iteration,loops,makespan optimal schedule,multiprocessor,necessary conditions,parallel algorithms,processor scheduling,programming environments,signal processing,single-processor schedule,software prototyping,specification,static scheduling,sufficient conditions,synchronous dataflow,throughput},
	pages = {397--408}
},

@techreport{ackerman_val--value-oriented_1979,
	title = {VAL--A value-oriented algorithmic language},
	number = {MIT-LCS-TR-218},
	institution = {Masscahusetts Institute of Technology},
	author = {W. Ackerman and Jack Dennis},
	month = jun,
	year = {1979}
},

@misc{_lcs_????,
	title = {LCS Publication - MIT-LCS-TR-154},
	url = {http://publications.csail.mit.edu/lcs/specpub.php?id=722},
	howpublished = {http://publications.csail.mit.edu/lcs/specpub.php?id=722}
},

@article{thies_common_2002,
	title = {A common machine language for grid-based architectures},
	volume = {30},
	url = {http://portal.acm.org/citation.cfm?id=571666.571673\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/571666.571673},
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
	number = {3},
	journal = {SIGARCH Comput. Archit. News},
	author = {William Thies and Michal Karczmarek and Michael Gordon and David Maze and Jeremy Wong and Henry Hoffmann and Matthew Brown and Saman Amarasinghe},
	year = {2002},
	pages = {13--14}
},

@inproceedings{bilsen_cyclo-static_1995,
	title = {Cyclo-static data flow},
	volume = {5},
	isbn = {1520-6149},
	doi = {10.1109/ICASSP.1995.479579},
	abstract = {The high sample-rates involved in many DSP-applications, require the use of static schedulers wherever possible. The construction of static schedules however is classically limited to applications that fit in the synchronous data flow model. In this paper we present cyclo-static data flow as a model to describe applications with a cyclically changing behaviour. We give both a necessary and sufficient condition for the existence of a static schedule for a cyclo-static data flow graph and show how such a schedule can be constructed. The example of a video encoder is used to illustrate the importance of cyclo-static data flow for real-life DSP-systems},
	booktitle = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on},
	author = {G. Bilsen and M. Engels and R. Lauwereins and J.A. Peperstraete},
	year = {1995},
	keywords = {cyclically changing behaviour,cyclo-static data flow graph,data flow graphs,DSP systems,DSP-applications,high sample-rates,necessary condition,processor scheduling,signal sampling,static schedule,static schedulers,sufficient condition,synchronous data flow model,video coding,video encoder,video equipment},
	pages = {3255--3258 vol.5}
},

@phdthesis{aziz_image-based_2007,
	type = {M.Eng. Thesis},
	title = {Image-Based Motion Estimation in a Stream Programming Language},
	url = {http://cag.lcs.mit.edu/commit/papers/07/aziz-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Abdulbasier Aziz},
	month = jun,
	year = {2007}
},

@article{amarasinghe_language_2005,
	title = {Language and compiler design for streaming applications},
	volume = {33},
	url = {http://portal.acm.org/citation.cfm?id=1152077.1152089\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	abstract = {High-performance streaming applications are a new and distinct domain of programs that is increasingly important. The StreamIt language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreamIt compiler aims to improve the performance of streaming applications via stream-specific analysis and optimizations. In this paper, we motivate, describes and justify the StreamIt language which include a structured model of streams, a messaging system for control, and a natural textual syntax.},
	number = {2},
	journal = {Int. J. Parallel Program.},
	author = {Saman Amarasinghe and Michael I. Gordon and Michal Karczmarek and Jasper Lin and David Maze and Rodric M. Rabbah and William Thies},
	year = {2005},
	keywords = {parallelizing compiler,productivity,stream computing,streamit,tiled-processor architectures},
	pages = {261--278}
},

@article{karp_properties_1966,
	title = {Properties of a Model for Parallel Computations: Determinacy, Termination, Queueing},
	volume = {14},
	url = {http://link.aip.org/link/?SMM/14/1390/1},
	number = {6},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Richard M. Karp and Rayamond E. Miller},
	year = {1966},
	pages = {1390--1411}
},

@article{nikhil_computation_????,
	title = {Computation Structures Group},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4920},
	doi = {10.1.1.18.4920},
	author = {Rishiyur S Nikhil and Rishiyur S Nikhil}
},

@inbook{ellson_graphviz_2002,
	title = {Graphviz— Open Source Graph Drawing Tools},
	url = {http://dx.doi.org/10.1007/3-540-45848-4\_57},
	abstract = {Graphviz is a heterogeneous collection of graph drawing tools containing batch layout programs (dot, neato, fdp, twopi); a platform for incremental layout (Dynagraph); customizable graph editors (dotty, Grappa); a server for including graphs in Web pages (WebDot); support for graphs as COM objects (Montage); utility programs useful in graph visualization; and libraries for attributed graphs. The software is available under an Open Source license. The article[1] provides a detailed description of the package. },
	journal = {Graph Drawing},
	author = {John Ellson and Emden Gansner and Lefteris Koutsofios and Stephen North and Gordon Woodhull},
	year = {2002},
	pages = {594--597}
},

@misc{_using_2005,
	title = {Using Cell Phones to Improve Treatment of Cape Town Tuberculosis Patients: An Evaluation},
	url = {http://www.idrc.ca/en/ev-87881-201-1-DO\_TOPIC.html},
	year = {2005},
	note = {This evaluation of successes and failures in an innovative pilot project in Cape Town, South Africa, has produced guidelines for delivering effective, low cost programs that could improve cure rates by using cell phones to remind patients to take their medication.},
	keywords = {toread},
	howpublished = {http://www.idrc.ca/en/ev-87881-201-1-DO\_TOPIC.html}
},

@article{ashcroft_lucidnonprocedural_1977,
	title = {Lucid, a nonprocedural language with iteration},
	volume = {20},
	url = {http://portal.acm.org.libproxy.mit.edu/citation.cfm?id=359636.359715},
	doi = {10.1145/359636.359715},
	abstract = {Lucid is a formal system in which programs can be written and proofs of programs carried out. The proofs are particularly easy to follow and straightforward to produce because the statements in a Lucid program are simply axioms from which the proof proceeds by (almost) conventional logical reasoning, with the help of a few axioms and rules of inference for the special Lucid functions. As a programming language, Lucid is unconventional because, among other things, the order of statements is irrelevant and assignment statements are equations. Nevertheless, Lucid programs need not look much different than iterative programs in a conventional structured programming language using assignment and conditional statements and loops.},
	number = {7},
	journal = {Commun. ACM},
	author = {E. A. Ashcroft and W. W. Wadge},
	year = {1977},
	keywords = {formal systems,iteration,program proving,semantics,structured programming},
	pages = {519--526}
},

@inproceedings{henderson_lazy_1976,
	address = {Atlanta, Georgia},
	title = {A lazy evaluator},
	url = {http://portal.acm.org/citation.cfm?id=811543\&dl=GUIDE\&dl=ACM},
	doi = {10.1145/800168.811543},
	abstract = {A different way to execute pure LISP programs is presented. It delays the evaluation of parameters and list structures without ever having to perform more evaluation steps than the usual method. Although the central idea can be found in earlier work this paper is of interest since it treats a rather well-known language and works out an algorithm which avoids full substitution. A partial correctness proof using Scott-Strachey semantics is sketched in a later section.},
	booktitle = {Proceedings of the 3rd ACM SIGACT-SIGPLAN symposium on Principles on programming languages},
	publisher = {ACM},
	author = {Peter Henderson and Jr James H. Morris},
	year = {1976},
	pages = {95--103}
},

@phdthesis{lamb_linear_2003,
	type = {M.Eng. Thesis},
	title = {Linear Analysis and Optimization of Stream Programs},
	url = {http://cag.lcs.mit.edu/commit/papers/03/aalamb-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Andrew A. Lamb},
	month = may,
	year = {2003}
},

@article{hanscom_computerized_2002,
	title = {Computerized questionnaires and the quality of survey data},
	volume = {27},
	issn = {1528-1159},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12195074},
	doi = {12195074},
	abstract = {STUDY DESIGN: A retrospective data quality analysis was conducted. OBJECTIVE: To compare missing response rates and internal consistency between computerized and paper surveys administered to spine patients. SUMMARY OF BACKGROUND DATA: Computerized patient surveys have been shown to offer numerous advantages over traditional paper surveys. It has been assumed that computerized surveys also improve data quality, but quantitative comparisons have not been made. METHODS: Between January 1998 and December 2000, approximately 3500 computerized questionnaires and 15,000 paper questionnaires containing the MOS 36-Item Short-Form Health Survey (SF-36) and the Oswestry Low Back Pain Disability Questionnaire were administered in the National Spine Network. Missing response rates and the Response Consistency Index (RCI) were compared between computerized and paper questionnaire data. RESULTS: Computer surveys had approximately half the missing response rate of paper surveys. For the SF-36, the computer survey had 1.7\% missing, as compared with 3.3\% missing on paper (P {\textless} 0.001). For the Oswestry, the computer survey had 2.9\% missing, as compared with 6\% missing on paper (P {\textless} 0.001). Whereas 84\% of the SF-36 surveys and 85\% of the Oswestry surveys collected by computer were completely filled out (no missing responses), only 68\% of the SF-36 surveys (P {\textless} 0.001) and 77\% of the Oswestry surveys (P {\textless} 0.001) collected on paper were completely filled out. The SF-36 data collected by computer had better internal consistency than the paper-form data, with average Response Consistency Index scores of 0.12 and 0.16, respectively (P = 0.001). CONCLUSIONS: Superior response rates and higher internal consistency suggest that computerized survey systems improve data quality, and may enhance instrument validity for commonly used measures of spine patient health.},
	number = {16},
	journal = {Spine},
	author = {Brett Hanscom and Jon D Lurie and Karen Homa and James N Weinstein},
	month = aug,
	year = {2002},
	note = {PMID: 12195074},
	keywords = {Cohort Studies,Computer Simulation,Confounding Factors (Epidemiology),Disability Evaluation,Female,Health Status,Health Status Indicators,Humans,Logistic Models,Low Back Pain,Male,Middle Aged,Multivariate Analysis,Quality Control,Questionnaires,Reproducibility of Results,Research Design,Retrospective Studies},
	pages = {1797--801}
},

@phdthesis{wong_algorithms_1989,
	title = {Algorithms for systolic array synthesis},
	url = {http://portal.acm.org/citation.cfm?id=916449},
	school = {Yale University},
	author = {Yiwan Wong},
	year = {1989},
	pages = {192}
},

@techreport{nikhil_id_1991,
	title = {ID Language Reference Manual Version 90.1},
	url = {http://citeseer.ist.psu.edu/527820.html},
	abstract = {Id is a general-purpose parallel programming language designed by members of the Computation Structures Group in MIT's Laboratory for Computer Science, and is used for programming dataflow and other parallel machines.},
	number = {Computation Structures Group Memo 284-2},
	institution = {Masscahusetts Institute of Technology},
	author = {Rishiyur S.  Nikhil},
	month = jul,
	year = {1991},
	keywords = {Rishiyur S. Nikhil ID Language Reference Manual Version 90.1,}
},

@techreport{mcgraw_sisal:_1985,
	type = {Language Reference Manual, Version 1.2},
	title = {SISAL: streams and iteration in a single assignment language},
	institution = {Lawrence Livermore National Laboratory, California},
	author = {J. McGraw and S. Skedzielewski and S. Allan and R. Oldhoeft and J. Glauert and C.C. Kirkham and B. Noyce and R. Thomas},
	month = mar,
	year = {1985}
},

@book{armstrong_concurrent_1993,
	title = {Concurrent Programming in Erlang},
	isbn = {0132857928},
	publisher = {Prentice Hall},
	author = {Joe Armstrong and Robert Virding and Mike Williams},
	month = mar,
	year = {1993},
	pages = {350}
},

@article{_telemedicine:_????,
	title = {Telemedicine: Telemedicine comes home},
	issn = {0013-0613},
	url = {http://www.economist.com/science/tq/displaystory.cfm?story\_id=11482580\&fsrc=RSS},
	abstract = {Medicine: Telemedicine permits remote consultations by video link and even remote surgery, but its future may lie closer to home},
	journal = {The Economist},
	month = jun,
	keywords = {toread}
},

@article{murata_petri_1989,
	title = {Petri nets: Properties, analysis and applications},
	volume = {77},
	issn = {0018-9219},
	doi = {10.1109/5.24143},
	abstract = {Starts with a brief review of the history and the application areas considered in the literature. The author then proceeds with introductory modeling examples, behavioral and structural properties, three methods of analysis, subclasses of Petri nets and their analysis. In particular, one section is devoted to marked graphs, the concurrent system model most amenable to analysis. Introductory discussions on stochastic nets with their application to performance modeling, and on high-level nets with their application to logic programming, are provided. Also included are recent results on reachability criteria. Suggestions are provided for further reading on many subject areas of Petri nets},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {T. Murata},
	year = {1989},
	keywords = {behavioural properties,concurrent system model,high-level nets,logic programming,marked graphs,performance modeling,Petri nets,reachability criteria,stochastic nets,stochastic processes,structural properties,subclasses},
	pages = {541--580}
},

@inproceedings{armstrong_history_2007,
	address = {San Diego, California},
	title = {A history of Erlang},
	isbn = {978-1-59593-766-X},
	url = {http://portal.acm.org.libproxy.mit.edu/citation.cfm?doid=1238844.1238850},
	doi = {10.1145/1238844.1238850},
	abstract = {Erlang was designed for writing concurrent programs that "run forever." Erlang uses concurrent processes to structure the program. These processes have no shared memory and communicate by asynchronous message passing. Erlang processes are lightweight and belong to the language, not the operating system. Erlang has mechanisms to allow programs to change code "on the fly" so that programs can evolve and change as they run. These mechanisms simplify the construction of software for implementing non-stop systems.},
	booktitle = {Proceedings of the third ACM SIGPLAN conference on History of programming languages},
	publisher = {ACM},
	author = {Joe Armstrong},
	year = {2007},
	pages = {6--1-6-26}
},

@inproceedings{ellsworth_accelerating_2003,
	title = {Accelerating large data analysis by exploiting regularities},
	doi = {10.1109/VISUAL.2003.1250420},
	abstract = {We present techniques for discovering and exploiting regularity in large curvilinear data sets. The data can be based on a single mesh or a mesh composed of multiple submeshes (also known as zones). Multi-zone data are typical in Computational Fluid Dynamics (CFD) simulations. Regularities include axis-aligned rectilinear and cylindrical meshes as well as cases where one zone is equivalent to a rigid body transformation of another. Our algorithms can also discover rigid-body motion of meshes in time-series data. Next, we describe a data model where we can utilize the results from the discovery process in order to accelerate large data visualizations. Where possible, we replace general curvilinear zones with rectilinear or cylindrical zones. In rigid-body motion cases, we replace a time-series of meshes with a transformed mesh object where a reference mesh is dynamically transformed based on a given time value in order to satisfy geometry requests, on demand. The data model enables us to make these substitutions and dynamic transformations transparently with respect to the visualization algorithms. We present results with large data sets where we combine our mesh replacement and transformation techniques with out-of-core paging in order to achieve analysis speedups ranging from 1.5 to 2.},
	booktitle = {Visualization, 2003. VIS 2003. IEEE},
	author = {D. Ellsworth and P.J. Moran},
	year = {2003},
	keywords = {C++ language,CFD simulation,computational fluid dynamics,curvilinear data sets,cylindrical meshes,data analysis,data models,data visualisation,data visualization,demand-driven evaluation,large data sets,mesh replacement,mesh replacements,mesh rigid-body motion discovery,mesh transformation,multizone data,object-oriented methods,object-oriented programming,out-of-core paging,regularity finding,rigid body motion,scientific visualization,time series,time-series data,visualization algorithms},
	pages = {561--568}
},

@inproceedings{bhattacharyya_self-timed_1996,
	title = {Self-Timed Resynchronization: A Post-Optimization for Static Multiprocessor Schedules},
	isbn = {0-8186-7255-2},
	url = {http://portal.acm.org.libproxy.mit.edu/citation.cfm?id=645606.660850\&coll=GUIDE\&dl=GUIDE},
	abstract = {In a shared-memory multiprocessor system, it is possible that certain synchronization operations are redundant - that is, their corresponding sequencing requirements are enforced completely by other synchronizations in the system - and can be eliminated without compromising correctness. This paper addresses the problem of adding new synchronization operations in a multiprocessor implementation in such a way that the number of original synchronizations that consequently become redundant significantly exceeds the number of new synchronizations. We refer to this approach to reducing synchronization overhead as resynchronization. In this paper we formally define the resynchronization problem, we show that optimal resynchronization is NP-hard, and we propose a family of heuristics for this problem. Finally we present a practical example where resynchronization is useful.},
	booktitle = {Proceedings of the 10th International Parallel Processing Symposium},
	publisher = {IEEE Computer Society},
	author = {Shuvra S. Bhattacharyya and Sundararajan Sriram and Edward A. Lee},
	year = {1996},
	keywords = {embedded systems,inter-processor communication overhead,multiprocessors,scheduling,static schedules,synchronization},
	pages = {199--205}
},

@article{murthy_shared_2001,
	title = {Shared buffer implementations of signal processing systems using lifetime analysis techniques},
	volume = {20},
	issn = {0278-0070},
	abstract = {There has been a proliferation of block-diagram environments for specifying and prototyping digital signal processing (DSP) systems. These include tools from academia such as Ptolemy and commercial tools such as DSPCanvas from Angeles Design Systems, signal processing work system (SPW) from Cadence, and COSSAP from Synopsys. The block diagram languages used in these environments are usually based on dataflow semantics because various subsets of dataflow have proven to be good matches for expressing and modeling signal processing systems. In particular, synchronous dataflow (SDF) has been found to be a particularly good match for expressing multirate signal processing systems. One of the key problems that arises during synthesis from an SDF specification is scheduling. Past work on scheduling from SDF has focused on optimization of program memory and buffer memory under a model that did not exploit sharing opportunities. In this paper, we build on our previously developed analysis and optimization framework for looped schedules to formally tackle the problem of generating optimally compact schedules for SDF graphs. We develop techniques for computing these optimally compact schedules in a manner that also attempts to minimize buffering memory under the assumption that buffers will be shared. This results in schedules whose data memory usage is drastically lower than methods in the past have achieved. The method we use is that of lifetime analysis; we develop a model for buffer lifetimes in SDF graphs and develop scheduling algorithms that attempt to generate schedules that minimize the maximum number of live tokens under the particular buffer lifetime model. We develop several efficient algorithms for extracting the relevant lifetimes from the SDF schedule. We then use the well-known first-fit heuristic for packing arrays efficiently into memory. We report extensive experimental results on applying these techniques to several practical SDF systems and show improvements that average 50\% over previous techniques, with some systems exhibiting up to an 83\% improvement over previous techniques },
	number = {2},
	journal = {Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on},
	author = {P.K. Murthy and S.S. Bhattacharyya},
	year = {2001},
	keywords = {block-diagram environments,buffer lifetime model,buffer lifetimes,buffer storage,data flow computing,data memory usage,dataflow semantics,digital signal processing chips,digital signal processing systems,dynamic programming,first-fit heuristic,graph colouring,lifetime analysis techniques,live tokens,multirate signal processing,optimally compact schedules,program compilers,scheduling,shared buffer implementations,shared memory systems,storage allocation,synchronous dataflow},
	pages = {177--198}
},

@article{kushniruk_technology_2005,
	title = {Technology induced error and usability: the relationship between usability problems and prescription errors when using a handheld application},
	volume = {74},
	issn = {1386-5056},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16043081},
	doi = {S1386-5056(05)00011-0},
	abstract = {This paper describes an innovative approach to the evaluation of a handheld prescription writing application. Participants (10 physicians) were asked to perform a series of tasks involving entering prescriptions into the application from a medication list. The study procedure involved the collection of data consisting of transcripts of the subjects who were asked to "think aloud" while interacting with the prescription writing program to enter medications. All user interactions with the device were video and audio recorded. Analysis of the protocols was conducted in two phases: (1) usability problems were identified from coding of the transcripts and video data, (2) actual errors in entering prescription data were also identified. The results indicated that there were a variety of usability problems, with most related to interface design issues. In examining the relationship between usability problems and errors, it was found that certain types of usability problems were closely associated with the occurrence of specific types of errors in prescription of medications. Implications for identifying and predicting technology-induced error are discussed in the context of improving the safety of health care information systems.},
	number = {7-8},
	journal = {International Journal of Medical Informatics},
	author = {Andre W Kushniruk and Marc M Triola and Elizabeth M Borycki and Ben Stein and Joseph L Kannry},
	month = aug,
	year = {2005},
	note = {PMID: 16043081},
	keywords = {Adult,Aged,Audiovisual Aids,Computers, Handheld,Humans,Medication Errors,Middle Aged,Prescriptions, Drug,United States,User-Computer Interface},
	pages = {519--26}
},

@phdthesis{greif_semantics_1975,
	type = {Ph.D. Thesis},
	title = {Semantics of Communicating Parallel Processes},
	school = {Massachusetts Insitute of Technology},
	author = {Irene Greif},
	year = {1975},
	keywords = {actors}
},

@inproceedings{kudlur_orchestratingexecution_2008,
	address = {Tucson, AZ, USA},
	title = {Orchestrating the execution of stream programs on multicore platforms},
	isbn = {978-1-59593-860-2},
	url = {http://portal.acm.org.libproxy.mit.edu/citation.cfm?id=1375581.1375596\&jmp=cit\&coll=GUIDE\&dl=GUIDE},
	doi = {10.1145/1375581.1375596},
	abstract = {While multicore hardware has become ubiquitous, explicitly parallel programming models and compiler techniques for exploiting parallelism on these systems have noticeably lagged behind. Stream programming is one model that has wide applicability in the multimedia, graphics, and signal processing domains. Streaming models execute as a set of independent actors that explicitly communicate data through channels. This paper presents a compiler technique for planning and orchestrating the execution of streaming applications on multicore platforms. An integrated unfolding and partitioning step based on integer linear programming is presented that unfolds data parallel actors as needed and maximally packs actors onto cores. Next, the actors are assigned to pipeline stages in such a way that all communication is maximally overlapped with computation on the cores. To facilitate experimentation, a generalized code generation template for mapping the software pipeline onto the Cell architecture is presented. For a range of streaming applications, a geometric mean speedup of 14.7x is achieved on a 16-core Cell platform compared to a single core.},
	booktitle = {Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation},
	publisher = {ACM},
	author = {Manjunath Kudlur and Scott Mahlke},
	year = {2008},
	keywords = {cell processor,multicore,software pipelining,stream programming,streamit},
	pages = {114--124}
},

@article{shirima_use_2007,
	title = {The use of personal digital assistants for data entry at the point of collection in a large household survey in southern Tanzania},
	volume = {4},
	issn = {1742-7622},
	url = {http://www.ete-online.com/content/4/1/5},
	doi = {10.1186/1742-7622-4-5},
	abstract = {BACKGROUND:Survey data are traditionally collected using pen-and-paper, with double data entry, comparison of entries and reconciliation of discrepancies before data cleaning can commence. We used Personal Digital Assistants (PDAs) for data entry at the point of collection, to save time and enhance the quality of data in a survey of over 21,000 scattered rural households in southern Tanzania.METHODS:Pendragon Forms 4.0 software was used to develop a modular questionnaire designed to record information on household residents, birth histories, child health and health-seeking behaviour. The questionnaire was loaded onto Palm m130 PDAs with 8 Mb RAM. One hundred and twenty interviewers, the vast majority with no more than four years of secondary education and very few with any prior computer experience, were trained to interview using the PDAs. The 13 survey teams, each with a supervisor, laptop and a four-wheel drive vehicle, were supported by two back-up vehicles during the two months of field activities. PDAs and laptop computers were charged using solar and in-car chargers.Logical checks were performed and skip patterns taken care of at the time of data entry. Data records could not be edited after leaving each household, to ensure the integrity of the data from each interview. Data were downloaded to the laptop computers and daily summary reports produced to evaluate the completeness of data collection. Data were backed up at three levels: (i) at the end of every module, data were backed up onto storage cards in the PDA; (ii) at the end of every day, data were downloaded to laptop computers; and (iii) a compact disc (CD) was made of each team's data each day.A small group of interviewees from the community, as well as supervisors and interviewers, were asked about their attitudes to the use of PDAs.RESULTS:Following two weeks of training and piloting, data were collected from 21,600 households (83,346 individuals) over a seven-week period in July-August 2004. No PDA-related problems or data loss were encountered.Fieldwork ended on 26 August 2004, the full dataset was available on a CD within 24 hours and the results of initial analyses were presented to district authorities on 28 August. Data completeness was over 99\%.The PDAs were well accepted by both interviewees and interviewers.CONCLUSION:The use of PDAs eliminated the usual time-consuming and error-prone process of data entry and validation. PDAs are a promising tool for field research in Africa.},
	number = {1},
	journal = {Emerging Themes in Epidemiology},
	author = {Kizito Shirima and Oscar Mukasa and Joanna Schellenberg and Fatuma Manzi and Davis John and Adiel Mushi and Mwifadhi Mrisho and Marcel Tanner and Hassan Mshinda and David Schellenberg},
	year = {2007},
	pages = {5}
},

@inproceedings{stuijk_exploring_2006,
	address = {San Francisco, CA, USA},
	title = {Exploring trade-offs in buffer requirements and throughput constraints for synchronous dataflow graphs},
	isbn = {1-59593-381-6},
	url = {http://portal.acm.org/citation.cfm?id=1146909.1147138},
	doi = {10.1145/1146909.1147138},
	abstract = {Multimedia applications usually have throughput constraints. An implementation must meet these constraints, while it minimizes resource usage and energy consumption. The compute intensive kernels of these applications are often specified as Synchronous Dataflow Graphs. Communication between nodes in these graphs requires storage space which influences throughput. We present exact techniques to chart the Pareto space of throughput and storage trade-offs, which can be used to determine the minimal storage space needed to execute a graph under a given throughput constraint. The feasibility of the approach is demonstrated with a number of examples.},
	booktitle = {Proceedings of the 43rd annual conference on Design automation},
	publisher = {ACM},
	author = {Sander Stuijk and Marc Geilen and Twan Basten},
	year = {2006},
	keywords = {buffering,optimization,synchronous dataflow,throughput},
	pages = {899--904}
},

@article{hoare_communicating_1978,
	title = {Communicating sequential processes},
	volume = {21},
	url = {http://portal.acm.org/citation.cfm?id=359585},
	doi = {10.1145/359576.359585},
	abstract = {This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.},
	number = {8},
	journal = {Commun. ACM},
	author = {C. A. R. Hoare},
	year = {1978},
	keywords = {classes,concurrency,conditional critical regions,coroutines,data representations,guarded commands,input,iterative arrays,monitors,multiple entries,multiple exits,nondeterminacy,output,parallel programming,procedures,program structures,programming,programming languages,programming primitives,recursion},
	pages = {666--677}
},

@phdthesis{karczmarek_constrained_2002,
	type = {S.M. Thesis},
	title = {Constrained and Phased Scheduling of Synchronous Data Flow Graphs for StreamIt Language},
	url = {http://cag.lcs.mit.edu/commit/papers/02/karczma-thesis-SM.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Michal Karczmarek},
	month = dec,
	year = {2002}
},

@inproceedings{barton_streaming_2003,
	title = {Streaming XPath processing with forward and backward axes},
	abstract = {We present a streaming algorithm for evaluating XPath expressions that use backward axes (parent and ancestor) and forward axes in a single document-order traversal of an XML document. Other streaming XPath processors handle only forward axes. We show through experiments that our algorithm significantly outperforms (by more than a factor of two) a traditional nonstreaming XPath engine. Furthermore, our algorithm scales better because it retains only the relevant portions of the input document in memory. Our engine successfully processes documents over 1GB in size, whereas the traditional XPath engine degrades considerably in performance for documents over 100 MB in size and fails to complete for documents of size over 200 MB.},
	booktitle = {Data Engineering, 2003. Proceedings. 19th International Conference on},
	author = {C. Barton and P. Charles and Deepak  Goyal and Mukund  Raghavachari and M. Fontoura and V. Josifovski},
	year = {2003},
	keywords = {backward axes,forward axes,streaming algorithm,traditional XPath engine,tree based representation,tree data structures,tree searching,XML,XML document-order traversal,XPath expressions,XPath processing,X-tree},
	pages = {455--466}
},

@inproceedings{karczmarek_phased_2003,
	address = {San Diego, California, USA},
	title = {Phased scheduling of stream programs},
	isbn = {1-58113-647-1},
	url = {http://portal.acm.org/citation.cfm?id=780732.780747\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/780732.780747},
	abstract = {As embedded DSP applications become more complex, it is increasingly important to provide high-level stream abstractions that can be compiled without sacrificing efficiency. In this paper, we describe scheduler support for StreamIt, a high-level language for signal processing applications. A StreamIt program consists of a set of autonomous filters that communicate with each other via FIFO queues. As in Synchronous Dataflow (SDF), the input and output rates of each filter are known at compile time. However, unlike SDF, the stream graph is represented using hierarchical structures, each of which has a single input and a single output.We describe a scheduling algorithm that leverages the structure of StreamIt to provide a flexible tradeoff between code size and buffer size. The algorithm describes the execution of each hierarchical unit as a set of phases. A complete cycle through the phases represents a single steady-state execution. By varying the granularity of a phase, our algorithm provides a continuum between single appearance schedules and minimum latency schedules. We demonstrate that a minimal latency schedule is effective in decreasing buffer requirements for some applications, while the phased representation mitigates the associated increase in code size.},
	booktitle = {Proceedings of the 2003 ACM SIGPLAN conference on Language, compiler, and tool for embedded systems},
	publisher = {ACM},
	author = {Michal Karczmarek and William Thies and Saman Amarasinghe},
	year = {2003},
	keywords = {buffer size,code size,cyclo-static dataflow,dsp,phased scheduling,stream programming,streamit,synchronous dataflow},
	pages = {103--112}
},

@phdthesis{wong_modelingscalability_2004,
	type = {M.Eng. Thesis},
	title = {Modeling the Scalability of Acyclic Stream Programs},
	url = {http://cag.lcs.mit.edu/commit/papers/04/wong-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Jeremy Wong},
	year = {2004}
},

@techreport{lee_overview_2003,
	title = {Overview of the Ptolemy Project},
	number = {UCB/ERL M03/25},
	institution = {University of California, Berkeley},
	author = {Edward A Lee},
	year = {2003}
},

@misc{xuejun_yang_efficient_2007,
	type = {text},
	title = {Efficient generation of stream programs from loops},
	url = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/\&toc=comp/proceedings/icpads/2007/1889/01/1889toc.xml\&DOI=10.1109/ICPADS.2007.4447758},
	author = {Xuejun Yang},
	month = dec,
	year = {2007},
	note = {The efficiency of scientific applications on the Imagine stream processor is increasingly concerned by researchers. One of the obstacles is that the programming language of Imagine does not target the scientific computing. This paper introduces a program transformation algorithm to automatically transform loops to the stream programs executed on Imagine. The optimization for memory accessing is also considered during the transformation. We have implemented the transformation and optimization algorithm with the GFORTRAN frontend. Preliminary results over benchmark kernels show that our approach is a convenient and efficient solution to develop scientific applications on the Imagine stream processor.},
	howpublished = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/\&toc=comp/proceedings/icpads/2007/1889/01/1889toc.xml\&DOI=10.1109/ICPADS.2007.4447758}
},

@inbook{gassend_predicting_2006,
	title = {Predicting Secondary Structure of All-Helical Proteins Using Hidden Markov Support Vector Machines},
	url = {http://dx.doi.org/10.1007/11818564\_11},
	abstract = {Our goal is to develop a state-of-the-art secondary structure predictor with an intuitive and biophysically-motivated energy model through the use of Hidden Markov Support Vector Machines (HM- SVMs), a recent innovation in the field of machine learning. We focus on the prediction of alpha helices and show that by using HM-SVMs, a simple 7-state HMM with 302 parameters can achieve a Qα value of 77.6\% and a SOVα value of 73.4\%. As detailed in an accompanying technical report[11], these performance numbers are among the best for techniques that do not rely on external databases (such as multiple sequence alignments). },
	journal = {Pattern Recognition in Bioinformatics},
	author = {Blaise Gassend and Charles O’Donnell and William Thies and Andrew Lee and Marten van Dijk and Srinivas Devadas},
	year = {2006},
	pages = {93--104}
},

@inproceedings{murthy_system_2001,
	address = {Copenhagen, Denmark},
	title = {System canvas: a new design environment for embedded DSP and telecommunication systems},
	isbn = {1-58113-364-2},
	url = {http://portal.acm.org/citation.cfm?id=371675},
	doi = {10.1145/371636.371675},
	abstract = {We present a new design environment, called System Canvas, targeted at DSP and telecommunication system designs. Our environment uses an easy-to-use block-diagram syntax to specify systems at a very high level of abstraction. The block diagram syntax is based on formal semantics, and uses a number of different models of computation including cyclo-static dataflow, dynamic dataflow, and a discrete-event model. A key feature of our tool is that the user does not need to have an awareness of which model is being used; the models can be freely mixed and matched and a simulation can consist of an arbitrary combination of models. The blocks are written in `C'or `C++' and it is straightforward to write custom blocks and incorporate them into custom libraries. Other key features include the ability to control simulations via language-neutral scripts, and a powerful optimization engine that enables optimization of the system over arbitrarily specified parameters, constraints, and cost functions. Fixed-point analysis capability allows any signal or variable in the system to be set to any type of number system before the simulation proceeds. The tool is available on the Windows NT platform and incorporates modern and ubiquitous Windows GUI look and feel.},
	booktitle = {Proceedings of the ninth international symposium on Hardware/software codesign},
	publisher = {ACM},
	author = {Praveen K. Murthy and Etan G. Cohen and Steve Rowland},
	year = {2001},
	pages = {54--59}
},

@misc{_workshopstreaming_????,
	title = {Workshop on Streaming Systems},
	url = {http://cag.csail.mit.edu/wss03/},
	howpublished = {http://cag.csail.mit.edu/wss03/}
},

@inproceedings{bhattacharyya_optimal_1995,
	title = {Optimal parenthesization of lexical orderings for DSP block diagrams},
	doi = {10.1109/VLSISP.1995.527489},
	abstract = {Minimizing memory requirements for program and data are critical objectives when synthesizing software for embedded DSP applications. Previously, it has been demonstrated that for graphical programs based on the widely-used synchronous dataflow model an important class of minimum code size implementations can be viewed as parenthesizations of lexical orderings of the computational blocks. Such a parenthesization corresponds to the hierarchy of loops in the software implementation. In this paper, we present a dynamic programming technique for constructing a parenthesization that minimizes data memory cost from a given lexical ordering of a synchronous dataflow graph. For graphs that do not contain delays, this technique always constructs a parenthesization that has minimum data memory cost from among all parenthesizations for the given lexical ordering. When delays are present, the technique may make refinements to the lexical ordering while it is computing the parenthesization, and the data memory cost of the result is guaranteed to be less than or equal to the data memory cost of all valid parenthesizations for the initial lexical ordering},
	booktitle = {VLSI Signal Processing, VIII, 1995. IEEE Signal Processing Society [Workshop on]},
	author = {S.S. Bhattacharyya and P.K. Murthy and E.A. Lee},
	year = {1995},
	keywords = {code size,data flow graphs,data memory,delays,digital signal processing chips,DSP block diagrams,dynamic programming,lexical orderings,microprogramming,optimal parenthesization,software synthesis,synchronous dataflow graph},
	pages = {177--186}
},

@article{forster_evaluation_1991,
	title = {Evaluation of a computerized field data collection system for health surveys},
	volume = {69},
	issn = {0042-9686},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/2054915},
	doi = {2054915},
	abstract = {A customized field data collection system (FDCS) has been developed for a hand-held computer to collect and check questionnaire data. The data quality, preparation time, and user acceptability of the system were evaluated during a malaria morbidity survey in Bakau, the Gambia. Eight field-workers collected data with either the FDCS or on paper questionnaire forms in alternate weeks over a 6-week period. Significantly fewer item errors occurred with the FDCS, and by the end of the survey period interview times were significantly less with the FDCS than with the paper and pencil questionnaire. Advanced appropriate technology may have a useful role in providing accurate and rapid information, particularly in overcoming bottlenecks in data processing, and in obviating the need for costly expertise and equipment. In developing countries this could help to improve the quality of data on health care.},
	number = {1},
	journal = {Bulletin of the World Health Organization},
	author = {D Forster and R H Behrens and H Campbell and P Byass},
	year = {1991},
	note = {PMID: 2054915},
	keywords = {Data Collection,Gambia,Health Surveys,Humans,Malaria,Microcomputers,Questionnaires,Software},
	pages = {107--11}
},

@article{maraninchi_argos:automaton-based_2001,
	title = {Argos: an automaton-based synchronous language},
	volume = {27},
	url = {http://www.sciencedirect.com/science?\_ob=ArticleURL\&\_udi=B6TYK-44CHN96-5\&\_user=10\&\_rdoc=1\&\_fmt=\&\_orig=search\&\_sort=d\&view=c\&\_version=1\&\_urlVersion=0\&\_userid=10\&md5=2d9b87dc1fd9b0a7d03dd3bd4c6d645a},
	doi = {10.1016/S0096-0551(01)00016-9},
	abstract = {Argos belongs to the family of synchronous languages, designed for programming reactive systems: Lustre (Proceedings of the 14th Symposium on Principles of Programming Languages, Munich, 1987; Proc. IEEE 79(9) (1999) 1305), Esterel (Sci. Comput. Programming 19(2) (1992) 87), Signal (Technical Report, IRISA Report 246, IRISA, Rennes, France, 1985). Argos is a set of operators that allow to combine Boolean Mealy machines, in a compositional way. It takes its origin in Statecharts (Sci. Comput. Programming 8 (1987) 231), but with the Argos operators, one can build only a subset of Statecharts, roughly those that do not make use of multi-level arrows. We explain the main motivations for the definition of Argos, and the main differences with Statecharts and their numerous semantics. We define the set of operators, give them a perfectly synchronous semantics in the sense of Esterel, and prove that it is compositional, with respect to the trace equivalence of Boolean Mealy machines. We give an overview of the work related to the definition and implementation of Argos (code generation, connection to verification tools, introduction of non-determinism, etc.). This paper also gives a set of guidelines for building an automaton-based, Statechart-like, yet perfectly synchronous, language. Author Keywords: Argos; Synchronous language; Semantics; Compositionality 1 VERIMAG is a joint laboratory of UJF, CNRS and INPG. Corresponding author. Tel.: +33-4-76-63-48-53; fax: +33-4-76-63-48-50; email: florence.maraninchi@imag.fr },
	number = {1-3},
	journal = {Computer Languages},
	author = {Florence Maraninchi and Yann Remond},
	month = oct,
	year = {2001},
	pages = {61--92}
},

@article{karp_organization_1967,
	title = {The Organization of Computations for Uniform Recurrence Equations},
	volume = {14},
	url = {http://portal.acm.org/citation.cfm?id=321406.321418\&coll=GUIDE\&dl=GUIDE},
	doi = {10.1145/321406.321418},
	abstract = {A set equations in the quantities ai(p), where i = 1, 2, · · ·, m and p ranges over a set R of lattice points in n-space, is called a system of uniform recurrence equations if the following property holds: If p and q are in R and w is an integer n-vector, then ai(p) depends directly on aj(p - w) if and only if ai(q) depends directly on aj(q - w). Finite-difference approximations to systems of partial differential equations typically lead to such recurrence equations. The structure of such a system is specified by a dependence graph G having m vertices, in which the directed edges are labeled with integer n-vectors. For certain choices of the set R, necessary and sufficient conditions on G are given for the existence of a schedule to compute all the quantities ai(p) explicitly from their defining equations. Properties of such schedules, such as the degree to which computation can proceed “in parallel,” are characterized. These characterizations depend on a certain iterative decomposition of a dependence graph into subgraphs. Analogous results concerning implicit schedules are also given.},
	number = {3},
	journal = {J. ACM},
	author = {Richard M. Karp and Raymond E. Miller and Shmuel Winograd},
	year = {1967},
	pages = {563--590}
},

@inproceedings{ali_advanced_2008,
	address = {Island of Kos, Greece},
	title = {Advanced collective communication in aspen},
	isbn = {978-1-60558-158-3},
	url = {http://portal.acm.org/citation.cfm?id=1375527.1375543\&coll=portal\&dl=ACM\&type=series\&idx=SERIES383\&part=series\&WantType=Proceedings\&title=ICS},
	abstract = {Aspen is a programming language that relies on high-level messaging to support communication among different program tasks executing in parallel. Unlike MPI, the computational logic of Aspen tasks is specified and developed independently of the global communication structure of the program. A root module specifies the communication structure of the program. The semantics and generality of these specifications enable novel forms of collective communication, including asynchronous and concurrent collective operations and reduction type operations with subsets of the participants being receivers of the reduced data, and with receivers that do not provide data to the reduction. This paper describes efficient implementations of these and other collective communication operations in Aspen. We demonstrate the ease-of-use of these features using several code examples and quantify their performance impact through both microbenchmarks and a quantum chemistry code used in rubber chemistry. Aspen's performance is competitive with, or slightly better than, the performance of MPI implementations for both the chemistry application and the microbenchmarks.},
	booktitle = {Proceedings of the 22nd annual international conference on Supercomputing},
	publisher = {ACM},
	author = {Qasim Ali and Vijay S. Pai and Samuel P. Midkiff},
	year = {2008},
	keywords = {algorithms,parallel programming,programming languages,reductions},
	pages = {83--93}
},

@article{landin_correspondence_1965,
	title = {Correspondence between ALGOL 60 and Church's Lambda-notation: part I},
	volume = {8},
	url = {http://portal.acm.org/citation.cfm?id=363744.363749},
	doi = {10.1145/363744.363749},
	abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
	number = {2},
	journal = {Commun. ACM},
	author = {P. J. Landin},
	year = {1965},
	pages = {89--101}
},

@inproceedings{moonen_cache_2008,
	title = {Cache Aware Mapping of Streaming Applications on a Multiprocessor System-on-Chip},
	doi = {10.1109/DATE.2008.4484696},
	abstract = {Efficient use of the memory hierarchy is critical for achieving high performance in a multiprocessor system-on-chip. An external memory that is shared between processors is a bottleneck in current and future systems. Cache misses and a large cache miss penalty contribute to a low processor utilisation. In this paper, we describe a novel cache optimisation technique to reduce instruction and data cache misses for streaming applications. The instruction and data locality are improved by executing a task multiple times before moving to the next task. Furthermore, we introduce a dataflow model that is used to trade-off the number of cache misses against end-to-end latency and memory usage. For our industrial application, which is a Digital Radio Mondiale receiver, the number of cache misses is reduced with a factor 4.2.},
	booktitle = {Design, Automation and Test in Europe, 2008. DATE '08},
	author = {Arno Moonen and Marco Bekooij and Rene van den Berg and Jef van Meerbergen},
	year = {2008},
	keywords = {toread},
	pages = {300--305}
},

@article{thies_abstraction_2008,
	title = {Abstraction layers for scalable microfluidic biocomputing},
	volume = {7},
	url = {http://portal.acm.org/citation.cfm?id=1375389.1375394\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	abstract = {Microfluidic devices are emerging as an attractive technology for automatically orchestrating the reactions needed in a biological computer. Thousands of microfluidic primitives have already been integrated on a single chip, and recent trends indicate that the hardware complexity is increasing at rates comparable to Moore's Law. As in the case of silicon, it will be critical to develop abstraction layers--such as programming languages and Instruction Set Architectures (ISAs)--that decouple software development from changes in the underlying device technology. Towards this end, this paper presents BioStream, a portable language for describing biology protocols, and the Fluidic ISA, a stable interface for microfluidic chip designers. A novel algorithm translates microfluidic mixing operations from the BioStream layer to the Fluidic ISA. To demonstrate the benefits of these abstraction layers, we build two microfluidic chips that can both execute BioStream code despite significant differences at the device level. We consider this to be an important step towards building scalable biological computers.},
	number = {2},
	journal = {Natural Computing: an international journal},
	author = {William Thies and John Paul Urbanski and Todd Thorsen and Saman Amarasinghe},
	year = {2008},
	keywords = {biological computation,dna computing,laboratory automation,microfluidics,programming languages,self-assembly},
	pages = {255--275}
},

@article{medford_microsoft/yahoo_2008,
	title = {Microsoft/Yahoo is Mobile Equal of Google},
	url = {http://www.redherring.com/Home/23680},
	journal = {Red Herring},
	author = {Cassimir Medford},
	month = feb,
	year = {2008}
},

@article{parks_presented_????,
	title = {Presented at the Asilomar Conference on Signals, Systems and Computers--- October 1995 A Comparison of Synchronous and Cyclo-Static Dataflow},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.9852},
	doi = {10.1.1.24.9852},
	author = {Thomas M Parks and Jose Luis Pino and Edward A Lee}
},

@article{le_guernic_signal--data_1986,
	title = {Signal--A data flow-oriented language for signal processing},
	volume = {34},
	issn = {0096-3518},
	abstract = {We present the language SIGNAL which is a data flow-oriented real-time, synchronous, side effect-free language suited to the expression and recovery of the parallelism in signal or image processing algorithms. The language is intended to be, at the same time, an executable simulation language, and a specification of a virtual machine implementing the algorithm. The language is semantically sound, and is suitable to perform program transforms-a major requirement when the ultimate goal is an aid to the architecture design.},
	number = {2},
	journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
	author = {P. Le Guernic and A. Benveniste and P. Bournai and T. Gautier},
	year = {1986},
	pages = {362--374}
},

@article{missinou_short_2005,
	title = {Short Report: Piloting Paperless Data Entry for Clinical Research in Africa},
	volume = {72},
	url = {http://www.ajtmh.org/cgi/content/abstract/72/3/301},
	abstract = {Direct data entry, using handheld computers, may simplify and streamline data management, especially in remote settings. We compared the accuracy of data entry using the current standard practice (a paper-based case report form with double data entry) with that using a personal digital assistant (PDA) in a clinical study in rural Gabon. The rate of discrepant entries was 1.7\%. Categorical data (presented in "pull down" menus on the PDA) were more commonly discrepant than were continuous "typed in" data (2.4\% versus 1.2\%; P = 0.001). Both systems functioned smoothly and no data were lost. The clinicians involved in this study preferred the handheld computers, and their use will be considered in future studies in an African clinical research network. },
	number = {3},
	journal = {Am J Trop Med Hyg},
	author = {MICHEL A. MISSINOU and CHRISTOPHER H. O. OLOLA and SAADOU ISSIFOU and PIERRE-BLAISE MATSIEGUI and AYOLA A. ADEGNIKA and STEFFEN BORRMANN and DAVID WYPIJ and TERRIE E. TAYLOR and PETER G. KREMSNER},
	month = mar,
	year = {2005},
	pages = {301--303}
},

@inproceedings{gorlick_using_1991,
	title = {Using weaves for software construction and analysis},
	doi = {10.1109/ICSE.1991.130620},
	abstract = {The authors discuss the architectural features of weaves, their implementation, and their use in a variety of applications. Weaves are networks of concurrently executing tool fragments that communicate by passing objects. Weaves are distinguished from other dataflow styles by their emphasis on instrumentation, continuous observability, and dynamic rearrangement: basic low-overhead instrumentation is inserted automatically, executing weaves can be observed at any time by means of sophisticated analysis agents, without degrading the performance of the weave, and weaves can be dynamically snipped and spliced without interrupting the data flow},
	booktitle = {Software Engineering, 1991. Proceedings., 13th International Conference on},
	author = {M.M. Gorlick and R.R. Razouk},
	year = {1991},
	keywords = {analysis agents,concurrently executing tool fragments,continuous observability,dataflow styles,dynamic rearrangement,instrumentation,networks,object passing,parallel programming,software analysis,software construction,software engineering,weaves},
	pages = {23--34}
},

@article{buck_brook_2004,
	title = {Brook for GPUs: stream computing on graphics hardware},
	volume = {23},
	url = {http://portal.acm.org/citation.cfm?id=1015706.1015800\&coll=portal\&dl=ACM\&idx=J778\&part=transaction\&WantType=Transactions\&title=ACM\%20Transactions\%20on\%20Graphics\%20(TOG)},
	abstract = {In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.},
	number = {3},
	journal = {ACM Trans. Graph.},
	author = {Ian Buck and Tim Foley and Daniel Horn and Jeremy Sugerman and Kayvon Fatahalian and Mike Houston and Pat Hanrahan},
	year = {2004},
	keywords = {brook,data parallel computing,gpu computing,programmable graphics hardware,stream computing},
	pages = {777--786}
},

@phdthesis{clinger_foundations_1981,
	type = {Ph.D. Thesis},
	title = {Foundations of Actor Semantics},
	school = {Massachusetts Insitute of Technology},
	author = {William Douglas Clinger},
	year = {1981},
	keywords = {actors}
},

@phdthesis{kuo_streamit_2004,
	type = {M.Eng. Thesis},
	title = {The StreamIt Development Tool: A Programming Environment for StreamIt},
	url = {http://cag.lcs.mit.edu/commit/papers/04/kkuo-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Kimberly Kuo},
	month = jun,
	year = {2004}
},

@inproceedings{liu_perts:prototyping_1993,
	title = {PERTS: A prototyping environment for real-time systems},
	doi = {10.1109/REAL.1993.393502},
	abstract = {PERTS is a prototyping environment for real-time systems. It contains schedulers and resource access protocols for time-critical applications, together with a comprehensive set of tools for the analysis, validation, and evaluation of real-time systems built on the scheduling paradigms supported by these building blocks. This paper describes the underlying models of real-time systems supported by PERTS, as well as its capabilities and intended use. A key component is the schedulability analyzer. The basic version of this system of tools supports the validation and evaluation of real-time systems built on the framework of the periodic-task model. This system of tools is now available},
	booktitle = {Real-Time Systems Symposium, 1993., Proceedings.},
	author = {J.W.S. Liu and J.L. Redondo and Z. Deng and T.S. Tia and R. Bettati and A. Silberman and M. Storch and R. Ha and W.K. Shih},
	year = {1993},
	keywords = {analysis,evaluation,performance evaluation,periodic-task model,PERTS,program verification,prototyping environment,real-time systems,resource access protocols,schedulability analyzer,schedulers,scheduling,software prototyping,software tools,time-critical applications,validation},
	pages = {184--188}
},

@article{eker_taming_2003,
	title = {Taming heterogeneity - the Ptolemy approach},
	volume = {91},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2002.805829},
	abstract = {Modern embedded computing systems tend to be heterogeneous in the sense of being composed of subsystems with very different characteristics, which communicate and interact in a variety of ways-synchronous or asynchronous, buffered or unbuffered, etc. Obviously, when designing such systems, a modeling language needs to reflect this heterogeneity. Today's modeling environments usually offer a variant of what we call amorphous heterogeneity to address this problem. This paper argues that modeling systems in this manner leads to unexpected and hard-to-analyze interactions between the communication mechanisms and proposes a more structured approach to heterogeneity, called hierarchical heterogeneity, to solve this problem. It proposes a model structure and semantic framework that support this form of heterogeneity, and discusses the issues arising from heterogeneous component interaction and the desire for component reuse. It introduces the notion of domain polymorphism as a way to address these issues.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {J. Eker and J.W. Janneck and E.A. Lee and Jie Liu and Xiaojun Liu and J. Ludvig and S. Neuendorffer and S. Sachs and Yuhong Xiong},
	year = {2003},
	keywords = {component reuse,component-based design,domain polymorphism,embedded computing systems,embedded systems,heterogeneous modeling,hierarchical heterogeneity,modeling environments,models of computation,object-oriented programming,programming environments,Ptolemy,Ptolemy II,software architecture,software environment,software reusability},
	pages = {127--144}
},

@article{ali_organizational_2006,
	title = {Organizational aspects and implementation of data systems in large-scale epidemiological studies in less developed countries},
	volume = {6},
	issn = {1471-2458},
	url = {http://www.biomedcentral.com/1471-2458/6/86},
	doi = {10.1186/1471-2458-6-86},
	abstract = {BACKGROUND:In the conduct of epidemiological studies in less developed countries, while great emphasis is placed on study design, data collection, and analysis, often little attention is paid to data management. As a consequence, investigators working in these countries frequently face challenges in cleaning, analyzing and interpreting data. In most research settings, the data management team is formed with temporary and unskilled persons. A proper working environment and training or guidance in constructing a reliable database is rarely available. There is little information available that describes data management problems and solutions to those problems. Usually a line or two can be obtained in the methods section of research papers stating that the data are doubly-entered and that outliers and inconsistencies were removed from the data. Such information provides little assurance that the data are reliable. There are several issues in data management that if not properly practiced may create an unreliable database, and outcomes of this database will be spurious.RESULTS:We have outlined the data management practices for epidemiological studies that we have modeled for our research sites in seven Asian countries and one African country.CONCLUSION:Information from this model data management structure may help others construct reliable databases for large-scale epidemiological studies in less developed countries.},
	number = {1},
	journal = {BMC Public Health},
	author = {Mohammad Ali and Jin-Kyung Park and Lorenz von Seidlein and Camilo Acosta and Jacqueline Deen and John Clemens},
	year = {2006},
	pages = {86}
},

@article{stephens_survey_1997,
	title = {A survey of stream processing},
	volume = {34},
	url = {http://dx.doi.org/10.1007/s002360050095},
	doi = {10.1007/s002360050095},
	abstract = {Abstract.    Stream processing is a term that is used widely in the literature to describe a variety of systems. We present an overview of the historical development of stream processing and a detailed discussion of the different languages and techniques for programming with streams that can be found in the literature. This includes an analysis of dataflow, specialized  functional and logic programming with streams, reactive systems, signal processing systems, and the use of streams in the design and verification of hardware. The aim of this survey is an analysis of the development of each of these specialized topics to determine if a general theory of stream processing has emerged. As such, we discuss and classify the different classes of stream processing systems found in the literature from the perspective of programming primitives, implementation techniques, and computability issues, including a comparison of the semantic models that are used to formalize stream based computation. },
	number = {7},
	journal = {Acta Informatica},
	author = {Robert Stephens},
	month = jul,
	year = {1997},
	pages = {491--541}
},

@inproceedings{chen_reconfigurable_2005,
	address = {Los Angeles, California},
	title = {A reconfigurable architecture for load-balanced rendering},
	isbn = {1-59593-086-8},
	url = {http://portal.acm.org/citation.cfm?id=1071866.1071878\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/1071866.1071878},
	abstract = {Commodity graphics hardware has become increasingly programmable over the last few years but has been limited to fixed resource allocation. These architectures handle some workloads well, others poorly; load-balancing to maximize graphics hardware performance has become a critical issue. In this paper, we explore one solution to this problem using compile-time resource allocation. For our experiments, we implement a graphics pipeline on Raw, a tile-based multicore processor. We express both the full graphics pipeline and the shaders using StreamIt, a high-level language based on the stream programming model. The programmer specifies the number of tiles per pipeline stage, and the StreamIt compiler maps the computation to the Raw architecture.We evaluate our reconfigurable architecture using a mix of common rendering tasks with different workloads and improve throughput by 55-157\% over a static allocation. Although our early prototype cannot compete in performance against commercial state-of-the-art graphics processors, we believe that this paper describes an important first step in addressing the load-balancing challenge.},
	booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware},
	publisher = {ACM},
	author = {Jiawen Chen and Michael I. Gordon and William Thies and Matthias Zwicker and Kari Pulli and Frédo Durand},
	year = {2005},
	pages = {71--80}
},

@book{nikhil_implicit_2001,
	edition = {1st},
	title = {Implicit Parallel Programming in pH},
	isbn = {1558606440},
	publisher = {Morgan Kaufmann},
	author = {Rishiyur Nikhil and Arvind},
	month = may,
	year = {2001},
	pages = {400}
},

@article{gassend_learning_2007,
	title = {Learning biophysically-motivated parameters for alpha helix prediction},
	volume = {8},
	issn = {1471-2105},
	url = {http://www.biomedcentral.com/1471-2105/8/S5/S3},
	doi = {10.1186/1471-2105-8-S5-S3},
	abstract = {BACKGROUND:Our goal is to develop a state-of-the-art protein secondary structure predictor, with an intuitive and biophysically-motivated energy model. We treat structure prediction as an optimization problem, using parameterizable cost functions representing biological "pseudo-energies". Machine learning methods are applied to estimate the values of the parameters to correctly predict known protein structures.RESULTS:Focusing on the prediction of alpha helices in proteins, we show that a model with 302 parameters can achieve a Qalpha value of 77.6\% and an SOValpha value of 73.4\%. Such performance numbers are among the best for techniques that do not rely on external databases (such as multiple sequence alignments). Further, it is easier to extract biological significance from a model with so few parameters.CONCLUSION:The method presented shows promise for the prediction of protein secondary structure. Biophysically-motivated elementary free-energies can be learned using SVM techniques to construct an energy cost function whose predictive performance rivals state-of-the-art. This method is general and can be extended beyond the all-alpha case described here.},
	number = {Suppl 5},
	journal = {BMC Bioinformatics},
	author = {Blaise Gassend and Charles O'Donnell and William Thies and Andrew Lee and Marten van Dijk and Srinivas Devadas},
	year = {2007},
	pages = {S3}
},

@inproceedings{ad_data_1997,
	address = {Anaheim, California, United States},
	title = {Data memory minimisation for synchronous data flow graphs emulated on DSP-FPGA targets},
	isbn = {0-89791-920-3},
	url = {http://portal.acm.org/citation.cfm?id=266036\&coll=portal\&dl=ACM},
	doi = {10.1145/266021.266036},
	abstract = {The paper presents an algorithm to determine the close-to-smallestpossible data buffer sizes for arbitrary synchronous dataflow (SDF) applications, such that we can guarantee the existenceof a deadlock free schedule. The presented algorithm fits inthe design flow of GRAPE, an environment for the emulation andimplementation of digital signal processing (DSP) systems onarbitrary target architectures, consisting of programmable DSPprocessors and FPGAs. Reducing the size of data buffers is ofhigh importance when the application will be mapped on FieldProgrammable Gate Arrays (FPGA), since register resources arerather scarce.},
	booktitle = {Proceedings of the 34th annual conference on Design automation},
	publisher = {ACM},
	author = {Marleen Adé and Rudy Lauwereins and J. A. Peperstraete},
	year = {1997},
	pages = {64--69}
},

@article{ko_beyond_2007,
	title = {Beyond single-appearance schedules: Efficient DSP software synthesis using nested procedure calls},
	volume = {6},
	url = {http://portal.acm.org.libproxy.mit.edu/citation.cfm?doid=1234675.1234681},
	doi = {10.1145/1234675.1234681},
	abstract = {Synthesis of digital signal-processing (DSP) software from dataflow-based formal models is an effective approach for tackling the complexity of modern DSP applications. In this paper, an efficient method is proposed for applying subroutine call instantiation of module functionality when synthesizing embedded software from a dataflow specification. The technique is based on a novel recursive decomposition of subgraphs in a cluster hierarchy that is optimized for low buffer size. Applying this technique, one can achieve significantly lower buffer sizes than what is available for minimum code size inlined schedules, which have been the emphasis of prior work on software synthesis. Furthermore, it is guaranteed that the number of procedure calls in the synthesized program is polynomially bounded in the size of the input dataflow graph, even though the number of module invocations may increase exponentially. This recursive decomposition approach provides an efficient means for integrating subroutine-based module instantiation into the design space of DSP software synthesis. The experimental results demonstrate a significant improvement in buffer cost, especially for more irregular multirate DSP applications, with moderate code and execution time overhead.},
	number = {2},
	journal = {Trans. on Embedded Computing Sys.},
	author = {Ming-Yung Ko and Praveen K. Murthy and Shuvra S. Bhattacharyya},
	year = {2007},
	keywords = {block diagram compiler,design methodology,embedded systems,hierarchical graph decomposition,memory optimization,procedural implementation,synchronous dataflow},
	pages = {14}
},

@misc{__????
},

@phdthesis{petri_communication_1962,
	type = {Ph.D. Thesis},
	title = {Communication with Automata},
	school = {Darmstadt Institue of Technology},
	author = {C.A. Petri},
	year = {1962}
},

@article{frerichs_computer-assisted_1974,
	title = {Computer-assisted rapid surveys in developing countries},
	volume = {104},
	issn = {0033-3549},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/2522230},
	doi = {2522230},
	abstract = {Health surveys are an important source of population-based data in much of the developing world. Unfortunately, sample surveys often take more time to plan, process, and analyze than is practical, given the information needs of the local decision-makers. Rapid survey methodology (RSM) has been developed to permit health professionals to answer quickly questions about the health status and activities of people at the community level. These answers may be necessary for determining program priorities or for monitoring program activities. Rapid surveys are meant to supplement, rather than replace, information derived from existing sources of vital and health statistics data. RSM combines sample survey methods with contemporary software used in portable, battery-powered microcomputers. The ability to do rapid surveys in developing countries also requires knowledge of how to use appropriate computer hardware and software and how to apply cluster sampling theory in the local environment. RSM was used for the first time in Hlegu Township, Burma, to conduct a health survey of young children. The survey team started the field work on May 4, 1987. Four days later, while still in the field, the data were processed and rapidly analyzed by portable microcomputers for presentation to the local township medical officer and his staff. Within 10 days of starting the field work, we issued a detailed 50-page report of the study findings. This paper provides (a) a description of the components of rapid survey methodology, including the sample survey method, computer hardware, and computer software; (b) the general requirements for portable computer hardware in less developed regions of the world; (c) the procedures for doing a rapid survey; and (d) a summary of our experiences with RSM in Burma.},
	number = {1},
	journal = {Public Health Reports (Washington, D.C.: 1974)},
	author = {R R Frerichs and K T Tar},
	year = {1974},
	note = {PMID: 2522230},
	keywords = {Developing Countries,Epidemiologic Methods,Health Surveys,Humans,Microcomputers,Myanmar,Population Surveillance,Sampling Studies,Software},
	pages = {14--23}
},

@inproceedings{thies_teleport_2005,
	address = {Chicago, IL, USA},
	title = {Teleport messaging for distributed stream programs},
	isbn = {1-59593-080-9},
	url = {http://portal.acm.org/citation.cfm?id=1065944.1065975\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/1065944.1065975},
	abstract = {In this paper, we develop a new language construct to address one of the pitfalls of parallel programming: precise handling of events across parallel components. The construct, termed teleport messaging, uses data dependences between components to provide a common notion of time in a parallel system. Our work is done in the context of the Synchronous Dataflow (SDF) model, in which computation is expressed as a graph of independent components (or actors) that communicate in regular patterns over data channels. We leverage the static properties of SDF to compute a stream dependence function, SDEP, that compactly describes the ordering constraints between actor executions.Teleport messaging utilizes SDEP to provide powerful and precise event handling. For example, an actor A can specify that an event should be processed by a downstream actor B as soon as B sees the "effects" of the current execution of A. We argue that teleport messaging improves readability and robustness over existing practices. We have implemented messaging as part of the StreamIt compiler, with a backend for a cluster of workstations. As teleport messaging exposes optimization opportunities to the compiler, it also results in a 49\% performance improvement for a software radio benchmark.},
	booktitle = {Proceedings of the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming},
	publisher = {ACM},
	author = {William Thies and Michal Karczmarek and Janis Sermulins and Rodric Rabbah and Saman Amarasinghe},
	year = {2005},
	keywords = {dependence analysis,digital signal processing,embedded,event handling,streamit,synchronous dataflow},
	pages = {224--235}
},

@inproceedings{lamb_linear_2003-1,
	address = {San Diego, California, USA},
	title = {Linear analysis and optimization of stream programs},
	isbn = {1-58113-662-5},
	url = {http://portal.acm.org/citation.cfm?id=781131.781134\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/781131.781134},
	abstract = {As more complex DSP algorithms are realized in practice, there is an increasing need for high-level stream abstractions that can be compiled without sacrificing efficiency. Toward this end, we present a set of aggressive optimizations that target linear sections of a stream program. Our input language is StreamIt, which represents programs as a hierarchical graph of autonomous filters. A filter is linear if each of its outputs can be represented as an affine combination of its inputs. Linearity is common in DSP components; examples include FIR filters, expanders, compressors, FFTs and DCTs.We demonstrate that several algorithmic transformations, traditionally hand-tuned by DSP experts, can be completely automated by the compiler. First, we present a linear extraction analysis that automatically detects linear filters from the C-like code in their work function. Then, we give a procedure for combining adjacent linear filters into a single filter, as well as for translating a linear filter to operate in the frequency domain. We also present an optimization selection algorithm, which finds the sequence of combination and frequency transformations that will give the maximal benefit.We have completed a fully-automatic implementation of the above techniques as part of the StreamIt compiler, and we demonstrate a 450\% performance improvement over our benchmark suite.},
	booktitle = {Proceedings of the ACM SIGPLAN 2003 conference on Programming language design and implementation},
	publisher = {ACM},
	author = {Andrew A. Lamb and William Thies and Saman Amarasinghe},
	year = {2003},
	keywords = {algebraic simplification,dsp,embedded,fft,linear systems,optimization,stream programming,streamit},
	pages = {12--25}
},

@inproceedings{ceng_maps:integrated_2008,
	title = {MAPS: An integrated framework for MPSoC application parallelization},
	isbn = {0738-100X},
	abstract = {In the past few years, MPSoC has become the most popular solution for embedded computing. However, the challenge of programming MPSoCs also comes as the biggest side-effect of the solution. Especially, when designers have to face the legacy C code accumulated through the years, the tool support is mostly unsatisfactory. In this paper, we propose an integrated framework, MAPS, which aims at parallelizing C applications for MPSoC platforms. It extracts coarse-grained parallelism on a novel granularity level. A set of tools have been developed for the framework. We will introduce the major components and their functionalities. Two case studies will be given, which demonstrate the use of MAPS on two different kinds of applications. In both cases the proposed framework helps the programmer to extract parallelism efficiently.},
	booktitle = {Design Automation Conference, 2008. DAC 2008. 45th ACM/IEEE},
	author = {J. Ceng and J. Castrillon and W. Sheng and H. Scharwachter and R. Leupers and G. Ascheid and H. Meyr and T. Isshiki and H. Kunieda},
	year = {2008},
	keywords = {coarse-grained parallelism,Embedded,embedded computing,embedded systems,integrated framework,legacy C code,MAPS,MPSoC application parallelization,MPSoC Programming,multiprocessing systems,multiprocessor system-on-chip,parallel programming,Parallelization,Software,system-on-chip,toread},
	pages = {754--759}
},

@article{zitzler_multidimensional_2000,
	title = {Multidimensional Exploration of Software Implementationsfor DSP Algorithms},
	volume = {24},
	url = {http://portal.acm.org/citation.cfm?id=342496.342522},
	abstract = {When implementing software for programmable digital signal processors (PDSPs), the design space is defined by a complex range of constraints and optimization objectives. Three implementation metrics that are crucial in many PDSP applications are the program memory requirement (code size), data memory requirement, and execution time. This paper addresses the problem of exploring the 3-dimensional space of trade-offs that is defined by these crucial metrics. Given a software library for a target PDSP, and a dataflow-based block diagram specification of a DSP application in terms of this library, our objective in this paper is to compute a full range of Pareto-optimal solutions. For solving this multi-objective optimization problem, an evolutionary algorithm based approach is applied. We illustrate our techniques by analyzing the trade-off fronts of a practical application for a number of well-known, commercial PDSPs.},
	number = {1},
	journal = {J. VLSI Signal Process. Syst.},
	author = {Eckart Zitzler and Jürgen Teich and Shuvra S. Bhattacharyya},
	year = {2000},
	pages = {83--98}
},

@book{sachs_common_2008,
	title = {Common Wealth: Economics for a Crowded Planet},
	isbn = {1594201277},
	publisher = {Penguin Press HC, The},
	author = {Jeffrey D. Sachs},
	month = mar,
	year = {2008},
	keywords = {toread},
	pages = {400}
},

@article{lauwereins_grape-ii:system-level_1995,
	title = {Grape-II: a system-level prototyping environment for DSP applications},
	volume = {28},
	issn = {0018-9162},
	doi = {10.1109/2.347998},
	abstract = {We propose a rapid-prototyping setup to minimize development cost and a structured-prototyping methodology to reduce programming effort. The general-purpose hardware consists of commercial DSP processors, bond-out versions of core processors, and field-programmable gate arrays (FPGAs) linked to form a powerful, heterogeneous multiprocessor, such as the Paradigm RP developed within the Retides (Real-Time DSP Emulation System) Esprit project. Our Graphical Rapid Prototyping Environment (Grape-II) automates the prototyping methodology for these hardware systems by offering tools for resource estimation, partitioning, assignment, routing, scheduling, code generation, and parameter modification. Grape-II has been used successfully in three real-world DSP applications},
	number = {2},
	journal = {Computer},
	author = {R. Lauwereins and M. Engels and M. Ade and J.A. Peperstraete},
	year = {1995},
	keywords = {assignment,code generation,commercial DSP processors,configuration management,core processors,development cost,development systems,DSP applications,Esprit project,field-programmable gate arrays,general purpose computers,general purpose reusable hardware,general-purpose hardware,Grape-II,Graphical Rapid Prototyping Environment,heterogeneous multiprocessor,multiprocessing systems,partitioning,programming environments,rapid-prototyping,Real-Time DSP Emulation System,resource estimation,Retides,routing,scheduling,signal processing,software prototyping,software tools,structured-prototyping methodology,system-level prototyping environment},
	pages = {35--43}
},

@inproceedings{gordon_exploiting_2006,
	address = {San Jose, California, USA},
	title = {Exploiting coarse-grained task, data, and pipeline parallelism in stream programs},
	isbn = {1-59593-451-0},
	url = {http://portal.acm.org/citation.cfm?id=1168857.1168877\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/1168857.1168877},
	abstract = {As multicore architectures enter the mainstream, there is a pressing demand for high-level programming models that can effectively map to them. Stream programming offers an attractive way to expose coarse-grained parallelism, as streaming applications (image, video, DSP, etc.) are naturally represented by independent filters that communicate over explicit data channels.In this paper, we demonstrate an end-to-end stream compiler that attains robust multicore performance in the face of varying application characteristics. As benchmarks exhibit different amounts of task, data, and pipeline parallelism, we exploit all types of parallelism in a unified manner in order to achieve this generality. Our compiler, which maps from the StreamIt language to the 16-core Raw architecture, attains a 11.2x mean speedup over a single-core baseline, and a 1.84x speedup over our previous work.},
	booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
	publisher = {ACM},
	author = {Michael I. Gordon and William Thies and Saman Amarasinghe},
	year = {2006},
	keywords = {coarse-grained dataflow,multicore,raw,software pipelining,streamit,streams},
	pages = {151--162}
},

@article{_youtube_2006,
	title = {YouTube serves up 100 million videos a day online},
	url = {http://www.usatoday.com/tech/news/2006-07-16-youtube-views\_x.htm},
	journal = {USA TODAY},
	month = jul,
	year = {2006},
	keywords = {News}
},

@misc{_graphviz_????-1,
	title = {Graphviz},
	url = {http://graphviz.org/}
},

@article{peters_can_2006,
	title = {Can computers improve patient care by primary health care workers in India?},
	volume = {18},
	issn = {1353-4505},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17041232},
	doi = {mzl053},
	abstract = {OBJECTIVE: The objective was to test whether a decision support technology for non-physicians can increase health care utilization and quality. DESIGN: Before and after measurements were taken from a systematic random sample of patients and staff at randomly assigned intervention and control facilities. SETTING: The study took place at primary health facilities in rural Tamil Nadu, India. PARTICIPANTS: One thousand two hundred and eighty-six patients and 82 staff were interviewed. INTERVENTION: A computer-assisted decision support technology was introduced to assist with patient screening. MAIN OUTCOME MEASURES: Outcome measures included new patient visits per month, a Global Patient Assessment of Care Index, and health worker attitude variables. RESULTS: There was a difference of difference of 430 new patient visits per month at the intervention sites (P = 0.005), an increase from baseline of 18\% at intervention sites compared with a decline of 5\% at control sites. The intervention was associated with significant improvements in a Global Patient Assessment of Care Index (mean difference of difference 7.9, P {\textless} 0.001). The largest gains were made in patient communication, technical quality, and general satisfaction with care. The attitudes of public health workers toward the new technology and their jobs did not change. CONCLUSIONS: Decision support technologies have considerable potential to improve coverage and quality of health care for the poor and where there is no doctor, but the unreceptive attitude of public health workers would need to be overcome. Application of these technologies should take advantage of their popularity with patients and the opportunity to work through the private sector.},
	number = {6},
	journal = {International Journal for Quality in Health Care: Journal of the International Society for Quality in Health Care / ISQua},
	author = {David H Peters and Manish Kohli and Maya Mascarenhas and Krishna Rao},
	month = dec,
	year = {2006},
	note = {PMID: 17041232},
	keywords = {Attitude of Health Personnel,Decision Support Techniques,Focus Groups,Humans,India,Least-Squares Analysis,Patient Satisfaction,Primary Health Care,Quality of Health Care,Rural Health,Sensitivity and Specificity},
	pages = {437--45}
},

@misc{t._m._parks_comparison_1995,
	type = {text},
	title = {A comparison of synchronous and cycle-static dataflow},
	url = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/\&toc=comp/proceedings/asilomar/1995/7370/00/7370toc.xml\&DOI=10.1109/ACSSC.1995.540541},
	author = {T. M. Parks},
	month = oct,
	year = {1995},
	note = {We compare synchronous dataflow (SDF) and cyclo-static dataflow (CSDF), which are each special cases of a model of computation we call dataflow process networks. In SDF actors have static firing rules: they consume and produce a fixed number of data tokens in each firing. This model is well suited to multirate signal processing applications and lends itself to efficient static scheduling, avoiding the run-time scheduling overhead incurred by general implementations of process networks. In CSDF which is a generalization of SDF actors have cyclically changing firing rules. In some situations, the added generality of CSDF can unnecessarily complicate the scheduling. We show how higher-order functions can be used to transform a CSDF graph into a SDF graph, simplifying the scheduling problem. In other situations, CSDF has a genuine advantage over SDF: simpler precedence constraints. We show how this makes it possible to eliminate unnecessary computations and expose additional parallelism. We use digital sample rate conversion as an example to illustrate these advantages of CSDF. },
	keywords = {data flow computing; scheduling; data flow graphs; signal sampling; cycle-static dataflow; synchronous dataflow; computation model; dataflow process networks; static firing rules; data tokens; multirate signal processing applications; static scheduling; higher-order functions; CSDF graph; SDF graph; scheduling problem; precedence constraints; parallelism; digital sample rate conversion},
	howpublished = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/\&toc=comp/proceedings/asilomar/1995/7370/00/7370toc.xml\&DOI=10.1109/ACSSC.1995.540541}
},

@inproceedings{caspi_lustre:declarative_1987,
	address = {Munich, West Germany},
	title = {LUSTRE: a declarative language for real-time programming},
	isbn = {0-89791-215-2},
	url = {http://portal.acm.org/citation.cfm?id=41625.41641\&coll=ACM\&dl=ACM\&type=series\&idx=SERIES317\&part=series\&WantType=Proceedings\&title=POPL},
	abstract = {LUSTRE is a synchronous data-flow language for programming systems which interact with their environments in real-time. After an informal presentation of the language, we describe its semantics by means of structural inference rules. Moreover, we show how to use this semantics in order to generate efficient sequential code, namely, a finite state automaton which represents the control of the program. Formal rules for program transformation are also presented.},
	booktitle = {Proceedings of the 14th ACM SIGACT-SIGPLAN symposium on Principles of programming languages},
	publisher = {ACM},
	author = {P. Caspi and D. Pilaud and N. Halbwachs and J. A. Plaice},
	year = {1987},
	pages = {178--188}
},

@article{chen_convergence_2008,
	title = {Convergence of Recognition, Mining, and Synthesis Workloads and Its Implications},
	volume = {96},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2008.917729},
	abstract = {This paper examines the growing need for a general-purpose ldquoanalytics enginerdquo that can enable next-generation processing platforms to effectively model events, objects, and concepts based on end-user input, and accessible datasets, along with an ability to iteratively refine the model in real-time. We find such processing needs at the heart of many emerging applications and services. This processing is further decomposed in terms of an integration of three fundamental compute capabilities-recognition, mining, and synthesis (RMS). The set of RMS workloads is examined next in terms of usage, mathematical models, numerical algorithms, and underlying data structures. Our analysis suggests a workload convergence that is analyzed next for its platform implications. In summary, a diverse set of emerging RMS applications from market segments like graphics, gaming, media-mining, unstructured information management, financial analytics, and interactive virtual communities presents a relatively focused, highly overlapping set of common platform challenges. A general-purpose processing platform designed to address these challenges has the potential for significantly enhancing users' experience and programmer productivity.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Yen-Kuang  Chen and J. Chhugani and P. Dubey and C.J. Hughes and Daehyun Kim and S. Kumar and V.W. Lee and A.D. Nguyen and M. Smelyanskiy},
	year = {2008},
	keywords = {Algorithms,data mining,data structures,emerging applications,financial analytics,gaming,general-purpose analytics engine,graphics,interactive virtual communities,mathematical models,media mining,numerical algorithms,parallel architectures,pattern recognition,synthesis workloads,unstructured information management},
	pages = {790--807}
},

@article{ashcroft_common_1980,
	title = {Some common misconceptions about Lucid},
	volume = {15},
	url = {http://portal.acm.org.libproxy.mit.edu/citation.cfm?id=947727.947728\&coll=GUIDE\&dl=GUIDE},
	doi = {10.1145/947727.947728},
	abstract = {This paper attempts to clear up several misconceptions about the language Lucid. In the process we claim that Lucid is in fact a real programming language, and indicate various ways in which implementations might be feasible.},
	number = {10},
	journal = {SIGPLAN Not.},
	author = {Ed Ashcroft and Bill Wadge},
	year = {1980},
	pages = {15--26}
},

@techreport{arvind_asynchronous_1978,
	title = {An asynchronous programming language and computing machine},
	number = {TR 114a},
	institution = {University of California, Irvine},
	author = {Arvind and K.P. Gostelow and W. Plouffe},
	year = {1978}
},

@inbook{williams_static_2005,
	title = {Static Deadlock Detection for Java Libraries},
	url = {http://dx.doi.org/10.1007/11531142\_26},
	abstract = {Library writers wish to provide a guarantee not only that each procedure in the library performs correctly in isolation, but also that the procedures perform correctly when run in conjunction. To this end, we propose a method for static detection of deadlock in Java libraries. Our goal is to determine whether client code exists that may deadlock a library, and, if so, to enable the library writer to discover the calling patterns that can lead to deadlock. Our flow-sensitive, context-sensitive analysis determines possible deadlock configurations using a lock-order graph. This graph represents the order in which locks are acquired by the library. Cycles in the graph indicate deadlock possibilities, and our tool reports all such possibilities. We implemented our analysis and evaluated it on 18 libraries comprising 1245 kLOC. We verified 13 libraries to be free from deadlock, and found 14 distinct deadlocks in 3 libraries.},
	journal = {ECOOP 2005 - Object-Oriented Programming},
	author = {Amy Williams and William Thies and Michael D. Ernst},
	year = {2005},
	pages = {602--629}
},

@article{lee_static_1987,
	title = {Static scheduling of synchronous data flow programs for digital signal processing},
	volume = {36},
	url = {http://portal.acm.org/citation.cfm?id=22812.22814},
	number = {1},
	journal = {IEEE Trans. Comput.},
	author = {Edward Ashford Lee and David G. Messerschmitt},
	year = {1987},
	pages = {24--35}
},

@inproceedings{consel_spidle:dsl_2003,
	address = {Erfurt, Germany},
	title = {Spidle: a DSL approach to specifying streaming applications},
	isbn = {3-540-20102-5},
	url = {http://portal.acm.org/citation.cfm?id=954187\&dl=GUIDE\&coll=GUIDE\&CFID=36821696\&CFTOKEN=74967313},
	abstract = {Multimedia stream processing is a rapidly evolving domain which requires much software development and expects high performance. Developing a streaming application often involves low-level programming, critical memory management, and finely tuned scheduling of processing steps.To address these problems, we present a domain-specific language (DSL) named {\textless}i{\textgreater}Spidle{\textless}/i{\textgreater}, for specifying streaming applications. Spidle offers high-level and declarative constructs; compared to general-purpose languages (GPL), it improves robustness by enabling a variety of verifications to be performed.To assess the expressiveness of Spidle in practice, we have used it to specify a number of standardized and special-purpose streaming applications. These specifications are up to 2 times smaller than equivalent programs written in a GPL such as C.We have implemented a compiler for Spidle. Preliminary results show that compiled Spidle programs are roughly as efficient as the compiled, equivalent C programs.},
	booktitle = {Proceedings of the 2nd international conference on Generative programming and component engineering},
	publisher = {Springer-Verlag New York, Inc.},
	author = {Charles Consel and Hedi Hamdi and Laurent Réveillère and Lenin Singaravelu and Haiyan Yu and Calton Pu},
	year = {2003},
	pages = {1--17}
},

@inproceedings{halbwachs_synchronous_1998,
	title = {Synchronous Programming of Reactive Systems},
	isbn = {3-540-64608-6},
	url = {http://portal.acm.org/citation.cfm?id=647767.733784\&coll=GUIDE\&dl=GUIDE\&CFID=827015\&CFTOKEN=22989611},
	booktitle = {Proceedings of the 10th International Conference on Computer Aided Verification},
	publisher = {Springer-Verlag},
	author = {Nicolas Halbwachs},
	year = {1998},
	pages = {1--16}
},

@misc{_streamit_2006,
	title = {StreamIt Language Specification, Version 2.1},
	url = {http://cag.csail.mit.edu/streamit/papers/streamit-lang-spec.pdf},
	month = sep,
	year = {2006}
},

@techreport{asanovic_landscape_2006,
	title = {The Landscape of Parallel Computing Research: A View from Berkeley},
	url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
	number = {UCB/EECS-2006-183},
	institution = {EECS Department, University of California, Berkeley},
	author = {Krste Asanovic and Ras Bodik and Bryan Christopher Catanzaro and Joseph James Gebis and Parry Husbands and Kurt Keutzer and David A Patterson and William Lester Plishker and John Shalf and Samuel Webb Williams and Katherine A Yelick},
	month = dec,
	year = {2006}
},

@phdthesis{agrawal_linear_2004,
	type = {M.Eng. Thesis},
	title = {Linear State-Space Analysis and Optimization of StreamIt Programs},
	url = {http://cag.lcs.mit.edu/commit/papers/04/sitij-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Sitij Agrawal},
	month = aug,
	year = {2004}
},

@inproceedings{agrawal_optimizing_2005,
	address = {San Francisco, California, USA},
	title = {Optimizing stream programs using linear state space analysis},
	isbn = {1-59593-149-X},
	url = {http://portal.acm.org/citation.cfm?id=1086297.1086315\&coll=GUIDE\&dl=},
	doi = {10.1145/1086297.1086315},
	abstract = {Digital Signal Processing (DSP) is becoming increasingly widespread in portable devices. Due to harsh constraints on power, latency, and throughput in embedded environments, developers often appeal to signal processing experts to hand-optimize algorithmic aspects of the application. However, such DSP optimizations are tedious, error-prone, and expensive, as they require sophisticated domain-specific knowledge.We present a general model for automatically representing and optimizing a large class of signal processing applications. The model is based on linear state space systems. A program is viewed as a set of filters, each of which has an input stream, an output stream, and a set of internal states. At each time step, the filter produces some outputs that are a linear combination of the inputs and the state values; the state values are also updated in a linear fashion. Examples of linear state space filters include IIR filters and linear difference equations.Using the state space representation, we describe a novel set of program transformations, including combination of adjacent filters, elimination of redundant states and reduction of the number of system parameters. We have implemented the optimizations in the StreamIt compiler and demonstrate improved generality over previous techniques.},
	booktitle = {Proceedings of the 2005 international conference on Compilers, architectures and synthesis for embedded systems},
	publisher = {ACM},
	author = {Sitij Agrawal and William Thies and Saman Amarasinghe},
	year = {2005},
	keywords = {embedded,linear state space systems,stream programing,streamit,synchronous dataflow},
	pages = {126--136}
},

@inproceedings{gordon_stream_2002,
	address = {San Jose, California},
	title = {A stream compiler for communication-exposed architectures},
	isbn = {1-58113-574-2},
	url = {http://portal.acm.org/citation.cfm?id=605397.605428\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/605397.605428},
	abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.},
	booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
	publisher = {ACM},
	author = {Michael I. Gordon and William Thies and Michal Karczmarek and Jasper Lin and Ali S. Meli and Andrew A. Lamb and Chris Leger and Jeremy Wong and Henry Hoffmann and David Maze and Saman Amarasinghe},
	year = {2002},
	pages = {291--303}
},

@inproceedings{thies_unified_2001,
	address = {Snowbird, Utah, United States},
	title = {A unified framework for schedule and storage optimization},
	isbn = {1-58113-414-2},
	url = {http://portal.acm.org/citation.cfm?id=378795.378852\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	doi = {10.1145/378795.378852},
	abstract = {We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage mappings that collapse one dimension of a multi-dimensional array, and programs that are in a single assignment form with a one-dimensional schedule. Our technique combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.},
	booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
	publisher = {ACM},
	author = {William Thies and Frédéric Vivien and Jeffrey Sheldon and Saman Amarasinghe},
	year = {2001},
	pages = {232--242}
},

@misc{mattson_streaming_2003,
	address = {Dedham, MA},
	title = {"Streaming" as a Pattern},
	url = {http://cag.csail.mit.edu/wss03/},
	author = {Peter Mattson and Richard Lethin},
	month = aug,
	year = {2003}
},

@article{halbwachs_synchronous_1991,
	title = {The synchronous data flow programming language LUSTRE},
	volume = {79},
	issn = {0018-9219},
	doi = {10.1109/5.97300},
	abstract = {The authors describe LUSTRE, a data flow synchronous language designed for programming reactive systems-such as automatic control and monitoring systems-as well as for describing hardware. The data flow aspect of LUSTRE makes it very close to usual description tools in these domains (block-diagrams, networks of operators, dynamical sample-systems, etc.), and its synchronous interpretation makes it well suited for handling time in programs. Moreover, this synchronous interpretation allows it to be compiled into an efficient sequential program. The LUSTRE formalism is very similar to temporal logics. This allows the language to be used for both writing programs and expressing program properties, which results in an original program verification methodology},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {N. Halbwachs and P. Caspi and P. Raymond and D. Pilaud},
	year = {1991},
	keywords = {description tools,LUSTRE,parallel languages,program verification,program verification methodology,reactive systems,sequential program,synchronous data flow programming language,temporal logic,temporal logics},
	pages = {1305--1320}
},

@article{urbanski_digital_2006,
	title = {Digital microfluidics using soft lithography},
	volume = {6},
	issn = {1473-0197},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16372075},
	doi = {10.1039/b510127a},
	abstract = {Although microfluidic chips have demonstrated basic functionality for single applications, performing varied and complex experiments on a single device is still technically challenging. While many groups have implemented control software to drive the pumps, valves, and electrodes used to manipulate fluids in microfluidic devices, a new level of programmability is needed for end users to orchestrate their own unique experiments on a given device. This paper presents an approach for programmable and scalable control of discrete fluid samples in a polydimethylsiloxane (PDMS) microfluidic system using multiphase flows. An immiscible fluid phase is utilized to separate aqueous samples from one another, and a novel "microfluidic latch" is used to precisely align a sample after it has been transported a long distance through the flow channels. To demonstrate the scalability of the approach, this paper introduces a "general-purpose" microfluidic chip containing a rotary mixer and addressable storage cells. The system is general purpose in that all operations on the chip operate in terms of unit-sized aqueous samples; using the underlying mechanisms for sample transport and storage, additional sensors and actuators can be integrated in a scalable manner. A novel high-level software library allows users to specify experiments in terms of variables (i.e., fluids) and operations (i.e., mixes) without the need for detailed knowledge about the underlying device architecture. This research represents a first step to provide a programmable interface to the microfluidic realm, with the aim of enabling a new level of scalability and flexibility for lab-on-a-chip experiments.},
	number = {1},
	journal = {Lab on a Chip},
	author = {John Paul Urbanski and William Thies and Christopher Rhodes and Saman Amarasinghe and Todd Thorsen},
	year = {2006},
	note = {PMID: 16372075},
	keywords = {Computers,Dimethylpolysiloxanes,Microfluidic Analytical Techniques,Printing,Sensitivity and Specificity},
	pages = {96--104}
},

@misc{_actors:model_????,
	title = {ACTORS: A Model of Concurrent Computation in Distributed Systems},
	url = {http://dspace.mit.edu/handle/1721.1/6952},
	howpublished = {http://dspace.mit.edu/handle/1721.1/6952}
},

@inproceedings{geilen_minimising_2005,
	address = {Anaheim, California, USA},
	title = {Minimising buffer requirements of synchronous dataflow graphs with model checking},
	isbn = {1-59593-058-2},
	url = {http://portal.acm.org/citation.cfm?id=1065579.1065796},
	doi = {10.1145/1065579.1065796},
	abstract = {Signal processing and multimedia applications are often implemented on resource constrained embedded systems. It is therefore important to find implementations that use as little resources as possible. These applications are frequently specified as synchronous dataflow graphs. Communication between actors of these graphs requires storage capacity. In this paper, we present an exact method to determine the minimum storage capacity required to execute the graph using model-checking techniques. This can be done for different measures of storage capacity. The problem is known to be NP-complete and because of this, existing buffer minimisation techniques are heuristics and hence not exact. Modern model-checking tools are quite efficient and they have been successfully applied to scheduling-related problems. We study the feasibility of this approach with examples.},
	booktitle = {Proceedings of the 42nd annual conference on Design automation},
	publisher = {ACM},
	author = {Marc Geilen and Twan Basten and Sander Stuijk},
	year = {2005},
	keywords = {buffering,model-checking,optimization,synchronous dataflow},
	pages = {819--824}
},

@phdthesis{drake_stream_2006,
	type = {M.Eng. Thesis},
	title = {Stream Programming for Image and Video Compression},
	url = {http://cag.lcs.mit.edu/commit/papers/06/madrake-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Matthew Drake},
	month = may,
	year = {2006}
},

@article{forster_using_1992,
	title = {Using microcomputers for rapid data collection in developing countries},
	volume = {7},
	url = {http://heapol.oxfordjournals.org/cgi/content/abstract/7/1/67},
	doi = {10.1093/heapol/7.1.67},
	abstract = {Effective health planning requires good quality data, but many health facilities lack the ability to provide this. Health questions often have to be answered within specific research studies. Microcomputers are now generally recommended and used by researchers for data analysis at the end of projects. This paper reviews the use of microcomputer-based management of data collection during a study. A selection of projects are described, all of which have used microcomputers in a decentralized fashion, closer to the point of data collection. The main advantages of this approach are a significant reduction in error rates, and the ability to produce data quickly. },
	number = {1},
	journal = {Health Policy Plan.},
	author = {DAYO FORSTER and BOB SNOW},
	month = mar,
	year = {1992},
	pages = {67--71}
},

@phdthesis{reyes_graph_2004,
	type = {M.Eng. Thesis},
	title = {A Graph Editing Framework for the StreamIt Language},
	url = {http://cag.lcs.mit.edu/commit/papers/04/reyes-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Juan C Reyes},
	month = jun,
	year = {2004}
},

@misc{lauwereins_grape-ii:system-level_1995-1,
	type = {text},
	title = {Grape-II: A System-Level Prototyping Environment for DSP Applications},
	url = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/co/\&toc=comp/mags/co/1995/02/r2toc.xml\&DOI=10.1109/2.347998},
	author = {Rudy  Lauwereins},
	month = feb,
	year = {1995},
	note = {Grape-II (Graphical Rapid Prototyping Environment) is an advanced system-level development environment for specifying, compiling, debugging, simulating, and emulating digital-signal-processing applications. Its structured prototyping methodology reduces programming effort, and its use of general-purpose reusable hardware minimizes development cost.   The general-purpose hardware consists of commercial DSP processors, bond-out versions of core processors, and FPGAs linked to form a powerful, heterogeneous multiprocessor, such as the Paradigm RP developed within the Retides (Real-Time DSP Emulation System) Esprit project and marketed by InCA/Zycad. Grape-II automates the prototyping methodology for these systems by offering tools for resource estimation, partitioning, assignment, routing, scheduling, code generation, and parameter modification.   This prototyping approach has been successfully used for an audio processor for the consumer market, for a sender, receiver and channel simulator for digital audio broadcasting, and for a real- time video encoder for mobile applications.   The video-encoder case study, described in the article, resulted in a full-speed operational prototype. This and other successes demonstrate the feasibility of the authors' strategy for prototyping real-time color video compression on a commercial DSP multiprocessor.},
	howpublished = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/co/\&toc=comp/mags/co/1995/02/r2toc.xml\&DOI=10.1109/2.347998}
},

@misc{thies_searchingworld_2002,
	title = {Searching the World Wide Web in Low-Connectivity Communities},
	url = {http://wwwconf.ecs.soton.ac.uk/archive/00000218/},
	abstract = {The Internet has the potential to deliver information to communities around the world that have no other information resources. High telephone and ISP fees - in combination with low-bandwidth connections - make it unaffordable for many people to browse the Web online. We are developing the TEK system to enable users to search the Web using only email. TEK stands for "Time Equals Knowledge," since the user exchanges time (waiting for email) for knowledge. The system contains three components: 1) the client, which provides a graphical interface for the end user, 2) the server, which performs the searches from MIT, and 3) a reliable email-based communication protocol between the client and the server. The TEK search engine differs from others in that it is designed to return low-bandwidth results, which are achieved by special filtering, analysis, and compression on the server side. We believe that TEK will bring Web resources to people who otherwise would not be able to afford them.},
	author = {William Thies and Janelle Prevost and Tazeen Mahtab and Genevieve T. Cuevas and Saad Shakhshir and Alexandro Artola and Binh D. Vo and Yuliya Litvak and Sheldon Chan and Sid Henderson and Mark Halsey and Libby Levison and Saman Amarasinghe},
	year = {2002},
	keywords = {GC-01 A Global Society? [1],},
	comment = {WWW Conferences Archive [http://wwwconf.ecs.soton.ac.uk/perl/oai2] (United Kingdom) ER}
},

@article{berry_esterel_1992,
	title = {The ESTEREL synchronous programming language: design, semantics, implementation},
	volume = {19},
	url = {http://portal.acm.org/citation.cfm?id=147276.147279},
	number = {2},
	journal = {Sci. Comput. Program.},
	author = {Gérard Berry and Georges Gonthier},
	year = {1992},
	pages = {87--152}
},

@inbook{thies_abstraction_2006,
	title = {Abstraction Layers for Scalable Microfluidic Biocomputers},
	url = {http://dx.doi.org/10.1007/11925903\_24},
	abstract = {Microfluidic devices are emerging as an attractive technology for automatically orchestrating the reactions needed in a biological computer. Thousands of microfluidic primitives have already been integrated on a single chip, and recent trends indicate that the hardware complexity is increasing at rates comparable to Moore’s Law. As in the case of silicon, it will be critical to develop abstraction layers—such as programming languages and Instruction Set Architectures (ISAs)—that decouple software development from changes in the underlying device technology. Towards this end, this paper presents BioStream, a portable language for describing biology protocols, and the Fluidic ISA, a stable interface for microfluidic chip designers. A novel algorithm translates microfluidic mixing operations from the BioStream layer to the Fluidic ISA. To demonstrate the benefits of these abstraction layers, we build two microfluidic chips that can both execute BioStream code despite significant differences at the device level. We consider this to be an important step towards building scalable biological computers. },
	journal = {DNA Computing},
	author = {William Thies and John Urbanski and Todd Thorsen and Saman Amarasinghe},
	year = {2006},
	pages = {308--323}
},

@phdthesis{zhang_streaming_2007,
	type = {M.Eng. Thesis},
	title = {A Streaming Computation Framework for the Cell Processor},
	url = {http://cag.lcs.mit.edu/commit/papers/07/zhang-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Xin David Zhang},
	month = aug,
	year = {2007}
},

@article{johnston_advances_2004,
	title = {Advances in dataflow programming languages},
	volume = {36},
	url = {http://portal.acm.org/citation.cfm?id=1013208.1013209},
	doi = {10.1145/1013208.1013209},
	abstract = {Many developments have taken place within dataflow programming languages in the past decade. In particular, there has been a great deal of activity and advancement in the field of dataflow visual programming languages. The motivation for this article is to review the content of these recent developments and how they came about. It is supported by an initial review of dataflow programming in the 1970s and 1980s that led to current topics of research. It then discusses how dataflow programming evolved toward a hybrid von Neumann dataflow formulation, and adopted a more coarse-grained approach. Recent trends toward dataflow visual programming languages are then discussed with reference to key graphical dataflow languages and their development environments. Finally, the article details four key open topics in dataflow programming languages.},
	number = {1},
	journal = {ACM Comput. Surv.},
	author = {Wesley M. Johnston and J. R. Paul Hanna and Richard J. Millar},
	year = {2004},
	keywords = {component software,co-ordination languages,data flow visual programming,dataflow,graphical programming,multithreading,software engineering},
	pages = {1--34}
},

@inproceedings{thies_streamit:language_2002,
	title = {StreamIt: A Language for Streaming Applications},
	isbn = {3-540-43369-4},
	url = {http://portal.acm.org/citation.cfm?id=647478.727935\&coll=GUIDE\&dl=\&CFID=80417411\&CFTOKEN=42786916},
	booktitle = {Proceedings of the 11th International Conference on Compiler Construction},
	publisher = {Springer-Verlag},
	author = {William Thies and Michal Karczmarek and Saman P. Amarasinghe},
	year = {2002},
	pages = {179--196}
},

@phdthesis{sermulins_cache_2005,
	type = {M.Eng. Thesis},
	title = {Cache Optimizations for Stream Programs},
	url = {http://cag.lcs.mit.edu/commit/papers/05/sermulins-meng-thesis.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Janis Sermulins},
	month = may,
	year = {2005}
},

@inproceedings{thies_practical_2007,
	title = {A Practical Approach to Exploiting Coarse-Grained Pipeline Parallelism in C Programs},
	isbn = {0-7695-3047-8},
	url = {http://portal.acm.org/citation.cfm?id=1331699.1331731},
	abstract = {The emergence of multicore processors has heightened the need for effective parallel programming practices. In addition to writing new parallel programs, the next gener- ation of programmers will be faced with the overwhelming task of migrating decades' worth of legacy C code into a parallel representation. Addressing this problem requires a toolset of parallel programming primitives that can broadly apply to both new and existing programs. While tools such as threads and OpenMP allow programmers to express task and data parallelism, support for pipeline parallelism is distinctly lacking. In this paper, we offer a new and pragmatic approach to leveraging coarse-grained pipeline parallelism in C pro- grams. We target the domain of streaming applications, such as audio, video, and digital signal processing, which exhibit regular flows of data. To exploit pipeline paral- lelism, we equip the programmer with a simple set of an- notations (indicating pipeline boundaries) and a dynamic analysis that tracks all communication across those bound- aries. Our analysis outputs a stream graph of the applica- tion as well as a set of macros for parallelizing the program and communicating the data needed. We apply our method- ology to six case studies, including MPEG-2 decoding, MP3 decoding, GMTI radar processing, and three SPEC bench- marks. Our analysis extracts a useful block diagram for each application, and the parallelized versions offer a 2.78x mean speedup on a 4-core machine.},
	booktitle = {Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture},
	publisher = {IEEE Computer Society},
	author = {William Thies and Vikram Chandrasekhar and Saman Amarasinghe},
	year = {2007},
	pages = {356--369}
},

@article{murthy_multidimensional_2002,
	title = {Multidimensional synchronous dataflow},
	volume = {50},
	issn = {1053-587X},
	doi = {10.1109/TSP.2002.800830},
	abstract = {Signal flow graphs with dataflow semantics have been used in signal processing system simulation, algorithm development, and real-time system design. Dataflow semantics implicitly expose function parallelism by imposing only a partial ordering constraint on the execution of functions. One particular form of dataflow called synchronous dataflow (SDF) has been quite popular in programming environments for digital signal processing (DSP) since it has strong formal properties and is ideally suited for expressing multirate DSP algorithms. However, SDF and other dataflow models use first-in first-out (FIFO) queues on the communication channels and are thus ideally suited only for one-dimensional (1-D) signal processing algorithms. While multidimensional systems can also be expressed by collapsing arrays into 1-D streams, such modeling is often awkward and can obscure potential data parallelism that might be present. SDF can be generalized to multiple dimensions; this model is called multidimensional synchronous dataflow (MDSDF). This paper presents MDSDF and shows how MDSDF can be efficiently used to model a variety of multidimensional DSP systems, as well as other types of systems that are not modeled elegantly in SDF. However, MDSDF generalizes the FIFO queues used in SDF to arrays and, thus, is capable only of expressing systems sampled on rectangular lattices. This paper also presents a generalization of MDSDF that is capable of handling arbitrary sampling lattices and lattice-changing operations such as nonrectangular decimation and interpolation. An example of a practical system is given to show the usefulness of this model. The key challenge in generalizing the MDSDF model is preserving static schedulability, which eliminates the overhead associated with dynamic scheduling, and preserving a model where data parallelism, as well as functional parallelism, is fully explicit},
	number = {8},
	journal = {Signal Processing, IEEE Transactions on},
	author = {P.K. Murthy and E.A. Lee},
	year = {2002},
	keywords = {algorithm development,communication channels,data flow computing,data flow graphs,data parallelism,dataflow semantics,digital signal processing,FIFO queues,first-in first-out queues,functional parallelism,graphical programming model,interpolation,lattice-changing operations,multidimensional DSP systems,multidimensional signal processing,multidimensional synchronous dataflow,multirate DSP algorithms,nonrectangular decimation,partial ordering constraint,programming environments,queueing theory,real-time system design,rectangular lattices,sampling lattices,signal flow graphs,signal processing system simulation,signal sampling,static scheduling},
	pages = {2064--2079}
},

@phdthesis{agha_actors:model_1985,
	type = {Ph.D. Thesis},
	title = {Actors:  A Model of Concurrent Computation in Distributed Systems},
	school = {Massachusetts Insitute of Technology},
	author = {Gul Agha},
	year = {1985},
	keywords = {actors}
},

@inproceedings{parks_comparison_1995,
	title = {A comparison of synchronous and cycle-static dataflow},
	url = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/\&toc=comp/proceedings/asilomar/1995/7370/00/7370toc.xml\&DOI=10.1109/ACSSC.1995.540541},
	abstract = {We compare synchronous dataflow (SDF) and cyclo-static dataflow (CSDF), which are each special cases of a model of computation we call dataflow process networks. In SDF actors have static firing rules: they consume and produce a fixed number of data tokens in each firing. This model is well suited to multirate signal processing applications and lends itself to efficient static scheduling, avoiding the run-time scheduling overhead incurred by general implementations of process networks. In CSDF which is a generalization of SDF actors have cyclically changing firing rules. In some situations, the added generality of CSDF can unnecessarily complicate the scheduling. We show how higher-order functions can be used to transform a CSDF graph into a SDF graph, simplifying the scheduling problem. In other situations, CSDF has a genuine advantage over SDF: simpler precedence constraints. We show how this makes it possible to eliminate unnecessary computations and expose additional parallelism. We use digital sample rate conversion as an example to illustrate these advantages of CSDF. },
	booktitle = {Asilomar Conference on Signals, Systems, and Computers},
	author = {Thomas M Parks and Jose Luis Pino and Edward A Lee},
	month = oct,
	year = {1995},
	keywords = {data flow computing; scheduling; data flow graphs; signal sampling; cycle-static dataflow; synchronous dataflow; computation model; dataflow process networks; static firing rules; data tokens; multirate signal processing applications; static scheduling; higher-order functions; CSDF graph; SDF graph; scheduling problem; precedence constraints; parallelism; digital sample rate conversion}
},

@book{bhattacharyya_software_1996,
	title = {Software Synthesis from Dataflow Graphs},
	isbn = {0792397223},
	url = {http://portal.acm.org/citation.cfm?id=547038},
	abstract = {From the Publisher:Software Synthesis from Dataflow Graphs addresses the problem of generating efficient software implementations from applications specified as synchronous dataflow graphs for programmable digital signal processors (DSPs) used in embedded real-time systems. Software Synthesis from Dataflow Graphs reviews the state-of-the-art in constructing static, memory-optimal schedules for programs expressed as SDF graphs. Code size reduction is obtained by the careful organization of loops in the target code. Data buffering is optimized by constructing the loop hierarchy in provably optimal ways for many classes of SDF graphs. The central result is a uniprocessor scheduling framework that provably synthesizes the most compact looping structures, called single appearance schedules, for a certain class of SDF graphs. In addition, algorithms and heuristics are presented that generate single appearance schedules optimized for data buffering usage. Numerous practical examples and extensive experimental data are provided to illustrate the efficacy of these techniques.},
	publisher = {Kluwer Academic Publishers},
	author = {Shuvra S. Bhattacharyya and Edward A. Lee and Praveen K. Murthy},
	year = {1996},
	pages = {192}
},

@phdthesis{gordon_stream-aware_2002,
	type = {S.M. Thesis},
	title = {A Stream-Aware Compiler for Communication-Exposed Architectures},
	url = {http://cag.lcs.mit.edu/commit/papers//02/mgordon-thesis-SM.pdf},
	school = {Massachusetts Institute of Technology},
	author = {Michael Gordon},
	month = aug,
	year = {2002}
},

@inproceedings{fisher_pads:domain-specific_2005,
	address = {Chicago, IL, USA},
	title = {PADS: a domain-specific language for processing ad hoc data},
	isbn = {1-59593-056-6},
	url = {http://portal.acm.org/citation.cfm?id=1065010.1065046},
	doi = {10.1145/1065010.1065046},
	abstract = {PADS is a declarative data description language that allows data analysts to describe both the physical layout of ad hoc data sources and semantic properties of that data. From such descriptions, the PADS compiler generates libraries and tools for manipulating the data, including parsing routines, statistical profiling tools, translation programs to produce well-behaved formats such as Xml or those required for loading relational databases, and tools for running XQueries over raw PADS data sources. The descriptions are concise enough to serve as "living" documentation while flexible enough to describe most of the ASCII, binary, and Cobol formats that we have seen in practice. The generated parsing library provides for robust, application-specific error handling.},
	booktitle = {Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation},
	publisher = {ACM},
	author = {Kathleen Fisher and Robert Gruber},
	year = {2005},
	keywords = {data description language,domain-specific languages},
	pages = {295--304}
},

@article{frerichs_computer-assisted_1989,
	title = {Computer-assisted rapid surveys in developing countries.},
	volume = {104},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1580285\&amp;rendertype=abstract},
	abstract = {Health surveys are an important source of population-based data in much of the developing world. Unfortunately, sample surveys often take more time to plan, process, and analyze than is practical, given the information needs of the local decision-makers. Rapid survey methodology (RSM) has been developed to permit health professionals to answer quickly questions about the health status and activities of people at the community level. These answers may be necessary for determining program priorities or for monitoring program activities. Rapid surveys are meant to supplement, rather than replace, information derived from existing sources of vital and health statistics data. RSM combines sample survey methods with contemporary software used in portable, battery-powered microcomputers. The ability to do rapid surveys in developing countries also requires knowledge of how to use appropriate computer hardware and software and how to apply cluster sampling theory in the local environment. RSM was used for the first time in Hlegu Township, Burma, to conduct a health survey of young children. The survey team started the field work on May 4, 1987. Four days later, while still in the field, the data were processed and rapidly analyzed by portable microcomputers for presentation to the local township medical officer and his staff. Within 10 days of starting the field work, we issued a detailed 50-page report of the study findings. This paper provides (a) a description of the components of rapid survey methodology, including the sample survey method, computer hardware, and computer software; (b) the general requirements for portable computer hardware in less developed regions of the world; (c) the procedures for doing a rapid survey; and (d) a summary of our experiences with RSM in Burma.},
	number = {1},
	journal = {Public Health Reports},
	author = {R R Frerichs and K T Tar},
	month = feb,
	year = {1989},
	note = {PMC1580285},
	pages = {14–23}
},

@inproceedings{levison_providing_2002,
	title = {Providing Web search capability for low-connectivity communities},
	doi = {10.1109/ISTAS.2002.1013800},
	abstract = {There are many technical problems that exist in communities other than our own. These problems both deserve our attention and require focused research. We present one example: the TEK Search Engine. TEK is an email-based search engine designed to deliver low-bandwidth information to low-connectivity communities.},
	booktitle = {Technology and Society, 2002. (ISTAS'02). 2002 International Symposium on},
	author = {L. Levison and W. Thies and S. Amarasinghe},
	year = {2002},
	keywords = {electronic mail,email-based search engine,Internet,low-bandwidth information,low-connectivity communities,search engines,technical problems,TEK Search Engine,Web search capability},
	pages = {87--91}
},

@article{govindarajan_minimizing_2002,
	title = {Minimizing Buffer Requirements under Rate-Optimal Schedule in Regular Dataflow Networks},
	volume = {31},
	url = {http://dx.doi.org/10.1023/A:1015452903532},
	doi = {10.1023/A:1015452903532},
	abstract = {Large-grain synchronous dataflow graphs or multi-rate graphs have the distinct feature that the nodes of the dataflow graph fire at different rates. Such multi-rate large-grain dataflow graphs have been widely regarded as a powerful programming model for DSP applications. In this paper we propose a method to minimize buffer storage requirement in constructing rate-optimal compile-time (MBRO) schedules for multi-rate dataflow graphs. We demonstrate that the constraints to minimize buffer storage while executing at the optimal computation rate (i.e. the maximum possible computation rate without storage constraints) can be formulated as a unified linear programming problem in our framework. A novel feature of our method is that in constructing the rate-optimal schedule, it directly minimizes the memory requirement by choosing the schedule time of nodes appropriately. Lastly, a new circular-arc interval graph coloring algorithm has been proposed to further reduce the memory requirement by allowing buffer sharing among the arcs of the multi-rate dataflow graph.},
	number = {3},
	journal = {The Journal of VLSI Signal Processing},
	author = {R. Govindarajan and Guang R. Gao and Palash Desai},
	month = jul,
	year = {2002},
	pages = {207--229}
},

@phdthesis{chen_load-balanced_2005,
	type = {M.Eng. Thesis},
	title = {Load-balanced rendering on a general-purpose tiled architecture},
	copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
	url = {http://dspace.mit.edu/handle/1721.1/33115},
	school = {Massachusetts Insitute of Technology},
	author = {Jiawen Chen},
	year = {2005},
	note = {Thesis (M. Eng.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2005.},
	keywords = {Electrical Engineering and Computer Science.}
},

@article{kahn_semantics_1974,
	title = {The semantics of a simple language for parallel programming},
	journal = {Information Processing},
	author = {G Kahn},
	year = {1974},
	pages = {471--475}
},

@article{lee_gabriel:design_1989,
	title = {Gabriel: a design environment for DSP},
	volume = {37},
	issn = {0096-3518},
	doi = {10.1109/29.46557},
	abstract = {Gabriel is a software system intended to manage the complete development of real-time digital signal processing (DSP) applications, from conception and experimentation to implementation in real-time hardware. It performs non-real-time simulations as well as code synthesis for real-time hardware. It is intended to ease code development for architectures that are not easy targets for conventional compilers, such as multiprocessor systems built with very high-performance microcoded DSPs. The system is designed to be retargetable in two ways. First, it can synthesize code for a variety of multi-DSP architectures where the user specifies the salient features of the architecture. Second, it can target different DSPs. The authors have concentrated on code generation for the Motorola DSP56001, although code generation for the AT\&T DSP32 has been demonstrated. At the highest level, an algorithm is described using a hierarchical block diagram. At the lowest level, the user can either simulate the algorithm locally on the workstation, simulate the target architecture running the generated code, or download the code into hardware and run it in real time. Gabriel is capable of handling multiple sample rates, iteration, and recurrences},
	number = {11},
	journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
	author = {E.A. Lee and W.-H. Ho and E.E. Goei and J.C. Bier and S. Bhattacharyya},
	year = {1989},
	keywords = {AT\&,code synthesis,computerised signal processing,digital signal processing chips,DSP,Gabriel,hierarchical block diagram,Motorola DSP56001,multi-DSP architectures,programming environments,real-time digital signal processing,real-time hardware,software system,T DSP32},
	pages = {1751--1762}
},

@article{watkins_mash_2006,
	title = {Mash hits},
	journal = {The Guardian},
	author = {David Watkins},
	month = apr,
	year = {2006}
},

@inproceedings{buck_multirate_1991,
	title = {Multirate signal processing in Ptolemy},
	isbn = {1520-6149},
	doi = {10.1109/ICASSP.1991.150620},
	abstract = {The use of two models of computation, synchronous dataflow (SDF) and dynamic dataflow (DDF), to design and implement signal processing applications with multiple sample rates is discussed. The SDF model is used for synchronous applications. SDF is amenable to compile-time scheduling, and hence is much more efficient at runtime. The design environment, Ptolemy, can simultaneously support multiple models of computation, so SDF and DDF can be combined in a single application. Hence, the implementation will incur the run-time cost of DDF only for those asynchronous portions that absolutely must incur such cost. As an illustration, the authors detail a synchronous application, sample-rate conversion using polyphase filters, and an asynchronous application, timing recovery for an amplitude-shift-keyed signal},
	booktitle = {Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on},
	author = {J. Buck and S. Ha and E.A. Lee and D.G. Messerschmitt},
	year = {1991},
	keywords = {amplitude-shift-keyed signal,asynchronous application,circuit CAD,circuit simulation,compile-time scheduling,computer aided design,design environment,digital simulation,dynamic dataflow,multiple sample rates,polyphase filters,Ptolemy,run-time cost,sample-rate conversion,signal processing,signal processing applications,software tools,synchronisation,synchronous dataflow,timing recovery},
	pages = {1245--1248 vol.2}
},

@article{murthy_buffer_2004,
	title = {Buffer merging--a powerful technique for reducing memory requirements of synchronous dataflow specifications},
	volume = {9},
	url = {http://portal.acm.org/citation.cfm?id=989995.989999},
	doi = {10.1145/989995.989999},
	abstract = {We develop a new technique called buffer merging for reducing memory requirements of synchronous dataflow (SDF) specifications. SDF has proven to be an attractive model for specifying DSP systems, and is used in many commercial tools like System Canvas, SPW, and Cocentric. Good synthesis from an SDF specification depends crucially on scheduling, and memory is an important metric for generating efficient schedules. Previous techniques on memory minimization have either not considered buffer sharing at all, or have done so at a fairly coarse level (the meaning of this will be made more precise in the article). In this article, we develop a buffer overlaying strategy that works at the level of an input/output edge pair of an actor. It works by algebraically encapsulating the lifetimes of the tokens on the input/output edge pair, and determines the maximum amount of the input buffer space that can be reused by the output. We develop the mathematical basis for performing merging operations, and develop several algorithms and heuristics for using the merging technique for generating efficient implementations. We show improvements of up to 48\&percnt; over previous techniques.},
	number = {2},
	journal = {ACM Trans. Des. Autom. Electron. Syst.},
	author = {Praveen K. Murthy and Shuvra S. Bhattacharyya},
	year = {2004},
	keywords = {array lifetime,block diagram compiler,buffer overlaying,dataflow,design methodology,dsp and embedded systems,graph coloring,lifetime analysis,memory optimization,path covering,synchronous dataflow},
	pages = {212--237}
},

@article{levin_evaluation_2006,
	title = {Evaluation of Spoken Dialogue Technology for Real-Time Health Data Collection},
	volume = {8},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1794008},
	doi = {10.2196/jmir.8.4.e30},
	number = {4},
	journal = {Journal of Medical Internet Research},
	author = {Esther Levin and Alex Levin},
	month = dec,
	year = {2006},
	note = {PMC1794008},
	keywords = {toread},
	pages = {e30}
},

@misc{_overview03.pdf_????,
	title = {overview03.pdf (application/pdf Object)},
	url = {http://ptolemy.eecs.berkeley.edu/publications/papers/03/overview/overview03.pdf},
	howpublished = {http://ptolemy.eecs.berkeley.edu/publications/papers/03/overview/overview03.pdf}
},

@inproceedings{colao_towardshigher-order_2004,
	address = {Pisa, Italy},
	title = {Towards a higher-order synchronous data-flow language},
	isbn = {1-58113-860-1},
	url = {http://portal.acm.org/citation.cfm?id=1017792\&dl=GUIDE\&coll=GUIDE},
	doi = {10.1145/1017753.1017792},
	abstract = {The paper introduces a higher-order synchronous data-flow language in which communication channels may themselves transport programs. This provides a mean to dynamically reconfigure data-flow processes. The language comes as a natural and strict extension of both lustre and lucy. This extension is conservative, in the sense that a first-order restriction of the language can receive the same semantics.We illustrate the expressivity of the language with some examples, before giving the formal semantics of the underlying calculus. The language is equipped with a polymorphic type system allowing types to be automatically inferred and a clock calculus rejecting programs for which synchronous execution cannot be statically guaranteed. To our knowledge, this is the first higher-order synchronous data-flow language where stream functions are first class citizens.},
	booktitle = {Proceedings of the 4th ACM international conference on Embedded software},
	publisher = {ACM},
	author = {Jean-Louis Colaço and Alain Girault and Grégoire Hamon and Marc Pouzet},
	year = {2004},
	keywords = {dynamic reconfiguration,functional programming,kahn processes,stream functions,synchronous data-flow programming language,type system},
	pages = {230--239}
},

@article{halbwachs_synchronous_1991-1,
	title = {The synchronous data flow programming language LUSTRE},
	volume = {79},
	issn = {0018-9219},
	doi = {10.1109/5.97300},
	abstract = {The authors describe LUSTRE, a data flow synchronous language designed for programming reactive systems-such as automatic control and monitoring systems-as well as for describing hardware. The data flow aspect of LUSTRE makes it very close to usual description tools in these domains (block-diagrams, networks of operators, dynamical sample-systems, etc.), and its synchronous interpretation makes it well suited for handling time in programs. Moreover, this synchronous interpretation allows it to be compiled into an efficient sequential program. The LUSTRE formalism is very similar to temporal logics. This allows the language to be used for both writing programs and expressing program properties, which results in an original program verification methodology},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {N. Halbwachs and P. Caspi and P. Raymond and D. Pilaud},
	year = {1991},
	keywords = {description tools,LUSTRE,parallel languages,program verification,program verification methodology,reactive systems,sequential program,synchronous data flow programming language,temporal logic,temporal logics},
	pages = {1305--1320}
},

@inproceedings{hewitt_universal_1973,
	title = {A Universal Modular ACTOR Formalism for Artificial Intelligence},
	booktitle = {IJCAI},
	author = {Carl Hewitt and Peter Bishop and Richard Steiger},
	year = {1973},
	keywords = {actors},
	pages = {235--245}
},

@inproceedings{mark_cg:system_2003,
	address = {San Diego, California},
	title = {Cg: a system for programming graphics hardware in a C-like language},
	isbn = {1-58113-709-5},
	url = {http://portal.acm.org/citation.cfm?id=882362},
	doi = {10.1145/1201775.882362},
	abstract = {The latest real-time graphics architectures include programmable floating-point vertex and fragment processors, with support for data-dependent control flow in the vertex processor. We present a programming language and a supporting system that are designed for programming these stream processors. The language follows the philosophy of C, in that it is a hardware-oriented, general-purpose language, rather than an application-specific shading language. The language includes a variety of facilities designed to support the key architectural features of programmable graphics processors, and is designed to support multiple generations of graphics architectures with different levels of functionality. The system supports both of the major 3D graphics APIs: OpenGL and Direct3D. This paper identifies many of the choices that we faced as we designed the system, and explains why we made the decisions that we did.},
	booktitle = {ACM SIGGRAPH 2003 Papers},
	publisher = {ACM},
	author = {William R. Mark and R. Steven Glanville and Kurt Akeley and Mark J. Kilgard},
	year = {2003},
	pages = {896--907}
},

@article{corbett_cancellphone_2008,
	chapter = {Magazine},
	title = {Can the Cellphone Help End Global Poverty?},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2008/04/13/magazine/13anthropology-t.html?\_r=3\&oref=slogin\&oref=slogin\&oref=slogin},
	journal = {The New York Times},
	author = {Sara Corbett},
	month = apr,
	year = {2008},
	keywords = {Cellular Telephones,China,Economic Conditions and Trends,Ghana,Motorola Inc,Nokia Corp,Polak, Paul,Tajikistan,Third World and Developing Countries,toread,Uzbekistan,Vodafone Group Plc,Yunus, Muhammad}
},

@inproceedings{zhang_lightweight_2007,
	title = {A Lightweight Streaming Layer for Multicore Execution},
	url = {http://cag.lcs.mit.edu/commit/papers/07/zhang-dascmp07.pdf},
	booktitle = {Workshop on Design, Architecture and Simulation of Chip Multi-Processors},
	author = {Xin David Zhang and Qiuyuan J. Li and Rodric Rabbah and Saman Amarasinghe},
	year = {2007}
}