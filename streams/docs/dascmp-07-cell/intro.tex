\section{Introduction}

Multicore architectures have become the rule rather than the exception
in the changing computing landscape. With single-core performance
limited by power consumption, memory latency, and circuit complexity,
almost all new architectures (certain mobile and embedded applications
excepted) are branching into more cores rather than better
cores. Exploiting parallelism has already become absolutely critical
if applications wish to make full use of current and future
architectures.

Compilers for traditional imperative languages are faced with a
daunting task when attempting to aid the programmer in this regard: it
is very difficult to automatically extract parallelism from a
sequential program written in a von Neumann language such as C. Much
of the time, the task of parallelizing a program remains in the hands
of the programmer, who must manually convert a single-threaded
sequential program into a multi-threaded parallel one. While doing so,
programmers must contend with issues specific to the architectures
they target, thereby limiting portability. They must also worry about race
conditions and a number of other bugs that typically plague
multi-threaded programs. Programmers do have access to a number of
frameworks such as MPI and OpenMP to aid in their programming;
however, parallelization of sequential programs remains a difficult
process.

Streaming languages provide a way to alleviate the burden of manually
parallelizing applications. In a streaming language, the programmer
defines actors that operate on streams of data; the programmer then
composes actors and streams into a program. The structure that is
explicitly expressed by a streaming language exposes rather than hides
the parallelism present in a program, making it much easier for the
compiler to automatically extract parallelism. For the programmer,
many applications fit within the streaming model and can be naturally
expressed in various streaming languages.

Ideally, a compiler for a streaming language is able to focus on
high-level scheduling issues: finding parallelism and scheduling
actors to obtain the best possible utilization of available
computation resources. However, there are generally numerous low-level
issues the compiler must contend with, especially when presented with
a heterogeneous, distributed-memory architecture.

The Cell multicore architecture is one such example of a heterogeneous, 
distributed-memory architecture. Its design
is a trade-off favoring computing power, ease of manufacturing, and
low power consumption at the cost of increased programming
complexity. For programmers writing applications that run on Cell and
similar distributed-memory architectures, they are left with
additional programming complications to contend with: they not only
need to effectively orchestrate parallel computation, but must
carefully manage communication as well.

The goal of this paper is to create a general runtime framework for
streaming applications to alleviate these programming complexities for
multicores, especially distributed-memory architectures like Cell. The
paper \emph{i}) describes a multicore streaming layer (MSL) that abstracts
away the architecture-specific details that otherwise complicate the
scheduling of computation and communication in a stream program, and
\emph{ii}) demonstrates an implementation of the MSL for the Cell
processor. We chose Cell as a test target architecture because we
believe that (heterogeneous) multicore architectures with distributed
memory will be prevalent in the future, as they scale
better~\cite{theo-phd-07} than their shared-memory counterparts which
are predominant today.

The proposed MSL framework is primarily geared toward compilers rather
than programmers. Just as a programmer manually parallelizing a
sequential program has access to frameworks like MPI that abstract
certain low-level operations, the goal of the streaming framework is
to provide similar functionality to streaming language compilers and
schedulers that target multicore architectures. 

A noteworthy aspect of the MSL is its automatic management and
optimization of communication between cores. For example, a static (or
dynamic) scheduler targeting the MSL can transparently benefit from
double-buffering optimizations that can effectively hide communication
latencies. The automatic handling of communication lowers the burden
on compilers and programmers so that they are not involved in every
detail of the parallel computation.

The paper makes the following contributions:
\begin{enumerate}
\item A specification of a general runtime framework for streaming applications.
\item An implementation of such a runtime framework for the Cell processor.
\item A dynamic scheduler implemented on top of the runtime library
  that dynamically schedules computation and communication for
  streaming programs.
\item A static scheduler implemented on top of the runtime library
  that statically schedules streaming computation. The static
  scheduler relies on the runtime library to automatically manage
  communication.
\end{enumerate}

With our implementation for Cell, we achieve at least 97\% utilization
on data parallel applications, giving a reasonable 3\% overhead,
 and at least 88\% utilization on pipelined applications,
giving an acceptable 12\% overhead. The amount of code
needed is also significantly reduced compared to programming directly
at a lower level, thereby simplifying the implementation of a
scheduler.

%% new roadmap here
%%Chapters~2 presents basic background information and related work on streaming languages and parallelization frameworks, and describes the StreamIt language and the Cell architecture in detail. Chapter~3 presents the runtime library along with a discussion of design decisions and implementation issues. Chapter~4 describes how common scheduling patterns a compiler or programmer might generate can be mapped to the library, and compares code complexity and flexibility to programming for Cell directly. Chapter~5 discusses the dynamic scheduler. Chapter~6 gives a brief analysis of the performance of the framework on a test benchmark. Chapter~7 concludes with a discussion of future work and extensions.
