\section{Performance}\label{ch:perf}

We evaluate the performance of the MSL library and StreamIt compiler backend
on a set of four StreamIt applications and using different scheduling
methodologies. The applications are described in Table~\ref{fig:perf:apps}.
For statically-scheduled benchmarks, the scheduler executes a sufficiently
coarsened steady state to reduce library overhead, and imposes an explicit
synchronization barrier between steady state iterations. The dynamic scheduler
automatically coarsens work functions as necessary.

\begin{table}[!htb]
\begin{center}
\begin{tabular}{|l|p{2.25in}|}
\hline
BitonicSort & 8-element bitonic sort \\
\hline
DCT         & 16x16 IEEE reference DCT \\
\hline
FFT         & 256-element FFT \\
\hline
MPEG        & MPEG-2 block and motion vector decoding (subset of full MPEG-2 decoder) \\
\hline
\end{tabular}
\end{center}
\caption{Benchmark applications.}
\label{fig:perf:apps}
\end{table}

The benchmarks were run on PlayStation 3 hardware, which only provides
six usable SPEs. All benchmarks were scheduled to use six SPEs except
for the two MPEG benchmarks, which use five SPEs.
Benchmarks were run for a large number of steady state iterations
to reduce the effect of one-time execution startup overhead.
Performance results are given in Table~\ref{fig:perf:stats}.

\begin{table*}[!tb]
\begin{center}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
                             & Util (\%) & Lib (\%) & Sched (\%) & Min Sched (\%) & Max Sched (\%) & Throughput \\
\hline
\textsf{BitonicSort\_static} & 97.1 & 0.1 &   2.9 &       2.9 &       2.9 &        0.5 GOPs \\ \hline \hline
\textsf{DCT\_static}         & 97.7 & 1.0 &   1.3 &       1.3 &       1.3 &        3.2 GFLOPs \\ \hline \hline
\textsf{FFT\_c}              & 99.1 & --- &   --- &       --- &       --- &        2.5 GFLOPs \\ \hline
\textsf{FFT\_static}         & 98.6 & 0.6 &   0.9 &       0.9 &       0.9 &        1.9 GFLOPs \\ \hline
\textsf{FFT\_dynamic}        & 88.8 & 9.2 &   2.0 &       1.2 &       2.9 &        2.2 GFLOPs \\ \hline
\textsf{FFT\_pipeline}       & 78.0 & 3.6 &  18.5 &       4.2 &      33.9 &        1.9 GFLOPs \\ \hline \hline
\textsf{MPEG\_static}        & 95.1 & 1.0 &   3.9 &       1.5 &       8.1 &       46.9 fps \\ \hline
\textsf{MPEG\_dynamic}       & 97.8 & 1.7 &   0.5 &       0.3 &       0.8 &       48.2 fps \\ \hline
\hline
\end{tabular}
\end{center}
\label{fig:perf:stats}
\caption{Benchmark performance.}
\end{table*}

The \emph{Util} column gives the average SPE utilization of the
benchmark. This is the percentage of total execution time spent inside filter work functions,
averaged over all SPEs. The remainder is overhead, which we divide into two categories:
\emph{library overhead} and \emph{scheduling overhead}. \emph{Library overhead}
is time during which an SPE has active \textsf{filter\_run} commands but is not
running a work function. This type of overhead is added entirely by library code
when it is either dispatching or executing other commands.
\emph{Scheduling overhead} is time during which an SPE has no active
\textsf{filter\_run} commands. During this time, the SPE has no useful work to do.
It may be either \emph{i}) waiting for filters to be scheduled or \emph{ii})
waiting for sufficient input or output data to be transferred to allow a scheduled filter to run
(i.e., due to inadequate double-buffering).
When large, scheduling overhead can be viewed as resulting from the scheduling algorithm
(or the nature of the program). However, a component of scheduling overhead is also
due to the latency/efficiency with which the library executes commands.

The \emph{Lib} and \emph{Sched} columns give the average library and scheduling overhead
as a percentage of total execution time, averaged over all SPEs.
The \emph{Min Sched} and \emph{Max Sched} columns give the scheduling overhead
(as a percentage of total execution time) on the SPEs with the minimum and maximum
scheduling overhead; wider ranges indicate larger work imbalance
resulting from the scheduling algorithm.
For \textsf{BitonicSort\_static}, the \emph{Throughput} column gives
billions of compare operations per second. For the MPEG benchmarks,
the \emph{Throughput} column gives the number of 352x240 frames processed per second.
For all other benchmarks, the \emph{Throughput} column gives GFLOPs.

The \textsf{BitonicSort\_static}, \textsf{DCT\_static}, and \textsf{FFT\_static} benchmarks
are statically scheduled and generated by the StreamIt compiler.
These applications are fully data-parallel and the compiler fuses the stream graph
into a single filter, which is then duplicated to the number of cores.
As expected from fully data-parallel programs, average utilization remains nearly the same
as the number of SPEs is varied from one to six and scheduling overhead is essentially identical
on all SPEs, demonstrating nearly perfectly linear speedup.
The total overhead in each benchmark is less than 3\%. Scheduling overhead appears
to dominate total overhead, largely due to the synchronization barrier after each steady state
iteration that creates repeated additional costs as the program executes.

For programs consisting of a single fused data-parallel filter, the dynamic scheduler produces identical performance as static scheduling. Results for the dynamic scheduler on these applications are not separately given.

The \textsf{FFT\_dynamic} and \textsf{FFT\_pipeline} benchmarks are different manual implementations of the FFT application. The FFT stream graph consists of a single pipeline with 15 filters. \textsf{FFT\_dynamic} schedules this pipeline using the dynamic scheduler. 

In \textsf{FFT\_pipeline}, the stream graph is first manually fused into a pipeline of six filters, each of which is statically placed on a different SPE. All communication except for input and output is done directly between SPE local stores and remains entirely on-chip. GFLOPs numbers for \textsf{FFT\_dynamic} and \textsf{FFT\_pipeline} can be compared with each other but not directly to \textsf{FFT\_static}.
The former two have manual work function implementations that are slightly more efficient than the compiler-generated code.

For \textsf{FFT\_dynamic}, scheduling overhead is low and very similar across SPEs.
This indicates that the dynamic scheduler has no difficulties keeping all SPEs supplied with work.
Moreover, the results show that
if there is sufficient memory bandwidth, as is the case with the Cell architecture, it is practical to
perform all buffering to memory, avoiding core-to-core communication entirely.
Average utilization remains nearly identical as the number of SPEs is varied from one to six,
indicating almost perfectly linear speedup.

However, the library overhead on this benchmark is high, approaching 10\%, resulting in somewhat low average utilization. This overhead has two main sources: \emph{i}) individual filters in the pipeline have much lower communication--computation ratios than the single fused filter in the \textsf{FFT\_static} benchmark; and \emph{ii}) the dynamic scheduler continually issues additional commands to switch filters between SPEs, which are not needed in a static schedule.

For \textsf{FFT\_pipeline}, a single filter/SPE in the middle of the pipeline is the bottleneck.
Although average utilization is low, it is not due to library overhead, which is low.
Average scheduling overhead is high and varies widely between SPEs:
the bottleneck SPE had 92\% utilization, while the least-utilized SPE had only 63\% utilization.
In general, this illustrates the difficulty of performing direct static SPE--SPE pipelining.
Although SPE--SPE pipelining keeps communication on-chip, where extremely high bandwidth
is available and there is no danger of exhausting comparatively limited memory bandwidth,
work imbalances between filters make it difficult to fully utilize all SPEs.

For comparison, \textsf{FFT\_c} is a hand-tuned implementation of \textsf{FFT\_static} that
does not use the MSL library. The same work function code is used as \textsf{FFT\_dynamic} and
\textsf{FFT\_pipeline} (however, this is different from \textsf{FFT\_static},
which is generated by the StreamIt compiler) and data transfers are fully double-buffered.
Compared to \textsf{FFT\_c}, \textsf{FFT\_dynamic} is approximately 12\% slower, most of which
is due to library overhead. \textsf{FFT\_pipeline} is significantly slower, but this
is entirely due to the major work imbalance it exhibits.

MPEG has a small amount of state and cannot be fused into a single filter.
The compiler fuses the stream graph down to a single stateful filter and
a single stateless filter as the branches of a two-way splitjoin.
When mapping this to the MSL library, both statically- and dynamically-scheduled benchmarks
explicitly treat the scattering and gathering operations as separate stateless filters,
resulting in four total filters to schedule.

The \textsf{MPEG\_static} benchmark statically software-pipelines the four filters.
We generated the static schedule for five SPEs by manually profiling, duplicating, and
partitioning filters, and manually generating control code to issue the resulting schedule.
The mapping of filters to SPEs in the partition that we obtained is given in
Table~\ref{fig:perf:mpegs}.

\begin{table}[!htb]
\begin{center}
\begin{tabular}{|r|l|}
\hline
SPE & Filters \\
\hline
0 & $n$ splitter, $n$ stateful \\
\hline
1 & $6n$ joiner \\
\hline
2 & $2n$ stateless \\
\hline
3 & $2n$ stateless \\
\hline
4 & $2n$ stateless \\
\hline
\end{tabular}
\end{center}
\caption{Partition for \textsf{MPEG\_static} benchmark.}
\label{fig:perf:mpegs}
\end{table}

This partition exhibits a slight work imbalance: SPE 0 has slightly less work than SPE 1,
which has slightly less work than the remaining SPEs.
As a result, there is a small variation in scheduling overhead, resulting in somewhat lower
utilization than could be achieved.
The bottleneck SPEs were 98\% utilized, while SPE 0 was 90\% utilized.
Aside from the work imbalance, which can be reduced with better partitioning,
this benchmark shows that static software pipelining using the MSL library can be done
with very low overhead.

The \textsf{MPEG\_dynamic} benchmark uses the dynamic scheduler to schedule the four filters.
As with \textsf{FFT\_dynamic}, average utilization is very similar across all SPEs,
and there is still nearly perfectly linear speedup as the number of SPEs is varied.
Compared to the static schedule, the dynamic scheduler is not limited to executing
repetitions of a steady state and does not impose any synchronization barriers.
However, in order to dynamically switch filters on cores, it must issue more commands.
This translates into slightly lower scheduling overhead but slightly higher library overhead.
Overall, the dynamic scheduler is able to more efficiently distribute work across
all available cores, resulting in higher average utilization and 2.8\% increased throughput
compared to \textsf{MPEG\_static}.

It must be noted that the throughput obtained in these benchmarks is low
compared to the maximum performance attainable on Cell and the performance of other
implementations of the same benchmarks.
No SIMD vectorization (either automatic or manual) was performed within
filter work functions for any of the benchmarks;
as a result, they did not take advantage of the SIMD capability of Cell SPEs.
In addition, filter code could have been considerably optimized. For instance,
\textsf{FFT\_static} performs more integer operations to maintain loop counters than actual FLOPs.
However, the high utilization demonstrated in the benchmarks should extend to
real-world, optimized applications as long as individual filter work functions
perform a comparable amount of work.
