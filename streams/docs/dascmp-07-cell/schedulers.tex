\section{Scheduling Stream Programs Using the MSL}\label{ch:ds}

Streaming programs typically allow for a lot of freedom in terms of
orchestrating the parallel execution of the stream graph. This freedom is
afforded by the dataflow models of computation that many streaming
languages are founded on. In a stream program, the rules governing the
execution of a node or actor in the graph are often simple, and usually
reduce to having sufficient buffering on the input to an actor, and
sufficient buffering to store the output.

The execution of a stream program requires mapping and ordering actors
to cores, allocation of buffers, and managing data transfers between
buffers. Collectively, mapping, ordering, and buffer managements are
embodied in a schedule of execution.

It is possible to devise a schedule statically (e.g., compile time or
at graph creation time) or dynamically (e.g., runtime). The goal of a
scheduler is simply to maximize the throughput of the streaming
application. In the age of multicore architectures, a scheduler will
need to utilize multiple cores to increase concurrency and hence improve
the throughput of a given streaming application. 

A dynamic scheduler is conceptually easy to understand. The scheduler
maintains an internal representation of a given stream graph (e.g.,
list or priority queue). In a multicore architecture, when there is a
core available, the scheduler scans its internal representation of the
stream graph, and determines which actor is ready to fire.
The scheduler assigns the actor to the available core. The core is
also informed where the input buffer for the actor resides in memory,
and where in memory to commit (buffer) the output of the actor for its
successors. The scheduler then updates its internal representation to
indicate the actor firings and implement a fairness policy to assure
overall progress (e.g., a round-robin scheduler).

For stream graphs that are ``predictable'', dynamic scheduling
generally does not present any advantages over static
scheduling. Dynamic scheduling inevitably involves additional
communication and scheduling overhead due to extra filter loading and
unloading, buffer management, and scheduling computation. When all
filters in a program are data-parallel, a static scheduler can make
full use of all cores by simply executing each filter in turn on all
cores, with a sufficient coarsening of the steady state to amortize
filter load/unload and synchronization overhead. The optimal
situation results when the compiler can fuse all filters into a single
data-parallel filter; this produces the minimum possible communication.
This situation would also be optimal for a dynamic scheduler.

Even when filters are stateful and thus cannot be data-parallelized,
static software pipelining techniques~\cite{asplos06} can make full
use of cores when the compiler has an accurate static work estimator
and can divide filters in a steady state evenly across
cores. A single stateful filter with a heavily imbalanced work
function creates a bottleneck, but dynamic schedulers are also faced
with this problem. 
%%In addition, no ``unpredictable'' cache misses or
%%lengthy communication delays that can skew a static work estimate are
%%possible on the Cell architecture.

Dynamic scheduling becomes beneficial when filters are not
``predictable'': when it is difficult to statically balance load
across cores, difficult to estimate the amount of work done by filter
work functions, or work functions perform widely varying amounts of
work through the execution of the program. In these situations,
dynamic scheduling may be able to deliver better load-balancing than
static scheduling.

A dynamically scheduled program can be run on varying numbers of
processors without requiring recompilation or the reanalysis that
complex static schedulers would need to perform, and is also tolerant
of changes in the availability of processors while the program is
running. 
%% In addition, for stream graphs that contain filters with
%% dynamic rates, it may not be possible to statically predict how many
%% times filters will be run, and the balance of work in the stream graph
%% may change as the program is run. In this case, only dynamic
%% scheduling is able to shift workload to different portions of the
%% stream graph as needed.\footnote{However, the current dynamic
%% scheduler implementation does not support dynamic rates.}

%% \section{User Interface}\label{ch:ds:ui}

%% The user provides as input to the dynamic scheduler a complete
%% description of the stream graph, specifying filters and the
%% channels that connect them. Rates for all filters must be
%% specified. Duplicate splitters can be handled by setting parameters
%% of channels; round-robin splitters and joiners must be defined as
%% separate filters.

\subsection{Methodology}

We evaluated dynamic and static scheduling schemes for the Cell
multicore architecture. The Cell processor has 9
cores~\cite{Cell-hpca}: a PPE that serves as a general purpose
processor, and 8 SPEs that are designed to perform the bulk of the
computation. Each SPE has 256KB of local store and a DMA processor to
manage data in and out of the SPE. In our methodology, the schedule is
run on the PPE and the filters run on the SPEs.

We used the StreamIt~\cite{streamitweb} programming language to
generate the stream graph that is presented to the MSL and to both the
static and dynamic schedulers. The basic unit of computation in StreamIt is a 
{\it filter}. There are three basic constructs for composing filters
into a communicating network: a {\it pipeline}, a {\it splitjoin}, and
a {\it feedbackloop}. A pipeline behaves as the sequential composition
of all its child streams. A splitjoin is used to specify independent
parallel streams that diverge from a common {\it splitter} and merge
into a common {\it joiner}. A feedbackloop provides a way to create
cycles in the stream graph.

\subsection{Static Scheduler Implementation}

The Cell backend for the StreamIt compiler maps StreamIt programs to C code that
is run on the Cell processor through the MSL. The backend 
is built upon the robust StreamIt compiler infrastructure~\cite{asplos06}. 
Briefly, StreamIt code is converted into a
high-level stream IR which then undergoes a series of optimizing transformations.

In the case of static scheduling, the compiler processes the stream program
through four phases.
\begin{itemize}
\item In the scheduling phase, a steady state schedule, which maintains a constant
number of data items in each input and output buffer after every execution, is calculated based on each
filter's declared I/O rates. This steady state schedule can then
be run in an infinite loop. Additionally, an initialization schedule may be
generated to prime the buffers or initialize state.
\item In the partitioning phase, filters are fused or
fissed to achieve the best level of granularity for the best load balancing.
\item In the layout phase, filters are assigned to specific cores on which
they are run.
\item In the code generation phase, appropriate code that encapsulates the schedule,
partitioning, and layout are generated and run on the target architecture.
\end{itemize}

For our data parallel applications, our partitioning
phase fuses all filters into a single filter. We generate all code, both control and
filter work function code, through the backend. We then run this fused filter
in parallel across all SPEs. Many real-world applications are stateless and can
therefore be data-parallelized in this fashion.

%For MPEG2, we isolated the stateful filter and fused all other stateless filters.
%We then profiled these two filters to find a schedule that provides good load balancing
%between SPEs. We generate the work function code through the backend, but hand-coded
%the control code on the PPE.


\subsection{Dynamic Scheduler Implementation}\label{ch:ds:imp}

The Cell architecture's communication network provides very high
memory bandwidth. The design of the dynamic scheduler assumes that
memory bandwidth will never be a bottleneck, and the dynamic scheduler
buffers all output produced on SPEs to memory. The scheduler performs
dynamic course-grained software pipelining on the stream graph; if
sufficient data can be buffered in all channels at all times, pipeline
stalls can be avoided and all SPEs can be fully utilized. While
SPE--SPE communication is more efficient than SPE--memory
communication, SPE local store is generally too limited to store the
buffering needed for software pipelining, and thus the scheduler never
executes SPE--SPE pipelines; this avoids having to deal with work
imbalances between pairs of adjacent filters. At any time, any two
SPEs will typically be operating on data from different 
iterations of the program.

At startup, the dynamic scheduler allocates a large\footnote{1 MB in
the current implementation, but this can be adjusted.} buffer in
memory for each channel; this is used to buffer the output of the
upstream filter to provide input for the downstream filter. At any
time, for any specific filter, the amount of data available in its
input channels and amount of space available in its output channels,
along with its rates, determines the maximum number of iterations that
the filter can be run for.

The scheduler selects filters to run on SPEs based on a metric
computed from the maximum number of iterations and certain filter
properties (see below). When a filter is selected to run on an SPE, it
is scheduled for a limited but fairly large number of iterations in
order to amortize the cost of loading it. Filters run for their entire
allotment of iterations; however, allotments are kept small to allow
the scheduler to quickly schedule another filter if necessary in
response to the changing state in the stream graph.

A replacement filter for an SPE is selected when the current filter
scheduled on the SPE has almost finished running for all of its
allotted iterations. While the current filter is still running, the
scheduler issues additional commands to load the new filter, allocate
its buffers, and transfer data into its input buffers from memory;
this communication is overlapped with the computation done by the
current filter. When the replacement filter is the same as the current
filter, this additional work can be avoided. Finally, the first
command to run the new filter is queued after the last command to run
the current filter. When the replacement filter is selected early
enough, it will be set up on the SPE before the current filter
finishes running, ensuring that it can start running as soon as the
current filter finishes. When the current filter has completed all of
its allotted iterations, it is unloaded and can then be scheduled on
another SPE.

The dynamic scheduler can run multiple instances of data-parallel
filters on multiple SPEs at the same time. A data-parallel filter is
still selected by the same metric as other filters; it will only be
run on more than one SPE at once if it is significantly better than
other filters.

The current metric implemented is very simple: it prioritizes filters
based on the amount of data the state of their input and output
channels allow them to consume and produce, respectively. However, the
filter that is currently running on an SPE is prioritized when
considered for scheduling on the same SPE; this effectively causes
filters to be run for as long as possible on an SPE, with no load
overhead, while no other filters are significantly better. Other more
complex metrics can be easily substituted.

When the dynamic scheduler encounters a pipeline, the filter selection
metric quickly causes all filters in the pipeline to be run
sufficiently to generate some data in every channel
buffer. Thereafter, the sequence of filter executions selected by the
scheduler appears to perform software pipelining, although without a
recognizable steady state.

\subsection{Code Generation}

%Following the previous phases, code generation is the final phase in the StreamIt
%compiler. One option is to generate code directly targeting the Cell API.
%The disadvantages of this approach include the additional burden of
%dealing explicitly with DMA alignment requirements and the bloated code. Here,
%the library provides a convenient layer of abstraction which not only abstracts
%away DMA transfers but also provides for code that is more readable and easier
%for the compiler to generate.
For both static and dynamic scheduling cases, we must generate the code that
is to be run on the multicore. In the case of Cell, we generate C
code that utilizes the library and compile it with Cell's GCC. We run all
control code on the PPE and all filter initialization and work functions on the SPEs. 

We set up filter description parameters
specifying how many inputs and outputs a filter has, which input and output
buffers the filter reads from and writes to, and how many bytes the filter 
reads and writes in one execution. These parameters are then used by the
library to handles the necessary data transfers and executions of the
work function of the filter. 

In the case of dynamic scheduling, scheduling, partitioning, and layout
are handled at run-time by the dynamic scheduler. Thus, the compiler need only
generate the aforementioned code to set up the scheduler for execution.

In the case of static scheduling, we additionally set up filter layout parameters
which specify on which SPE a filer should be run. The init and steady state
schedules are explicit in the code: filters are loaded and run according to the
schedule, and callbacks are used to set up parameters for and to run the next filter in
the schedule.
