\section{Performance}\label{ch:perf}

Performance evaluation for the runtime library and dynamic scheduler was conducted by comparing different implementations of the FFT StreamIt program described in chapter~\ref{ch:use}. Six different implementations were tested:
\begin{enumerate}
\item FFT pipeline fused to single filter. All other infrastructure hand-coded and optimized.
\item Single fused filter, executed data-parallel using the library.
\item Full FFT pipeline, executed using dynamic scheduler, filters not specified as data-parallel.
\item Full FFT pipeline, executed using dynamic scheduler, filters specified as data-parallel.
\item Single fused filter, executed using dynamic scheduler, specified as data-parallel.
\item FFT pipeline partially fused to six filters, pipelined across six SPEs with direct SPE--SPE communication.
\end{enumerate}

The resulting programs were executed on PlayStation 3 hardware, which only provides six usable SPEs. During each execution, a program processed 20,000 iterations of data (approximately 20 MB). Each program iteration performs 16,384 floating point operations, for a total of 327 MFLOP. Runtimes for one and six SPEs are given in figure~\ref{fig:perf:fft} (file I/O time is excluded).

\begin{figure}[!htb]
\begin{center}
\begin{tabular}{|r|r|r|r|r|r|r|r|}
\hline
& \multicolumn{3}{|c|}{1 SPE} & \multicolumn{4}{|c|}{6 SPEs} \\
\cline{2-8}
& Time & Run \% & Work \% & Time & Run \% & Work \% & Speedup \\
\hline
\textsf{1} & 396.1 & \multicolumn{2}{|c|}{} & 66.6 & \multicolumn{2}{|c|}{} & 5.95 \\
\hline
\textsf{2} & 401.0 & 100.0 & 99.2 & 67.2 & 99.7 & 98.8 & 5.97 \\
\hline
\textsf{3} & 545.9 & 99.6 & 91.8 & 91.7 & 99.2 & 91.4 & 5.95 \\
\hline
\textsf{4} & \multicolumn{3}{|c|}{} & 91.9 & 99.0 & 91.2 & \\
\hline
\textsf{5} & 401.0 & 100.0 & 99.2 & 67.6 & 99.7 & 98.8 & 5.93 \\
\hline
\textsf{6} & \multicolumn{3}{|c|}{} & 86.2 & 95.9 & 92.2 & \\
\hline
\end{tabular}
\end{center}
\caption{Performance of different implementations of FFT.}
\label{fig:perf:fft}
\end{figure}

The \emph{Time} column gives the total time SPEs ran for, excluding one-time initialization done at the start of the program. Times are in milliseconds. For the five implementations that used the library, additional statistics maintained by the library are given. The \emph{Run \%} column gives the percentage of total time the SPE had an active \textsf{filter\_run} command. The remainder is overhead due to either \emph{i}) durations when SPEs do not have useful work to do (such as when waiting for filters to be loaded) or \emph{ii}) inadequate double-buffering of input or output data. In both cases, the scheduler (or the nature of the program) creates insufficient communication--computation concurrency. The \emph{Work \%} column gives the percentage of total time actually spent in the work function. The remainder represents the total overhead added by the library or caused by the scheduling algorithm. The large number of iterations tests ran for smoothes out any one-time execution startup overhead. For the tests involving six SPEs, the percentages given are for the SPE that reported the greatest work percentage. The \emph{Speedup} column gives the speedup relative to the same implementation on one SPE, where applicable.

Not surprisingly, hand-coded implementation \textsf{(1)} demonstrates almost perfect linear speedup on six SPEs.

Comparing results for implementation \textsf{(2)} to \textsf{(1)}, the overhead added by the library is around 1\%. Run \% is high, although not exactly 100\% due to the overhead of starting a long-term filter execution (loading the filter and allocating and attaching buffers). For both \textsf{(1)} and \textsf{(2)}, statistics for all SPEs were almost identical; this is not surprising given that the fused filter's work function performs a constant amount of work per iteration.

For implementations \textsf{(3)} and \textsf{(4)}, which involved the full pipeline on the dynamic scheduler, all six SPEs reported similar statistics (both absolute and percentage), indicating that the scheduler can make equal use of all available SPEs. Compared to implementation \textsf{(2)}, these implementations are 36\% slower. However, a large part of this can be accounted for by the observation that the total time spent in work functions in these implementations is 26\% greater. This is due to slightly more efficient code that is generated for the fused filter.

Run \% is slightly lower than in implementation \textsf{(2)}, but still fairly close to 100\%, and speedup on six SPEs is still almost perfectly linear; this indicates that the dynamic scheduler has no difficulties keeping all SPEs supplied with work. Moreover, it validates the critical assumption made in the design of the dynamic scheduler: the Cell architecture's memory bandwidth is indeed sufficient to perform all buffering to memory, avoiding SPE--SPE communication entirely. However, the lower Work \% indicates that the additional commands issued to SPEs by the dynamic scheduler to constantly switch filters do create a noticeable overhead: 8.6\% of total time, compared to 1.2\% for the data-parallel, fused, statically-scheduled implementation.

Not surprisingly, the almost identical results for implementations \textsf{(5)} and \textsf{(2)} indicate that the dynamic scheduler can execute a single fused data-parallel filter just as efficiently as a static schedule.

Implementation \textsf{(6)}, which pipelines multiple filters across SPEs, produces different results than the others. In this case, a single filter/SPE in the middle of the pipeline is a significant bottleneck; this is not surprising, since 15 filters were fused into six. Although total runtime is significantly worse than the hand-coded or fused data-parallel implementations, no SPE except for the bottleneck was fully utilized: two other SPEs had Work \% around 60\%. In general, this illustrates the difficulty of performing SPE--SPE pipelining. Although this implementation keeps communication on-chip (except for input and output to the pipeline), work imbalances make it difficult to fully utilize all SPEs.
