\section{Introduction}

Due to the inexorable advance of VLSI technology, transistors are
essentially free.  Microprocessor design is now dominated by wire
delay concerns.  One popular solution harnesses the plethora of
chip-level resources by replicating processing units.  This limits
wire delay because wires need not be longer than a replicated unit.
The multiple units communicate through shared memory or through an
on-chip communication network.  Recently the architecture community
has witnessed the ascendancy of {\it communication-exposed architectures},
examples include Raw \cite{raw}, Smart Memories
\cite{smartmemories}, Merrimac \cite{merrimac-sc03}, TRIPS
\cite{trips}, WaveScalar \cite{wavescalar}, and RDR \cite{rdr}.  These
machines propose to solve the wire delay problem by replicating
processing units and exposing the interconnect between these units to
a software layer.  The architecture remains simple and scalable, while
complexity is shifted to the compiler. For these architectures to gain
programmer acceptance, there must exist a high-level, portable
programming language that can be compiled efficiently to any of the
candidate targets.

A number of efforts have focused on stream programming as a paradigm
for producing high-level, efficient, and retargetable application code
for wire-exposed architectures \cite{streamit-asplos,imagine-ieee,merrimac-sc03,trips-isca03}.
In a stream program, computation is expressed as a set of filters that
operate over sequences of data.  Because each filter has an
independent address space and program counter, there is abundant task
and data parallelism that can be recognized by the compiler.  Also,
the data streams expose the communication between filters; the
compiler can predict the flow of data and orchestrate data movement.
These properties distinguish stream programs from imperative languages
such as C and FORTRAN.  While imperative languages were a good match
for von-Neumann machines, they are obsolete for architectures
containing multiple instruction streams and distributed memory banks.

\begin{figure*}[th]
  \centering
  \vspace{-18pt}
  \psfig{figure=space-vs-time.eps,width=4.5in}
  \vspace{-12pt}
  \caption{Techniques for scheduling stream programs. \protect\label{fig:spacevstime}}
  \vspace{-6pt}
\end{figure*}

Hitherto, there have been two basic approaches for compiling a stream
program to a communication-exposed architecture.  {\it Time
multiplexing} utilizes the entire chip for each filter, switching
between filters over time (see Figure~\ref{fig:spacevstime}b).  Time
multiplexing's efficacy extends from its freedom from having to
balance the workload between filters.  However, this technique can
lead to long latencies, increased memory traffic, and the utilization
is highly dependent upon how effectively each filter can be
parallelized across the machine.

Conversely, {\it space multiplexing} distributes filters across the
entire chip, running them continuously and in parallel (see
Figure~\ref{fig:spacevstime}a).  Space multiplexing affords $(i)$ no
filter swapping, $(ii)$ reduced memory traffic, $(iii)$ localized
communication, and $(iv)$ tighter latencies.  Because it distributes
computation across decentralized processing units, space multiplexing
supports architectures that scale spatially without any global wires.
Unfortunately, this approach is highly dependent on effective load
balancing techniques: the goal is to merge and split filters (into new
filters) until all computation resources are assigned filters with a
uniform work distribution.  Such load balancing can be very difficult
for applications with an irregular distribution of work.

This paper proposes a hybrid approach, {\it space-time multiplexing},
that exploits the advantages of both space multiplexing and time
multiplexing (see Figure~\ref{fig:spacevstime}c).  This technique uses
space multiplexing to schedule a group of filters for parallel
execution on part of the chip; however, time multiplexing is used to
switch between different groups as execution progresses.  Space-time
multiplexing affords the flexible load balancing of time multiplexing,
while preserving the locality and latency benefits of space
multiplexing.

In this paper, we describe and evaluate our SpaceTime Compiler, a
fully-automatic implementation of space-time multiplexing.  Our source
language is StreamIt~\cite{streamitcc}, a high-level stream
programming language that aims to be portable across next-generation
communication-exposed architectures.  Our target is the Raw
microprocessor~\cite{raw10,raw_isca}, a tiled architecture with
fine-grained, programmable communication between processors.

Our implementation of space-time multiplexing treats each group of
space-multiplexed filters, which we refer to as a {\it slice}, as an
atomic unit for scheduling. Our compiler first extracts slices from
the stream graph by aggregating contiguous filters into a slice. We
attempt to extract slices from the stream graph that are internally
load-balanced. Next, we schedule the slices in both space and time.
Due to the implicit outer loop of a stream graph (or
subgraph), slices can be scheduled using techniques traditionally
limited to individual instructions in a given loop nest.  Our approach
extends these loop-level techniques---in particular, software
pipelining---to a coarse level of granularity that encompasses
multiple program modules at a time.

Inter-slice communication leverages capacious off-chip DRAM resources.
This enables us to aggressively buffer data between slices, amortizing
the cost of slice pipeline startup and providing scheduling
flexibility. The space-time execution model uses rotating buffers to
buffer data between slices allowing the compiler remove dependences
between slices and allowing slices of different iterations of the
stream graph to execute concurrently.  These rotating buffers are akin
to a rotating register file found in some superscalar and VLIW
architectures and are distributed amongst the off-chip dram
banks. Furthermore, the inter-slice off-chip memory accesses are
highly regular (unit-stride bulk transfers) that can utilize batch
processing modes of modern DRAMs. \todo{Anything else here}?

Using the on-chip interconnect of a communication-exposed architecture
to communicate data between processing units is more efficient than
communicating through off-chip DRAM. Furthermore, as these
architectures mature (and the memory wall remains), the difference
will become more pronounced.  Thus, we use the on-chip interconnect of
the target architecture for intra-slice communication, for
space-multiplexing each slice.  Also, the on-chip interconnect is
harnessed to implement the distribution and merging of data streams
inherent in streaming languages. Abundant utilization of the on-chip
interconnect is essential to the scalability of a parallel execution
model for communication-exposed architectures.

In its tradition setting, software pipelining \cite{lam-softpipe,
rau-softpipe} is a scheduling technique which hides latency and
maximizes resource utilization by building a loop with instructions
from multiple iterations of the original loop.  In our context, the
software pipeliner is scheduling whole slices for execution on
multiple raw tiles. Our slice scheduling phase employs novel
techniques to arrive at the software pipelined schedule, these
techniques are enabled by the regular and structured slice graphs that
we are scheduling. Unlike traditional software pipelining algorithms,
we use simulated annealing to solve a 3-dimensional bin-packing
problem with irregular shaped objects.  These objects represent the
slices of the graph and the bins represent the ccmputational nodes of the
target architecture.  The solution to the 3D bin-packing gives us both
the slice scheduling order and the resource mapping for the filters of
each slice.

In summary, this paper makes the following contributions:

\begin{itemize}
%\item A procedure for eliminating synchronization from a hierarchical
%stream graph, yielding a flat graph that directly exposes
%communication patterns.
\item A description of the space-time multiplexing parallel execution
model.
\item A high-level intermediate representation for representing
space-time multiplexing in the context of a stream compiler.  
\item An algorithm for extracting load-balanced slices from a flat
stream graph.
\item A translation of the 3D bin-packing problem to our
software-pipelined slice scheduling problem where simulated annealing
is used to arrive at a solution.
\item An evaluation of space-time multiplexing in the StreamIt-to-Raw
compiler, demonstrating an average improvement of \todo{??} over a
space multiplexing approach.
\end{itemize}

The remainder of this paper is organized as follows.  Section
\ref{sec:streamit} gives an introduction to the StreamIt programming
language and Section \ref{sec:raw} provides an overview of
Raw. Section~\ref{sec:example} gives an illustrative example for our
technique.  Sections 5 through 10 describe our compilation framework,
including synchronization removal, trace extraction and scheduling,
and code generation.  Section \ref{sec:results} presents our results,
Section \ref{sec:related} reviews related work and Section
\ref{sec:conclusion} concludes.

%% In this paper we present a compiler for the StreamIt programming
%% language \cite{streamitcc}.  StreamIt is a high-level stream
%% programming language that aims to be portable across next-generation
%% communication-exposed architectures.  StreamIt contains basic
%% constructs that expose the parallelism and communication of streaming
%% applications without depending on the topology or granularity of the
%% target architecture \cite{streamit-asplos}. In StreamIt the basic unit
%% of computation is a {\it filter}, a single-input, single-output block.
%% Filters are composed into a communication network using hierarchical,
%% structured constructs (introduced below).  A {\it stream graph},
%% composed of filters and uni-directional FIFO channels connecting the
%% filters, describes the resulting computation. The StreamIt compiler
%% currently targets the Raw microprocessor, a tiled architecture with
%% fine-grained, programmable communication between processors.

%% Neglecting the irrelevant initial passes, the flow of the compiler is
%% as follows.  The compiler reaches the space-time backend with a
%% structured, hierarchical stream graph, where all filters of the
%% application are explicitly represented.  We first convert this stream
%% graph into to a flat, non-hierarchical graph with unnecessary
%% communication channels removed.  Next, we identify the linear
%% sub-components of the stream graph.  Our compiler then extracts the
%% traces from the stream graph, considering the concurrency,
%% communication, layout, and type (linear or non-linear) of each filter.
%% Next, we schedule the traces, producing both a multi-stage initialization
%% schedule and a steady-state schedule.  Lastly, we generate both
%% communication and computation code for Raw.  The contributions of this
%% monograph include:


%Also, since each trace is independent, a specific code generation
%strategy can be applied depending on the properties of the trace.  In
%this paper, we recognize traces that compute a {\it linear} function
%of their input.  Using the coefficients and I/O rates of the linear
%trace (extracted automatically from the code), we generate template
%assembly code based on a hand-optimized systolic algorithm.  By
%decoupling memory accesses from computation and carefully utilizing
%the register-mapped communication network on Raw, this code achieves
%near-optimal performance for linear functions.