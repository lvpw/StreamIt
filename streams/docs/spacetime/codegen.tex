\section{Code Generation}
\label{sec:codegen}
In this section we consider low-level code generation for the Raw
microprocessor. Each Raw tile is an amalgam of the filters that
execute on it. To generate the execution code for the entire
application, we traverse the schedule for each phase of computation:
first the peek initialization stage, then prologue stage, and finally
the steady-state.  As we traverse each schedule, we generate the
computation and communication code as we visit each slice.  

When we visit a slice, we first generate the DRAM requests and
communication code to load its inputs from its mapped DRAM. Then, we
generate the DRAM requests and communication code to store its outputs
to its mapped DRAM.  Finally, we generate computation and
communication code for each filter of the slice.

\subsection{Slice Data Reordering}
Before a steady-state execution can commence, each slice's single
input stream must be assembled in the order stipulated by the
round-robin weights on its incoming channels. To achieve this, the
various input streams of the slices are collected into the memory
module attached to the tile to which the first filter of the slice is
mapped. DRAM requests are generated to read each input
channel and to write the final, reassembled input.  Code is generated
for the static routers, as they perform the ordering of the final
input stream as stipulated by each joiner.  Splitting is done in much
the same way.  The compute processors of Raw are idle during
data-reorganization stage.

Remember from Section \ref{sec:softpipe} that the source and
destination buffers of this stage are the rotating buffers that
facilitate the software pipeline.  The number of individual memory
blocks that compose the rotating buffer is determined by the
multiplicity difference between the source slice and destination slice
in the prologue schedule.  Each time an off-chip memory request is
issued, the buffer will rotate, with two rotation structures for each
buffer that rotate separately, one for reading and one for writing.

\subsection{Filter Code Generation}
We generate computation and communication code for the individual
filters of the slice in much the same way as \cite{streamit-asplos}.  
%When we visit a slice in the schedule for each execution phase, we
%iterate over the filters that compose the slice, generating 
%communication and computation code necessary to execute the slice.
%The scheduling phase (Section \ref{sec:scheduling}) produces a mapping
%of filters to tiles.

%\subsubsection{Compute Processor}
%%For linear slices, we generate parameterized, template assembly code
%%for both computation and communication as described in Section
%%\ref{sec:linear}.  
%As we iterate over the filters of the slice, we append the code
%necessary to execute the current filter to the existing code for the
%tile to which the filter is mapped.  We also make sure to execute the
%{\tt init} function of each filter before we start the peek
%initialization stage. Consequently, each compute processor begins by
%calling the {\tt init} function of all the filters mapped to it.
%Next, the compute processor executes the {\tt work} function of each
%filter assigned to it in sequence.  The sequence is determined by the
%traversal of the schedule of each execution phase; first the
%peek initialization stage, then the prologue stage, and finally the
%steady-state.  Each work function execution is placed in a loop that
%executes the number of times the filter fires in the current stage.

%For Raw's compute processor, we generate a mixture of C code
%and assembly instructions that is compiled using Raw's GCC 3.3
%port. The translation of the {\tt init} function, {\tt work}
%functions, and any helper function is, for the large part,
%straightforward.  Most StreamIt expression have direct analogs in C
%except for the channel expressions {\tt push()}, {\tt peek(index)} and
%{\tt pop()}.

%One major benefit of StreamIt over other streaming languages is that
%the StreamIt compiler is responsible for buffer management.  The
%programmer is free to call {\tt pop()} or {\tt peek(index)} without
%having to worry about the structure of the buffer or updating the
%buffer's index variable after each execution of the work function.
%Currently, we classify filters as one of three types with respect to
%buffer management:
%\begin{enumerate}
%\item If a filter $(i)$ does not contain any {\tt peek} statements, $(ii)$
%does not contain any control-flow dependent channel expressions, and
%$(iii)$ has peek rate equals to pop rate, then a buffer is not needed
%and {\tt pop()} statements are translated directly into static network
%receives.  There exist other initialization considerations which are
%beyond the scope of this paper.
%\item If a filter contains peek statements, but $peek = pop$ then we can
%use a simple linear buffer that is reset after each work function execution.
%\item Otherwise, when $peek > pop$, we use a circular buffer to
%account for the data remaining on the channel after the work function
%invocation.
%\end{enumerate}
%Each {\tt push()} statement is translated directly into a static
%network send.  

%\subsubsection{Switch Processor}
%As mentioned in Section \ref{sec:implementation}, individual slices
%are limited to pipelines, thus we do not have to worry about
%complicated patterns for intra-slice communication.  For each tile we
%place the switch instructions to receive $pop$ items from the
%direction of its upstream neighbor and send $push$ items to the
%direction of its downstream neighbor.  The end-points of a slice send
%or receive their data to or from off-chip, routing their items to and
%from the side of the chip. These instructions are placed in a loop
%that executes as many times as the filter fires in the current stage.

