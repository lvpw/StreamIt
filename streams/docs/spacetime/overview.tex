\section{Stream Graph} 

\begin{figure*}
\centering
\psfig{figure=fm_example.eps,width=6.5in}
\caption{FMRadio with a 12-way equalizer after the passes of the
SpaceTime Compiler.
\protect\label{fig:fm-ex}}
\end{figure*}

Currently we use the StreamIt Programming Language as the input
language to the SpaceTime Compiler. After the StreamIt frontend
executes, it passes our compiler a complete stream graph representing
the computation and communication of the application.  In StreamIt,
the stream graph is a structured, hierarchical composition of
pipelines, splitjoins, and feedbackloops with filters, splitters, and
joiners as the leaf nodes of the graph (see Section
\ref{sec:streamit}).  

Throughout the following sections, we will use our FMRadio benchmark
to elucidate our discussion.  The FMRadio application is a software
implementation of FM radio with a 12-way equalizer.  StreamIt's stream
graph is given in Figure \ref{fig:fm-ex}a.  In the figure, we
represent splitters, joiners, and filters with circles.  Containers
(splitjoins and pipelines) are represented with as rectangles around
the leaf nodes they contain.  In the figure, the filters colored red
have the highest work estimation per steady-state execution (in this
case, the red nodes account for over 90\% of the computation in the
steady-state).

The initial pass of our compiler takes this structured stream graph
and de-structures it.  We remove all hierarchy and structure of the
containers and are left with only filters, splitters and joiners.
Furthermore, we introduce our own notion of a splitter and a joiner,
each slightly more powerful than their analog in StreamIt.  We still
have separate nodes for splitting an output stream and joining an
input stream, but these nodes can handle more complicated
data-reorganization patterns.  For example, our splitter node can
describe a round-robin distribution with duplication.  A more exact
discussion of our splitters and joiners is omitted, but it suffices to
say that we designed each node to closely match the data-organizations
that are possible with a scalar operand network \ref{scalaroperands}.
We next run a synchronization removal pass that converts StreamIt
splitters and joiners into our own notion of data-distribution nodes.
In the process we remove unnecessary synchronization points present in
the stream graph.  We introduce a (more powerful) splitter node after
each filter whose output is read by multiple filters and we introduce
a (more powerful) joiner node before each filter that reads data
produced by multiple filters.

After synchronization removal, we are left with a flat stream graph
composed of filters, and data-reorganization nodes (our more powerful
splitters and joiners), see Figure \ref{fig:fm_ex}b FMRadio's
flattened stream graph; note that some splitters have been
coalesced. For the remainder of this paper the terms splitter and
joiner refer to our more powerful splitting and joining nodes and the
term stream graph refers to the flattened graph composed of these
nodes and filters.  At this point we are ready to extract the traces
from the graph and schedule them for execution.

\todo{add the fm radio stream graph and its synch removed analog.}

%\begin{figure}
%\centering
%\psfig{figure=fm_example.eps,width=3in}
%\caption{FMRadion with a 12-way equalizer.
%\protect\label{fig:fm-example}}
%\end{figure}


\section{The Slice}
Before we move on to describing the algorithms that compose the
compiler, we will describe our implementation of a
slice. Conceptually, the slice is the atomic unit of scheduling in our
compiler.  Our scheduling algorithm operates at the slice level.
Each slice is composed of a contiguous region of the stream
graph with restrictions on its composition. 

A slice always begins with a joiner node to join the (possibly)
multiple inputs of the slice.  The slice continues with a pipeline of
filters from the stream graph.  The slice ends with a splitter node to
distribute the slice's output to its (possibly) multiple readers.
More specifically, only the end-points of slices can communicate with
more than one filter (these filters are contained in other slices);
inner filters are restricted to single input and single output.  Each
filter of the original stream graph is contained in exactly one slice.
Slices are not hierarchical.  The number of filters in a slice must be
less than or equal to the number of computational nodes of the
architecture to which we are targeting.  In Figure \ref{fig:fm_ex}c,
we give a possible slice graph for the 12-way FMRadio application.
The blue boxes represent individual slices.

These restrictions are not fundamental to the hybrid space-time
multiplexing execution model; they simplify the compiler.  In future
work, we plan to relax some of the restrictions.

In Section \ref{sec:extract} we describe our algorithm for extracting
slices from a stream graph.

\section{Execution Model}
\begin{figure}[t]
\centering
\psfig{figure=slices.eps,width=3.0in}
\caption{The characteristics of a slice.
\protect\label{fig:slice}}
\end{figure}

\section{The Raw Architecture}
\label{sec:raw}
After we compose a graph of slices from the stream graph, we are ready
to schedule the slices in both space and time on to the parallel
target.  In this section we will describe our current slice execution
model considering our backend for the Raw microprocessor.

Analogously, we can think of each slice as a single machine
instruction and we can think of each computational node of the
parallel target as a functional unit of a uniprocessor, except that
each instruction (slice) may occupy multiple functional units
(nodes). There exists data-flow dependencies between slices and slices
have an occupancy.  The stream graph, much like a data-flow graph
composed of machine instructions, encodes the data-flow dependencies
between slices. Thus, our compiler maps slices to computational nodes
much like a traditional scheduler maps instructions to functional
units.

After the space-time scheduler is completed, each computational node
will have a time-ordered list of filters to fire, with a multiplicity
for each filter, each filter from a different slice.  The first
filter mapped to the node will fire when its input has arrived, and
when it is finished, the next filter will execute.  In this way, a
complete space-time execution will unfold on the target.

\subsection{Software Pipelining}
\label{sec:softpipe}
In StreamIt's model of streaming computation, the stream graph is
infinite.  There exists an implicit infinite outer loop over the
stream graph.\footnote{Note that our technique is not just limited to
scheduling this outer loop because loops can be introduced throughout
a streaming application as long as we schedule and buffer properly.
However we leave it to future work to make this extension.}  The
SpaceTime compiler recognizes this outer loop and applies scheduling
techniques traditionally reserved for scheduling loops of machine
instructions.  In the streaming domain, a schedule that executes all
filters and can be repeated indefinitely is termed a {\it steady-state
schedule}.  In this work, we use a {\it single appearance}
steady-state schedule (SAS), where each filter occurs exactly once in
the schedule with an associated multiplicity.  As long as one buffers
adequately, a SAS can be executed in any order that respects the
number of items a filter needs to fire (its pop rate).

The goal of the SpaceTime compiler is to efficiently schedule the
steady-state of the stream graph.  We rely on the StreamIt frontend to
produce a SAS.  In the steady-state execution of our slice graph, each
slice executes exactly once and for each execution of a slice, each
filter that composes the slice fires the number of times stipulated by
its SAS multiplicity.  A slice cannot be interrupted once it begins
its execution (irrespective of hardware interrupts); it is atomic.

To achieve its goal, the compiler fully software pipelines the {\it
entire} stream graph at the slice level. By this we mean that during
execution of the steady-state, two slices that have a dependence chain
between them in the slice graph are from on different iterations of
the steady-state.  We do this to remove the dependence between.  The
can now execute out-of-order within the steady-state, and they do not
directly communicate.  Note that {\it filters} within a slice are all of the
same iteration executing in a normal pipeline fashion.

To achieve this software pipelined schedule, we must buffer adequately
between slices.  Inter-slice communication utilizes off-chip memory
modules of the target.  In our backend for Raw, for example, we target
the 16 DDR DRAM modules attached to the i/o ports of the chip.  It is
the task of the tile to send memory requests to these modules and
communicate data to and from the memory modules via the static network
or, for non-border tiles, the user-level dynamic network.  Each
conceptual buffer between two slices is represented as a rotating set
of buffers (much like a rotating register file), with a separate
rotation for the producer and the consumer, the producer
always being ahead of the consumer in the rotation.

Synergistically, software pipelining of slices on a parallel
architecture avoids many of the complications of and exposes many new
optimization opportunities versus traditional software pipelining.  In
traditional software pipelining, the limited size of the register file
is always an adversary (register pressure), but we instead use
voluminous off-chip DRAM, producing regular bulk transfers.  Another
recurring issue traditionally is the length of the prologue to the
software pipelined loop, but for our domain we have less interest in
this concern.  Lastly, and most importantly, we can use this synergy
to fully pipeline the graph, removing all dependencies and thus
removing all the constraints for our scheduling algorithm (discussed
below and in Section \ref{sec:scheduling}).

In our approach, we first assume that we have a machine with infinite
parallel resources.  We generate a prologue schedule that will
guarantee that each slice is ready to execute.  In software pipelining
terminology, the iteration interval is 1.  This is essential because,
unlike machine instructions, there exists complicated
data-organization between slices.  Therefore, we need to remove all
dependences between slices of the steady-state.  We rely on a joining
phase that executes before each steady-state to perform the
data-reorganization described by each joiner of each slice.  {\it
Before} we execute a steady-state schedule, all joining for the entire
slice graph is executed using the buffers in off-chip DRAMs as the
sources and destinations, and using the interconnect of the target to
perform the data-reorganization stipulated by each joiner.  {\it
After} each steady-state completes we perform the analog for
splitters.

When the steady-state is repeated, the splitting and joining phases
happen in sequence.  First, we perform all the splitting of the graph
from the steady-state execution that just finished; then, we perform
all the joining for the steady-state execution that is about to
commence. Next, computation of the slices begins.  Observations early
in the development of the SpaceTime compiler show that this separation
of computation from data-reorganization has a huge benefit over a
combined schedule that includes both.  The intuition is that we need
many tiles to perform most reorganizations, and during the
steady-state other tiles are off doing the computation of other
slices; it is very difficult to coordinate this interaction
effectively.

The buffers that are the sources and destinations of the
data-reorganization stage are the rotated.  Each conceptual arc in the
slice graph is concretely represented by allocating enough buffers as
the iteration difference between the source and destination slices in
the steady-state.

\subsection{Scheduling the Software Pipeline}
We have chosen to reduce our steady-state slice scheduling problem to
a 3D bin packing problem \ref{binpacking} with irregularly shaped
objects.  In the translation, each slice is represented by a
3-dimensional object.  The x,y area occupied by an object correspond
to the number of computational nodes occupied by the associated
slice. The z dimension is time.  The bins are the computational nodes
themselves.  We want to minimize the height (in the z dimension) of
the packing.  By doing this, we will minimize the critical path
schedule length.  We use simulated annealing to arrive at an optimized
solution to the packing.  The translation and the solution is
discussed in detail in section \ref{sec:scheduling}. Figure
\ref{fig:fm_ex}d presents a fictitious 3d conceptualization for
FMRadio. In the figure, each irregularly shaped object represents a
slice and we attempt to pack them into the cube while minimizing the
maximum height of packing.

\section{Compiler Flow}
Neglecting the irrelevant initial passes, the flow of the space-time
compiler is as follows.  The compiler reaches the space-time backend
with a structured, hierarchical stream graph, where all filters of the
application are explicitly represented.  We first convert this stream
graph into to a flat, non-hierarchical graph with unnecessary
communication channels removed.  Our compiler then extracts the
slices from the stream graph, considering the concurrency,
communication, and work load of the filters.
Next, we schedule the slices, producing both a multi-stage
initialization schedule and a steady-state schedule.  Lastly, we
generate both communication and computation code for Raw.  The next
sections will discuss these passes.