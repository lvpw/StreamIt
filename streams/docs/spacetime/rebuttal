First, we would like to thank the reviewers for their careful
reviews and thoughtful suggestions to improve our paper.

We used the space-multiplexing and time-multiplexing analogy to help
the reader understand our research.  However, our work is not simply a
new point on a continuum; it differs substantially from traditional
automatic parallelization frameworks for distributed memory machines.
 
Traditionally, a subset of the graph is mapped to each compute node
and the subset is scheduled statically (time-multiplexing).  The
subsets execute in parallel and each subset occupies its compute node
indefinitely (space-multiplexing).  Complicated dependencies can exist
between non-contiguous subsets.  It is very difficult to reconcile
load-balancing with locality and synchronization concerns.

The SpaceTime model maps a contiguous subgraph to multiple compute
nodes (space-multiplexing) and switches between these subgraphs at
runtime based on a static schedule (time-multiplexing).  Multiple
subgraphs can execute at the same time on different portions of the
chip.

SpaceTime scheduling can balance locality and load-balancing concerns
through the selection of the subgraphs (slices).  Within load-balanced
slices we exploit fast on-chip interconnects. Across contiguous slices
the dependencies remain simple. We employ scheduling techniques (in
this case software pipelining) previously reserved for scheduling
instructions in a loop.

Our research is applicable to any multicore architecture with
distributed memory but especially for architectures with a fast
on-chip communication network (CELL, Merrimac, Monarch, TRIPS, etc.).
For these architectures, forming slices leverages the on-chip network
necessary for scaling while benefiting from flexible load-balancing
and scheduling.  Previous approaches, such as shared memory models
(e.g. Cilk) and clustering, cannot make both claims. We use RAW to
prove the validity of our research, as a demonstration.

Reviewer 1 is concerned about the benchmarking baseline and asks how
it compares to a uniprocessor.  Previous work [Taylor et al., ISCA
04] has been shown that a space multiplexing compiler for StreamIt
targeting RAW is an order of magnitude faster than a comparable
superscalar. In this work we show that we are on average 38% better
than a space-multiplexing approach.

Some Related Concerns:
* Reviewer 1 wants more rigorous exploration of slice size (work threshold)
* Reviewer 3: Why did we not compare to time-multiplexing only
* Reviewer 3: Why not just toss the space-multiplexing aspect

In the final paper, we will rigorously investigate the effects of slice
size. We can approximate time-multiplexing by limiting slices to
one filter.  Note that even in this configuration there are many
novelties including software-pipelining of filters across a parallel
target. However, we will show that the space-multiplexing component of our
SpaceTime model is useful because it reduces off-chip memory traffic
by utilizing on-chip communication resources. This benefit will become
more pronounced as multicores scale up and the memory bottleneck
remains.

We agree with Reviewer 2, the organization and quality of the writing
needs to be improved, but Reviewer 2 is critical only of the
presentation.  

Dynamic input and output rates can be naturally and efficiently
supported by SpaceTime multiplexing.  This is one major benefits of
our SpaceTime model over space multiplexing.  It is straightforward to
limit dynamic I/O rates to the slice boundaries, allowing for load
balancing within a slice and off-chip buffering between dynamic rate
slices.

Reviewer 4 ask why we chose not to support for
feedbackloops. Feedbackloops occur very infrequently in practice.
There does not exist a fundamental limitation in our system that
prevents us from supporting them.  Feedbackloops are akin to
loop-carried dependencies which traditional software pipeliners
respect.

The final version of the paper will include a more detailed
explanation of the results and the 3 outlier cases in which
space-multiplexing has higher performance than SpaceTime. Note that
with fine-tuning of our parameters these 3 benchmarks cease to be
outliers.

In response to Reviewer 2, the utilization metric includes stalls due
to cache accesses and pipeline hazards for each tile.  Each RAW tile
is a single-issue, inorder processor.  Pipeline stalls are frequent
and thus depress the utilization metric.  

Utilization should not be compared because space multiplexing adds
compute instructions necessary to implement filter fusion (the data
buffering and redistribution of filters mapped to a single node). In
SpaceTime this is achieved using the communication networks.  A
throughput comparison makes more sense.

We did not enable code optimizations (unrolling, constant propagation,
and scalar replacement) for space-multiplexing because we did not have
these optimizations implemented in the SpaceTime compiler.  Thus,
the comparison would be unfair.  It is unclear which approach will
benefit more from these optimizations. 

