We would like to thank the reviewers for their careful reviews and
thoughtful suggestions to improve our paper.  The final paper will
improve the organization and the quality of the presentation as
suggested by the reviewers.

CPU vendors have already begun to embrace multicore architectures.
Our research is applicable to any multicore architecture with
distributed memory but especially for architectures with a fast
on-chip communication network. Examples include CELL, RAW, Monarch,
and TRIPS. RAW provides a vehicle for experimentation and validation
of our ideas.

We are quite different from traditional automatic parallelization
frameworks for distributed machines.  Traditional approaches separate
execution into the space-multiplexing of time-multiplexed components.
Our approach more freely combines time-multiplexing and
space-multiplexing.  We are not just another point on a
continuum. This has many advantages over the traditional approach:

* We form spatial pipelines (slices), leveraging fast on-chip
bandwidth and minimizing global communication.

* Our method maintains simple dependencies between components.
 
* We have more scheduling freedom, enabling us to introduce novel and
aggressive scheduling techniques for load-balancing.

* We reduce synchronization overhead by limiting on-chip communication
to simple patterns.

* Dynamic I/O rates can be more naturally and efficiently supported at
slice boundaries.

Previous approaches, such as shared memory models (e.g. Cilk) and
clustering, cannot make these claims.

The space-multiplexing component of our SpaceTime model is useful
because it reduces off-chip memory traffic by utilizing on-chip
communication resources. This benefit will become more pronounced as
multicores scale up and the memory bottleneck remains.

In response to Reviewer 4, while we are inspired by
instruction-level software pipelining, the novelty of our paper is in
recognizing that the technique can be applied at a much coarser level.
We provide a novel framework for such an application that leads to
increased throughput while exploiting spatial characteristics of the
hardware.

Reviewer 1 is concerned about the benchmarking baseline and asks how
it compares to a uniprocessor.  Previous work [Taylor et al., ISCA
04] has been shown that a space multiplexing compiler for StreamIt
targeting RAW is an order of magnitude faster than a comparable
superscalar. In this work we show that we are on average 38% better
than a space-multiplexing approach.

In the final paper, we will rigorously investigate the effects of slice
size and compare to a time-multiplexed strategy by limiting slices to
one filter.  This will address the related benchmarking concerns of
Reviewers 1 and 3.

To answer Reviewer 3, although time-multiplexing alone will benefit
from our novel scheduling techniques, it will increase off-chip memory
bandwidth requirements as it will not exploit the on-chip
communication networks.

Reviewer's 2 and 4 ask why we chose not to support for
feedbackloops. Feedbackloops occur very infrequently in practice.
There does not exist a fundamental limitation in our system that
prevents us from supporting them.  Feedbackloops are akin to
loop-carried dependencies which traditional software pipeliners
respect.

The final version of the paper will include a more detailed
explanation of the results and the 3 outlier cases in which
space-multiplexing has higher performance than SpaceTime. 

In response to Reviewer 2, the utilization metric includes stalls due
to cache accesses and pipeline hazards for each tile.  Each RAW tile
is a single-issue, inorder processor.  Pipeline stalls are frequent
and thus depress the utilization metric.  

Utilization should not be compared because space multiplexing adds
compute instructions necessary to implement filter fusion (the data
buffering and redistribution of filters mapped to a single node). In
SpaceTime this is achieved using the communication networks.  A
throughput comparison makes more sense.

We did not enable code optimizations (unrolling, constant propagation,
and scalar replacement) for space-multiplexing because we did not have
these optimizations implemented in the SpaceTime compiler.  Thus,
the comparison would be unfair.  It is unclear which approach will
benefit more from these optimizations.  This will be fixed in the
final version of the paper.

