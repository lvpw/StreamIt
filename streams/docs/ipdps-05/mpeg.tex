\Section{MPEG-2 Video Coding and Decoding}

MPEG-2~\cite{MPEG2} is a popular coding and decoding standard
for digital video data. The scheme is a subset of both the
DVD-Video~\cite{Taylor:1999:SDV} standard for storing movies, and the Digital
Video Broadcasting specifications for transmitting HDTV and
SDTV~\cite{DVB}. The scheme is used by a wide variety of multimedia
applications and appliances such as the Tivo Digital Video
Recorder~\cite{tivo}, and the DirecTV satellite broadcast
service~\cite{directv}.

MPEG-2 encoding uses both {\it lossy} compression and {\it lossless}
compression. Lossy compression permanently eliminates information from
a video based on a human perception model. Humans are much better at
discerning changes in color intensity (luminance information) than
changes in color (chrominance information). Humans are also much more
sensitive to low frequency image components, such as a blue sky, than
to high frequency image components, such as a plaid shirt. Details
which humans are likely to miss can be thrown away without affecting
the perceived video quality.

Lossless compression eliminates redundant information while allowing
for its later reconstruction. Similarities between adjacent video
pictures are encoded using motion prediction, and all data is Huffman
compressed\cite{Huffman52}. The amount of lossy and lossless
compression depends on the video data. Common compression ratios range
from 10:1 to 100:1. For example, HDTV, with a resolution of 1280x720
pixels and a streaming rate of 59.94 frames per second, has an
uncompressed data rate of 1.33 Gigabits per second. It is compressed at
an average rate of 66:1, reducing the required streaming rate to
20 Megabits per second~\cite{imagevidstandards}. %% page 3: [p.3]

\SubSection{MPEG Coding}

%% \begin{figure}[t]
%% \begin{center}
%% \vspace{-12pt}
%% % \framebox{
%% % \includegraphics[scale=1, angle=0]{./mpeg-encoder.eps}
%% %}
%% % \vspace{-6pt}
%% % \nocaptionrule
%%  \caption{MPEG encoder.}
%%  \label{fig:mpeg-encoder}
%% %\vspace{-18pt}
%% \end{center}
%% \end{figure}

%% An overview of the encoding process is illustrated in
%% Figure~\ref{fig:mpeg-encoder}.
The encoder operates on a sequence of pictures. Each picture is made
up of pixels arranged in a 16x16 array known as a macroblock.
Macroblocks consist of a 2x2 array of blocks (each of which contains
an 8x8 array of pixels).  There is a separate series of macroblocks
for each color channel, and the macroblocks for a given channel are
sometimes downsampled to a 2x1 or 1x1 block matrix.  The compression
in MPEG is achieved largely via motion estimation, which detects and
eliminates similarities between macroblocks across
pictures. Specifically, the motion estimator calculates a motion
vector to represent the horizontal and vertical displacement of a
given macroblock (i.e., the one being encoded) to a matching
macroblock-sized area in a reference picture.  The matching macroblock
is removed (subtracted) from the current picture on a pixel by pixel
basis, and an motion vector is associated with the macroblock
describing its displacement relative to the reference picture. The
result is a residual predictive-code (P) picture. It represents the
difference between the current picture and the reference
picture. Reference pictures encoded without the use of motion
prediction are intra-coded (I) pictures. In addition to forward motion
prediction, it is possible to encode new pictures using motion
estimation from both previous and subsequent pictures. Such pictures
are bidirectionally predictive-coded (B) pictures, and they exploit a
greater amount of temporal locality.

Each of the I, P, and B pictures then undergoes a 2-dimensional
discrete cosine transform (DCT) which separates the picture into parts
with varying visual importance. The input to the DCT is one block.
The output of the
DCT is an 8x8 matrix of frequency coefficients. The upper left corner
of the matrix represents low frequencies, whereas the lower right
corner represents higher frequencies. The latter are often small and
can be neglected without sacrificing human visual perception.

%\begin{figure}[t]
%\begin{center}
%\vspace{-12pt}
%\framebox{
% \includegraphics[scale=.5, angle=0]{./zigzag.eps}
%}
% \vspace{-6pt}
 %\nocaptionrule
% \caption{Zig-Zag scan patterns. (a) shows the default scan pattern, while (b) shows the alternate scan pattern.}
% \label{fig:zigzag}
%\vspace{-18pt}
%\end{center}
%\end{figure}

The DCT coeffecients are quantized to reduce the number of bits needed
to represent them. Following quantization, many coefficients are
effectively reduced to zero. The DCT matrix is then run-length encoded
by emitting each non-zero coefficient,
% TODO: clarify ``followed by the number of zeros that precede it''?
followed by the number of zeros that precede it, along with the number
of bits needed to represent the coefficient, and its value. The
run-length encoder scans the DCT matrix in a zig-zag order
(Figure~\ref{fig:zigzag-order}) to consolidate the zeros in the matrix.

Finally, the output of the run-length encoder, motion vector data,
and other information (e.g., type of picture), are Huffman coded to
further reduce the average number of bits per data item. The compressed
stream is sent to the output device.

\SubSection{MPEG Decoding}

An MPEG-2 input stream is organized as a Group of Pictures (GOP) which
contains all the information needed to reconstruct a video. The GOP
contains the three kinds of pictures produced by the encoder, namely
I, P, and B pictures. I pictures are intended to assist scene cuts,
random access, fast forward, or fast reverse
playback~\cite[p. 14]{MPEG2}. A typical I:P:B picture ratio in a GOP
is 1:3:8, and a typical picture pattern is a repetition of the
following logical sequence:
I$_1$~B$_2$~B$_3$~P$_4$~B$_5$~B$_6$~P$_7$~B$_8$~B$_9$~P$_{10}$~B$_{11}$~B$_{12}$
where the subscripts denote positions in the original video.  However,
to simplify the decoder, the encoder reorders the pictures to produce
the following pattern:
I$_1$~P$_4$~B$_2$~B$_3$~P$_7$~B$_5$~B$_6$~P$_{10}$~B$_8$~B$_9$~B$_{11}$~B$_{12}$.
Under this configuration, if the decoder encounters a P picture, its
motion prediction is with respect to the previously decoded I or P
picture; if the decoder encounters a B picture, its motion prediction
is with respect to the previously two decoded I or P pictures.

As with the encoding process, pictures are divided up into 16x16 pixel
macroblocks, themselves composed of 8x8 blocks. Macroblocks
specify colors using a {\it luminance} channel to represent saturation
(color intensity), and two {\it chrominance} channels to represent
hue. MPEG-2 streams specify a chroma format which allows the
chrominance data to be sampled at a lower rate. The most common chroma
format is 4:2:0 which represents a macroblock using four blocks for the
luminance channel and one block for each of the two chrominance
channels.

%% \begin{figure}[t]
%% \begin{center}
%% \vspace{-12pt}
%% % \framebox{
%% % \includegraphics[scale=1, angle=0]{./mpeg-decoder.eps}
%% %}
%% % \vspace{-6pt}
%% % \nocaptionrule
%%  \caption{MPEG decoder.}
%%  \label{fig:mpeg-decoder}
%% %\vspace{-18pt}
%% \end{center}
%% \end{figure}

%% is illustrated in Figure~\ref{fig:mpeg-decoder}. It
The decoding process is conceptually the reverse of the encoding
process. The input stream is Huffman and run-length decoded, resulting
in quantized DCT matrices. The DCT coefficients are scaled in
magnitude and an inverse DCT (IDCT) maps the frequency matrices to the
spatial domain.

Finally, the motion vectors parsed from the data stream are passed to
a motion compensator, which reconstructs the orginal pictures. In the
case of I pictures, the compensator need not make any changes since
these pictures were not subject to motion estimation\footnote{I
pictures are allowed to contain concealment motion vectors which aid in
macroblock reconstruction should a bitstream error destroy the
frequency coefficient data. We ignore this special case.}. In the case of P
and B pictures however, motion vectors are used to find the
corresponding region in the current reference pictures. The
compensator then adds the relevant reference macroblocks to the
current picture to reconstruct it. These pictures are then emitted to
an output device.