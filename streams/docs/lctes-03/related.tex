\section{Related Work}
\label{chpt:related}

There has been a wealth of research on scheduling dataflow graphs.
This section introduces some of the other projects.

Ptolemy \cite{ptolemyoverview} is a simulation environment for
heterogenous embedded systems, including Synchronous Data Flow, the
method used for {\StreamIt}. {\SDF} programs, however, do not include
the peeking constructs of {\StreamIt}.  In {\SDF} languages, actors
are the active computational elements ({\filters}).  The {\SDF}
computation model does not impose structure on the program.  All
actors are allowed to have multiple input and output channels.
\cite{benveniste93dataflow} provides an overview of dataflow
synchronous languages.

There are many results on the scheduling of {\SDF} programs
\cite{leesdf,bhattacharyya99synthesis}.  While the tradeoff between
data size and code size is well recognized, most projects focus on
minimizing memory requirements while maintaining minimal code size in
the form of a single appearance schedule.  A single appearance
schedule is attractive because filters can be inlined into the
schedule without effecting the size of the code.  In this paper, we
assume that the schedule and the filter code are stored separately,
and that non-single appearance schedules are supported with function
calls. \cite{bhat1999x1} considres a hybrid model between these two
extremes, in which actor invocations are inlined unless the resulting
code grows too large.

There are a number of approaches to minimizing the buffer requirements
for single-appearance schedules (see \cite{bhattacharyya99synthesis}
for a review).  APGAN (Pairwise Grouping of Adjacent Nodes) and RPMC
(Recursive Partitioning by Minimum Cuts) are two complementary
heuristics that have shown to be effective when applied together,
taking the best result~\cite{Bhatta97}.  Another technique for
reducing buffering requirements is buffer merging
\cite{murthy99buffer,murt1999x3}, which will be explored for use in
{\StreamIt} in the future.  Yet another approach is the GASAS system,
which uses genetic algorithms to minimize buffer size~\cite{GASAS}.

Buffer minimization has also been done in the context of a
multiprocessor implementation \cite{govindarajan-minimizing}. Using a
linear programming framework, they minimize the buffer size across
schedules that have optimal throughput.

There are some streaming computation models which are less constrained
than {\SDF}. The most relevant is Cyclo-Static Data Flow (CSDF)
\cite{BELP96,parks95comparison}.  CSDF actors have multiple {\work}
functions, with each one being allowed to produce/consume a different
number of data items.  Phased scheduling could be viewed as a
generalization of CSDF to the case where hierarchical stream
containers -- not just leaf nodes -- have cyclic phases.  In addition,
incorporating child phases into parent schedules allows the phase
information in a CSDF graph to be fully utilized, {\it e.g.,} for
decreasing latency and for scheduling tightly constrained feedback
loops.

\cite{wauters96cyclodynamic} proposes a model where the flow of data
is not static, but may depend on data being processed. The model is
called Cyclo-Dynamic Data Flow (CDDF). This greatly helps flexibility
of programming, but prevents fully static scheduling of programs. The
U.S. Navy Processing Graph Method (PGM) uses a version of {\SDF} with
an equivalent of peeking \cite{goddard00navy}.  The paper is focused
on real-time execution and provides analysis and verification of
latency of data flow through their system.

A large number of programming languages have included a concept of a
stream; see \cite{survey97} for a survey.  Synchronous languages such
as LUSTRE~\cite{lustre}, Esterel~\cite{esterel92}, and
Signal~\cite{signal} also target the embedded domain, but they are
more control-oriented than StreamIt and are less amenable to static
scheduling.  Sisal (Stream and Iteration in a Single Assignment
Language) is a high-performance, implicitly parallel functional
language~\cite{sisal}.  The Distributed Optimizing Sisal
Compiler~\cite{sisal} considers compiling Sisal to distributed memory
machines, although it is implemented as a coarse-grained master/slave
runtime system instead of a fine-grained static schedule.

% http://www.cis.ohio-state.edu/~gb/cis888.12g/Papers/bhattacharyya99software.pdf, page 14, reviews related work for buffer stuff