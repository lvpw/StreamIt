\section{Results}
\label{chpt:results}

This section presents results of creating schedules using
techniques described in Sections \ref{chpt:hierarchical} and
\ref{chpt:phased}.

Section \ref{sec:results:apps} presents the applications used for
testing.  Section \ref{sec:results:results} presents the results and
analysis.

\subsection{Applications}
\label{sec:results:apps}

Our benchmark suite contains 17 applications. Out of those
applications, 15 represent useful practical computation taken from
real-life applications, while two were chosen to highlight the
effectiveness of phased scheduling.

SJPeek1024 and SJPeek31 are synthetic benchmarks, designed to
highlight strengths of phased schedules. SJPeek1024 requires an
initialization schedule which benefits from finer granularity of
minimum latency schedule. SJPeek31 contains a push/pop mismatch
which causes a combinatorial blow-up using SAS.

Nine test applications (BitonicSort, FFT, FilterBank, FIR, Radio,
GSM, 3GPP, Radar and Vocoder) are applications used in
\cite{Gordo02}. BitonicSort performs a 32 element bitonic sort.
FFT performs a 64-element FFT, FilterBank is an 8 channel filter
bank.  FIR is a 64-tap FIR application. Radio is an FM radio
decoder with an equalizer.  3GPP is a 3GPP Radio Access Protocol
application. Radar is a radar array front-end application. Vocoder
is a 28 channel Vocoder.

Two test applications (QMF and CD-DAT) are applications used in
another publication on scheduling streaming applications
(\cite{murthy99buffer}). QMF is a filter bank application. CD-DAT
is a sample rate conversion application. The code inside of the
{\filters} has not been implemented. QMF application is a
qmf12\_3d.  It was slightly modified to account for {\StreamIt}
{\splitters} and {\joiners} not allowing any computation. The
high-pass and low-pass filtering in the {\splitters} has been
moved to just after data has been separated into two channels. The
re-combining of data in the {\joiners} has been moved to a
{\filter} just after the {\joiners}. The low and high pass filters
have also been given a peek amount of 16 so they can perform their
function in the way intended in {\StreamIt}.
 CD-DAT is exactly the same application as
that described in \cite{murthy99buffer}.

The remaining 4 applications were chosen from our sample
applications used for testing the StreamIt compiler. HDTV performs
a HDTV signal decoding/encoding. CFAR implements PCA Constant
False Alarm Rate detection. Block Matrix Mult performs a blocked
matrix multiplication application - it multiplies a 12x12 matrix
by a 9x12 matrix in blocks of 3x3 submatrices. Trellis performs
trellis encoding/decoding.

\begin{comment}

\subsection{Methodology}
\label{sec:results:methodology}

The following data has been collected: number of nodes, number of
node executions per steady state, schedule size and buffer size
for pseudo single appearance and minimal latency schedules.

\subsubsection{Schedule Compression}

\end{comment}

\begin{comment}
\subsubsection{Sinks}

Any application in {\StreamIt} must receive its data from outside,
and its data must be sent to outside. {\filters} that receive and
send data to outside are called sinks and sources. In particular,
sinks have the property of having $u_f = 0$ while sources have
$e_f = o_f = 0$. Sinks are problematic for minimal latency
scheduling, because they do not produce any data. Thus any
schedule of a sink operator is a minimal latency schedule. This
leads to the minimal latency schedule of the outer-most
{\pipeline} becoming a single appearance schedule, thus destroying
some of the benefit of using phased scheduling. The technique
used for scheduling {\pipeline} sinks has been discussed in
\cite{karczma-thesis}.

This problem has been alleviated by detecting sinks at the end of
a {\pipeline} and scheduling them in a unique way. Namely, a simple
attempt is made to minimize the amount of storage necessary to
store the phases of the {\pipeline}.

Let the amount of storage necessary to store one data item in
{\Input} {\Channel} to the sink be $x$, the amount of storage necessary
to store a phase be $y$, the sink consume $a$ data per steady
state execution of its parent {\pipeline} and $b$ be the number of
phases of the parent pipeline, then we have that amount of storage
necessary to store the phases and the buffer is
$$ {ax \over b} + by $$ We want to minimize this amount, with $b$
being the variable. We take a derrivative of the above expression,
set it to zero and solve:

\begin{displaymath}
\begin{array}{rcl}
-{ax \over b^2} + y & = & 0 \\
yb^2 & = & ax \\
b & = & \sqrt{ax \over y}
\end{array}
\end{displaymath}

For simplicity, we set $x = y = 1$, thus obtaining that $b =
\sqrt{a}$.

Now, for every phase of the parent {\pipeline} of the sink, the sink
is executed $\sqrt{a}$ times on the first step of scheduling a
phase of the {\pipeline}.
\end{comment}

\subsection{Results}
\label{sec:results:results}

\begin{figure}[t]
\psfig{figure=buffer-graph.eps,width=3.35in}
\caption{Buffer sizes.}
\end{figure}

\begin{figure}[t]
\psfig{figure=code-graph.eps,width=3.35in}
\caption{Code sizes.}
\end{figure}

\begin{figure}[t]
\psfig{figure=total-size-graph.eps,width=3.35in}
\caption{Sum of code size and buffer size.}
\end{figure}

\begin{table*} \centering \small
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline benchmark & \parbox{0.5in}{\centering number of nodes} & \parbox{0.5in}{\centering number of node executions} & \multicolumn{2}{c|}{pseudo single appearance} & \multicolumn{2}{c|}{minimal latency} \\
\cline{4-7} & & & \parbox{0.5in}{\centering schedule size} & \parbox{0.5in}{\centering buffer size} & \parbox{0.5in}{\centering schedule size} & \parbox{0.5in}{\centering buffer size} \\
\hline SJPeek31 & 6 & 12063 & 8 & 19964 & 24 & 874 \\
\hline HDTV & 170 & 390038 & 230 & 550692 & 1190 & 28300 \\
\hline CD-DAT & 6 & 612 & 6 & 1021 & 64 & 72 \\
\hline CFAR & 4 & 193 & 7 & 193 & 9 & 129 \\
\hline SJPeek1024 & 6 & 3081 & 8 & 7168 & 13 & 4864 \\
\hline Block Matrix Mult & 43 & 1956 & 48 & 4212 & 56 & 3132 \\
\hline Vocoder & 117 & 415 & 156 & 1285 & 205 & 1094 \\
\hline Radar & 68 & 161 & 68 & 332 & 68 & 332 \\
\hline BitonicSort & 370 & 468 & 370 & 2112 & 370 & 2112 \\
\hline 3GPP & 94 & 356 & 104 & 986 & 108 & 970 \\
\hline Trellis & 14 & 301 & 14 & 538 & 17 & 499 \\
\hline FIRfine & 132 & 152 & 132 & 1560 & 132 & 1560 \\
\hline FilterBank & 53 & 312 & 95 & 2063 & 116 & 1991 \\
\hline QMF & 65 & 184 & 85 & 1225 & 85 & 1225 \\
\hline Radio & 30 & 43 & 35 & 1351 & 35 & 1351 \\
\hline FFT & 26 & 448 & 26 & 3584 & 26 & 3584 \\
\hline GSM & 47 & 3356 & - & - & 64 & 3900 \\
\hline
\end{tabular}
\caption{Results of running pseudo single appearance and minimal
latency scheduling algorithms on various applications.}
\label{tbl:results}
\end{table*}

\begin{comment}
\begin{figure}
\centering \psfig{figure=kz-1.eps,width=6in} \caption[Buffer
storage space savings of Phased Minimal Latency schedule vs.
Hierarchical schedule.]{Buffer storage space savings of Phased
Minimal Latency schedule vs. Hierarchical schedule. All data in
all {\Channels} is assume to consume same amount of space.}
\end{figure}

\begin{figure}
\centering \psfig{figure=kz-2.eps,width=6in} \caption[Storage
usage comparison]{Storage usage for compressed Minimal Latency
Phased schedule vs. Hierarchical schedule. Left bars are for
Hierarchical schedules. Numbers are normalized to total storage
required by Hierarchical schedule. Each entry in every schedule
and data items in all {\Channels} are assumed to consume same
amount of space.}
\end{figure}
\end{comment}

Table \ref{tbl:results} presents buffer and schedule sizes
necessary to execute various applications using the algorithms
developed in this thesis.

The GSM application cannot be scheduled using pseudo
single-appearance algorithm, because it has a loop which is too
tight for execution under the SAS.

Several applications show a very large improvement in buffer size
necessary for execution.  Namely, CD-DAT decreases from 1021 to
72, a 93\% improvement. \cite{murthy99buffer} reports a buffer
size of 226 after applying buffer merging techniques. Our
improvement is due to reducing the combinatorial growth of the
buffers using phased scheduling.

Our synthetic benchmarks decrease from 7168 to 4864 and from 19964
to 12063, a 32\% and 40\% improvements. The first improvement is
due to creating fine grained phases which allow the initialization
schedule to transfer smaller amount of data and allow the children
of a {\splitjoin} to drain their data before the {\splitter}
provides them with more. This improvement is only created in
presence of peeking. The second improvement is due to reducing
combinatorial growth and due to finer grained schedules to deal
with peeking.

Other applications show no or little improvement in buffer
requirements. As expected, no application requires more buffer
space.
