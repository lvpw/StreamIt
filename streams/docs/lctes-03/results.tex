\section{Results}
\label{chpt:results}

\begin{figure}[t]
\psfig{figure=gsmfl.eps,width=3.5in}
\end{figure}

\begin{figure*}[t]
\psfig{figure=gsmfl-sched.eps,width=7in}
\end{figure*}

This section presents results of creating schedules using
techniques described in Sections \ref{chpt:hierarchical} and
\ref{chpt:phased}.

Section \ref{sec:results:apps} presents the applications used for
testing.  Section \ref{sec:results:methodology} presents the
methodology used for testing. Section \ref{sec:results:results}
presents the results and analysis.

\subsection{Applications}
\label{sec:results:apps}

Our benchmark suite contains 13 applications. Out of those
applications, 11 represent useful practical computation taken from
real-life applications, while two were chosen to highlight the
effectiveness of phased scheduling.

Nine test applications (bitonic sort, FFT, filter bank, FIR,
radio, GSM, 3GPP, radar and vocoder) used are code-complete and
perform the computations intended. Some results of compiling these
applications can be found in \cite{Gordo02}.

Two test applications (QMF and CD-DAT) are applications used in
another publication on scheduling streaming applications
(\cite{murthy99buffer}). The code inside of the {\filters} has not
been implemented.

The QMF application is a qmf12\_3d.  It had to be modified
slightly to account for {\StreamIt} {\splitters} and {\joiners}
not allowing any computation. The high-pass and low-pass filtering
in the {\splitters} has been moved to just after data has been
separated into two channels. The re-combining of data in the
{\joiners} has been moved to a {\filter} just after the
{\joiners}. The low and high pass filters have also been given a
peek amount of 16 so they can perform their function in the way
intended in {\StreamIt}.

CD-DAT is exactly the same application as that described in
\cite{murthy99buffer}.

The last two applications (SJ\_PEEK\_1024 and SJ\_PEEK\_31) are
synthetic benchmarks. They were introduced in
\cite{karczma-thesis}.

\subsection{Methodology}
\label{sec:results:methodology}

The following data has been collected: number of nodes, number of
node executions per steady state, schedule size and buffer size
for pseudo single appearance and minimal latency schedules.

\subsubsection{Schedule Compression}

The size of schedules for minimal latency technique contains two
numbers. The first one is an uncompressed schedule, while the
second is a compressed schedule. During testing it was found that
in some applications some operators had many phases that were
identical to other phases of the operator. Instead of including
these phases in the final schedule multiple times, they were
listed only once, and references to the duplicate phases have been
replaced with references to the one copy.

This optimization lead to improvements in schedule size for two
reasons. First, operators now had less phases, so their schedules
took up less space. Second, applications using the phased
schedules could now execute the same phase multiple times in a
row, which was optimized out using run length encoding.

This compression has no negative effects on speed of execution,
and never increases the size of a schedule. This compression has
no effect on the pseudo single-appearance schedules, thus is not
included in the results as a separate value.

\subsubsection{Sinks}

Any application in {\StreamIt} must receive its data from outside,
and its data must be sent to outside. {\filters} that receive and
send data to outside are called sinks and sources. In particular,
sinks have the property of having $u_f = 0$ while sources have
$e_f = o_f = 0$. Sinks are problematic for minimal latency
scheduling, because they do not produce any data. Thus any
schedule of a sink operator is a minimal latency schedule. This
leads to the minimal latency schedule of the outer-most
{\pipeline} becoming a single appearance schedule, thus destroying
some of the benefit of using phased scheduling. The technique
used for scheduling {\pipeline} sinks has been discussed in
\cite{karczma-thesis}.

\begin{comment}
This problem has been alleviated by detecting sinks at the end of
a {\pipeline} and scheduling them in a unique way. Namely, a simple
attempt is made to minimize the amount of storage necessary to
store the phases of the {\pipeline}.

Let the amount of storage necessary to store one data item in
{\Input} {\Channel} to the sink be $x$, the amount of storage necessary
to store a phase be $y$, the sink consume $a$ data per steady
state execution of its parent {\pipeline} and $b$ be the number of
phases of the parent pipeline, then we have that amount of storage
necessary to store the phases and the buffer is
$$ {ax \over b} + by $$ We want to minimize this amount, with $b$
being the variable. We take a derrivative of the above expression,
set it to zero and solve:

\begin{displaymath}
\begin{array}{rcl}
-{ax \over b^2} + y & = & 0 \\
yb^2 & = & ax \\
b & = & \sqrt{ax \over y}
\end{array}
\end{displaymath}

For simplicity, we set $x = y = 1$, thus obtaining that $b =
\sqrt{a}$.

Now, for every phase of the parent {\pipeline} of the sink, the sink
is executed $\sqrt{a}$ times on the first step of scheduling a
phase of the {\pipeline}.
\end{comment}

\subsection{Results}
\label{sec:results:results}

\begin{table*} \centering \small
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline benchmark & \parbox{0.5in}{\centering number of nodes} & \parbox{0.5in}{\centering number of node executions} & \multicolumn{2}{c|}{pseudo single appearance} & \multicolumn{3}{c|}{minimal latency} \\
\cline{4-8} & & & \parbox{0.5in}{\centering schedule size} & \parbox{0.5in}{\centering buffer size} & \parbox{0.5in}{\centering schedule size} & \parbox{0.8in}{\centering compressed schedule size} & \parbox{0.5in}{\centering buffer size} \\
\hline bitonic sort & 370 & 468 & 439 & 2112 & 448 & 448 & 2112 \\
\hline CD-DAT & 6 & 612 & 7 & 1021 & 170 & 72 & 72 \\
\hline FFT & 26 & 488 & 31 & 3584 & 31 & 31 & 3584 \\
\hline filter bank & 53 & 312 & 166 & 2063 & 160 & 145 & 1991 \\
\hline FIR & 132 & 152 & 133 & 1560 & 133 & 133& 1560 \\
\hline radio & 30 & 43 & 58 & 1351 & 50 & 50 & 1351 \\
\hline GSM & 47& 3356 & - & - & 724 & 78 & 3900 \\
\hline 3GPP & 94 & 356 & 147 & 986 & 149 & 137 & 970 \\
\hline QMF & 65 & 184 & 143 & 1225 & 132 & 122 & 1225 \\
\hline radar & 68 & 161 & 100 & 332 & 100 & 100 & 332 \\
\hline SJ\_PEEK\_1024 & 6 & 3081 & 11 & 7168 & 40 & 16 & 4864 \\
\hline SJ\_PEEK\_31 & 6 & 12063 & 11 & 19964 & 250 & 24 & 12063 \\
\hline vocoder & 117 & 415 & 172 & 1285 & 293 & 206 & 1094 \\
\hline
\end{tabular}
\caption{Results of running pseudo single appearance and minimal
latency scheduling algorithms on various applications.}
\label{tbl:results}
\end{table*}

\begin{comment}
\begin{figure}
\centering \psfig{figure=kz-1.eps,width=6in} \caption[Buffer
storage space savings of Phased Minimal Latency schedule vs.
Hierarchical schedule.]{Buffer storage space savings of Phased
Minimal Latency schedule vs. Hierarchical schedule. All data in
all {\Channels} is assume to consume same amount of space.}
\end{figure}

\begin{figure}
\centering \psfig{figure=kz-2.eps,width=6in} \caption[Storage
usage comparison]{Storage usage for compressed Minimal Latency
Phased schedule vs. Hierarchical schedule. Left bars are for
Hierarchical schedules. Numbers are normalized to total storage
required by Hierarchical schedule. Each entry in every schedule
and data items in all {\Channels} are assumed to consume same
amount of space.}
\end{figure}
\end{comment}

Table \ref{tbl:results} presents buffer and schedule sizes
necessary to execute various applications using the algorithms
developed in this thesis.

The GSM application cannot be scheduled using pseudo
single-appearance algorithm, because it has a loop which is too
tight for execution under the SAS.

Several applications show a very large improvement in buffer size
necessary for execution.  Namely, CD-DAT decreases from 1021 to
72, a 93\% improvement. \cite{murthy99buffer} reports a buffer
size of 226 after applying buffer merging techniques. Our
improvement is due to reducing the combinatorial growth of the
buffers using phased scheduling.

Our synthetic benchmarks decrease from 7168 to 4864 and from 19964
to 12063, a 32\% and 40\% improvements. The first improvement is
due to creating fine grained phases which allow the initialization
schedule to transfer smaller amount of data and allow the children
of a {\splitjoin} to drain their data before the {\splitter}
provides them with more. This improvement is only created in
presence of peeking. The second improvement is due to reducing
combinatorial growth and due to finer grained schedules to deal
with peeking.

Other applications show no or little improvement in buffer
requirements. As expected, no application requires more buffer
space.

It is interesting to note that for some applications the schedule
sizes have decreased between the single appearance and compressed
minimal latency phased schedules. This is due to slightly
different encoding technique of single appearance schedules.
