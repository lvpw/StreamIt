\section{Introduction}

%% Applications targetting embedded devices are becoming increasingly
%% large and complex.  
From handheld computers to cell phones to sensor networks, there has
been a surge of embedded applications that demand high-performance
digital signal processing.  These programs constitute a new and
important class of applications: those that are centered around {\it
streams} of data.  Despite the widespread parallelism and regular
communication patterns that are inherit in stream programs,
application development in the streaming domain is still very
labor-intensive and error-prone.  In order to optimize critical loops,
DSP programmers are often forced to resort to assembly code, thereby
sacrificing portability and robustness for the sake of performance.
As the complexity of embedded software grows, this practice will
become infeasible.  There is a pressing need to provide high-level
stream abstractions that can be compiled without sacrificing
efficiency.

The goal of the StreamIt project is to provide language and compiler
support for high-level stream programming.  A StreamIt program
consists of a set of autonomous filters that communicate using FIFO
queues.  Filters can be combined into single-input, single-output
modules by using a set of hierarchical primitives, thereby imposing a
structure on the stream graph that is akin to structured control flow
in a mainstream language.  In order to facilitate static scheduling,
the input and output rates of each filter are known at compile time.

In this paper, we present techniques for scheduling stream graphs such
as those found in StreamIt.  The StreamIt representation has much in
common with Synchronous Dataflow (SDF) graphs~\cite{lee87static}, for
which there is a large body of literature devoted to scheduling (see
\cite{bhattacharyya99synthesis} for a review).  There are two aspects
of StreamIt programs that distinguish our scheduling problem from a
general SDF graph: 1) StreamIt graphs are hierarchical, with each node
having only a single input and single output, and 2) StreamIt allows a
``peek'' operation whereby nodes can operate on items that they do not
consume until a future invocation.  In this context, this paper makes
the following contributions:
\begin{itemize}
\item Fundamental techniques for constructing a hierarchical schedule
from a hierarchical stream graph.

\item A method for computing an initialization schedule, which is a
unique requirement of graphs supporting the peek construct.

\item A parameterized phased scheduling algorithm that leverages the
structure of a StreamIt graph to give a flexible tradeoff between code
size and data size.

\item An instance of the phased scheduling algorithm that computes a
minimal latency schedule, guaranteed to avoid deadlock in any valid
feedback loop.
\end{itemize}
%% This paper makes the following contributions:
%% \begin{itemize}
%%
%% \item hieararhical scheduling of a streamig app hierarchical scheduling of
%% streaming application, a concept enabled by the {\StreamIt} language
%%
%% \item first formal handling of {\SDF} graphs with peeking
%%
%% \item a novel phased scheduling technique, giving a tradeoff between
%% code size and data size
%%
%% \item a minimum latency scheduler that uses hierarchical phases
%%
%% \end{itemize}
This paper is organized as follows.  The remainder of this section
gives an illustrating example and describes relevant {\StreamIt}
constructs; Section \ref{chpt:sched-basic} explains basic concepts in
scheduling {\StreamIt} graphs; Section \ref{chpt:phased} describes the
phased scheduling technique and presents a minimum latency scheduling
algorithm; Section \ref{chpt:results} presents experimental results;
Section \ref{chpt:related} describes related work and Section
\ref{chpt:conclusion} presents conclusions and planned future work.

\begin{figure}[t]
\psfig{figure=sched-diag2.eps,width=3.35in}
\caption{\small Execution trace of three different scheduling
strategies for one steady state execution of a simple pipeline.
Channels are annotated with the number of live data items that they
contain; shaded nodes represent those that fire on a given time
step.\protect\label{fig:trace}}
\end{figure}

\subsection{Example}
\label{sec:sched-vs-buffer}

A classic problem in the scheduling of synchronous dataflow graphs is
the tradeoff between code size and data size~\cite{bhat1999x1}.  Code
size refers to the space needed to represent the schedule, while data
size refers to the buffering of items during execution.  Generally
speaking, smaller schedules contain loops that require coarse-grained
execution of nodes, thereby leading to larger buffer requirements.

Consider the example stream graph depicted in Figure~\ref{fig:trace}.
Even given a simple pipeline of filters, there is a large space of
different schedules, each with different requirements for code and
buffer size.  Figure~\ref{fig:trace} illustrates two extreme
scheduling policies.  First is Single Appearance Scheduling (SAS),
which gives the minimal code size: the schedule is a loop nest with
each node appearing at a single position.  SAS is generally the method
of choice in the DSP community, as the contents of each node can be
inlined without duplicating code.  There are many different SAS
schedules for a given stream graph; the one shown in
Figure~\ref{fig:trace} is the best SAS schedule for this case.
%Even within sas schedules, it
%np-hard to determine the one that gives the smallest buffers.

At the other end of the spectrum is a push schedule, which results in
the minimal buffer size at the expense of code size.  A push schedule
starts by executing the top-most node, and then pushes the items
produced through the rest of the graph, always executing the most
downstream node possible.  When no further node can fire, the top node
is executed again.  In this case, the push schedule reduces the buffer
size by 48\% but increases code size by 325\% over the SAS schedule.
%However, since the code was originally smaller than the buffers, the
%total amount of space required decreases slightly (see
%Figure~\ref{fig:codedata}).

In this paper, we develop a phased scheduling algorithm that offers a
flexible alternative between the extremes of SAS and push scheduling.
Shown in Figure~\ref{fig:trace}(b) is the phased minimum latency
schedule.  It consists of three ``phases'', each of which is a
single-appearance sub-schedule that results in a single output item
being produced from the pipeline.  The schedule has the same latency
as the push schedule, but has reduced code size due the
single-appearance property of the phases and the collapse of phases 2
and 3 into a single representation ``E''.

Figure~\ref{fig:codedata} illustrates the tradeoff between code size
and data size for the scheduling schemes.  It shows that there can be
a large tradeoff between code size and buffer size, with phased
scheduling striking a compromise between extremes.  In
Section~\ref{chpt:phased}, we give a flexible version of our phased
scheduling algorithm, and we also demonstrate that it can handle tight
feedback loops for which there does not exist a valid single
appearance schedule.

\begin{comment}
We can compare the storage efficiency of these two schedules by
assuming that one data item in a buffer requires $x$ amount of memory
and each entry in a schedule requires $y$ amount of memory.  Thus the
two schedules will require the same amount of storage to store
themselves and execute if $11 x + 18 y = 39 x + 4 y$.

\begin{displaymath}
\begin{array}{rcl}
11 x + 18 y & = & 39 x + 4 y \\
14 y & = & 28 x \\
y & = & 2x
\end{array}
\end{displaymath}

Thus the smaller schedule is more efficient if every data item
requires less than twice the amount of storage than every entry in
the schedule.

One of the difficulties in scheduling {\StreamIt} programs lies in
finding a good set of trade-offs between schedule size and
buffering requirements.
\end{comment}

\begin{comment}

\subsection{Minimum Buffer Size between {\filters}}

As illustrated above, the amount of buffering in a {\pipeline} can
be affected greatly by the order of executions of {\filters} in the
{\pipeline}.  The following equation calculates the minimal buffer
size required in order for two {\filters} to be able to push data
between each other indefinitely in the most buffer-efficient way.
Buffers this size cannot always be achieved, because some
components require that data be buffered up for execution (ex.
{\feedbackloops} require data to exist internally in order to
execution to advance) or because extra latency constrains require
additional buffering.

\begin{equation}
buffer_{A \to B} = \left\lceil {{peek_B} \over {\gcd(push_A,
pop_B)}} - 1 \right\rceil \gcd (push_A, pop_B) + push_A
\end{equation}

\emph{I can explain this equation, but I cannot prove it.  what
should I do with this?  it's not necessary for the thesis, but it
is a neat result we never published (PLDI submission), nor have I
seen it in any other papers (nobody does peeking, so it can't be
anywhere else)}

\end{comment}

\begin{figure}[t]
\psfig{figure=codedata.eps,width=3.35in}
\caption{\small Buffer and code sizes for the execution traces of
Figure~\ref{fig:trace}.  For brevity, we show these figures on the
same graph, even though a unit of storage might have a different cost
for code and data.\protect\label{fig:codedata}}
\end{figure}
