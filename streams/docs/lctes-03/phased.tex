\section{Phased Scheduling}
\label{chpt:phased}

\begin{figure}[t]
\psfig{figure=pseudocode2.eps,width=3in}
\end{figure}

We now propose Phased Scheduling, a technique which allows us to
schedule all valid {\StreamIt} programs, and which allows for
better control of the trade-off between schedule size and buffer
size.

Section \ref{sec:phased:intro} provides an introduction to and
explanation of Phased Scheduling. Section \ref{sec:min-latency}
presents a Minimal Latency Schedule implementation using Phased
Scheduling.

\subsection{Phased Scheduling}
\label{sec:phased:intro}

The pseudo single-appearance hierarchical scheduling technique
presented in Section \ref{chpt:hierarchical}, while quite
effective in scheduling simple applications, cannot schedule a
small number of tight {\feedbackloops}. Furthermore, the
technique is quite inflexible when it comes to attempting to
create a different tradeoff between schedule size and buffer size.
Phased scheduling solves both of these problems.

A phased schedule is a hierarchical schedule, just like the pseudo
single appearance schedule. Every stream operator uses only the
schedule of its immediate child operators to create its own
schedule. Phased schedules, however, consist of several steps,
called phases. All of the phases must be executed in order to
guarantee correctness. Once all of the phases have been executed,
the complete schedule has been executed. A parent operator can
interleave execution of its children's phases in its own schedule,
to provide any level of granularity desired.

The granularity of splitting the steady state schedule into phases
is left up to the specific scheduler.  Different operators can use
different granularities of execution.  In principle, the parent
should not need to know the scheduling granularity of its
children. The only exception to this rule are {\feedbackloops},
which can have children which are not scheduled tightly enough to
allow the {\feedbackloop} to execute.

\subsubsection{Notation}

A phased schedule of a stream operator $s$ is an ordered set
$P_s$ of elements, $P_s = \{T_s, I_s, c_s, c^i_s\}$.  The first
element, $T_s$ denotes the phases used for the steady state
schedule of $s$. $I_s$ denotes the phases used for the
initialization schedule of $s$. $c_s$ and $c^i_s$ are defined
identically to their definitions in hierarchical schedules: $c_s$
is the consumption rate of the operator during its steady state
execution and $c^i_s$ is the consumption rate of the
initialization schedule.

$T_s$ and $I_s$ are defined by identical structures.  Both are
defined as ordered sets of phases. The only real difference
between $T_s$ and $I_s$ is that $T_s$ will be executed
indefinitely, while $I_s$ will be executed only once. A phase $A$
is defined as $A = \{E, c\}$.  $E$ is an ordered list of
children's phases, {\splitters} and {\joiners} that are to be
executed in order to execute the phase. $c$ is the consumption of
the phase, with respect to its operator.

\subsection{Minimal Latency Phased Scheduling}
\label{sec:min-latency}

One of the problems with pseudo single-appearance scheduling is
that it cannot schedule all legal {\StreamIt} programs.  A program
with a {\feedbackloop} can have requirements for tight execution
that cannot be satisfied using a pseudo single-appearance
schedule, leading to deadlock. Phased scheduling can alleviate
this problem by allowing the program to be scheduled in a more
fine-grained manner. Minimal latency scheduling is an example of a
specific scheduling strategy that solves the problem of deadlock.
A minimal latency schedule is a schedule that consumes a minimal
amount of input data in order to output data. In other words, a
minimal latency schedule only buffers up as much data as is
absolutely necessary to produce an output. This means that if a
{\feedbackloop} can be scheduled, a minimal latency schedule will
be able to schedule it.

\subsubsection{\filter}

Since {\filters} have no internal buffering and only one {\work}
function, their schedules are simple.\footnote{The description
presented here allows a {\filter} only a single work function. The
internal implementation of this algorithm uses the concept of
phases to implement a form of cyclo-static {\filters} - {\filters}
which have multiple work functions, each one with its own amount
of data to peek, pop and push. A steady state execution of such a
{\filter} executes once all of its work functions in order.} They
contain a single phase, which in turn contains a single execution
of the filter's {\work} function.  Although in principle, a
{\filter} does not need to be executed to be initialized, it may
require some data to be buffered up for its steady state
execution. This means that if $e_f > o_f$, we insert an artificial
initialization phase to phased schedules of {\filters}:

\begin{displaymath} \small
P_p = \left\{
\begin{array}{c}
T_p = \left\{
\begin{array}{c}
A_{f,0} = \left\{ \{f\}, \left[\begin{array}{c} e_f \\ o_f \\ u_f \end{array}\right]\right\} \\
\end{array}\right\}, \\
I_p = \left\{ A^i_{f,0} = \left\{ \{ \}, \left[\begin{array}{c}e_f - o_f \\ 0 \\ 0 \end{array}\right]\right\} \right\}, \\
c_p = \left[ \begin{array}{c} e_f \\ o_f \\ u_f \end{array}
\right], c^i_p = \left[ \begin{array}{c} e_f - o_f \\ 0 \\ 0
\end{array} \right]
\end{array}
\right\}
\end{displaymath}

\subsubsection{{\pipeline}, {\splitjoin} and {\feedbackloop}}

{\pipeline}, {\splitjoin} and {\feedbackloop} all share the same
principal algorithm for calculating the minimal latency phased
schedule.

For the steady state, every phase is calculated separately. For
every phase, the amount of data buffered between child operators
and the number of phases each child operator needs to execute to
complete a full steady state execution of the parent operator are
known. Phases are calculated until no more children can execute
without entering the next steady state execution of the parent
operator. The calculation of a phase is done in three steps. The
first two steps simulate execution of the child operators, while
the third step actually produces a scheduled phase.

In the first step, an output is produced from the most downstream
child, and all the upstream children provide sufficient data to
ensure correct execution. In the second step, any left-over data
is flushed down, and the downstream most child possibly produces
some additional output. The third step adds up the number of
phases executed by each child, and a phase which does not
interleave the phase executions is produced.

Although the second step is not absolutely necessary for the
correctness of the algorithm, it does ensure that we compute the
absolute minimum number of phases necessary for the minimal
latency schedule. The schedule produced is still a minimal latency
schedule, because the upstream most child is not allowed to
consume any data in the second step.

The computation of the first step in this algorithm is a little
complicated by the fact that it is not trivially obvious how many
phases the upstream children need to execute in order to feed
enough data to the downstream most child. The computation starts
with the downstream most child. This child consumes some data
until it produces some output. The data consumed by this child may
not have been produced by the immediately upstream child. For this
reason we introduce the concept of data borrowing. As a child
executes, it is allowed to consume or peek data from its {\Input}
{\Channel} without that data actually having been pushed onto the
{\Channel}. We track the amount of data borrowed from the
{\Channel} and ensure that the upstream child eventually produces
the appropriate amount of data to guarantee that the final
produced phase is valid. The amount of data borrowed from
{\Channel} $in_A$ is equal to $\max(-(in_A - e_A),0)$. Note that
because of borrowing, the amount of data stored in a {\Channel}
can become negative. Furthermore, the amount of data borrowed may
be greater than the negative of number of data stored in a
{\Channel}. The amount of data borrowed from a {\Channel} is shown
in Table \ref{tbl:min-lat-sj} in brackets.

The initialization schedule starts with no internally buffered
data (with exception of {\feedbackloops}) and executes as many
phases as is necessary to ensure that all children have executed
all of their initialization phases. Once that has been achieved,
the steady state schedule is created. The only difference between
computation of an initialization and steady state schedules is
that the steady state schedule stops executing children early, if
they have already executed all the phases allocated to them for
the steady state. The initialization schedule continues executing
until all initialization phases of all children have been
executed.

The only significant difference between the algorithms used for
minimal latency scheduling of different operator types
({\pipeline}, {\splitjoin} and {\feedbackloop}) is the order in
which children of the operator are considered for execution.

For a {\pipeline}, the order with which children are considered
for execution is as follows.  In the first step, all the children
are considered for execution moving from bottom to top. The
{\pipeline}'s last child executes just enough phases to produce
some data, borrowing data from its {\Input} {\Channel}. The child
directly above it executes just enough phases to provide
sufficient data for the last child to execute (reset the amount of
data borrowed from the {\Channel} to zero). This process is
repeated until the top-most child is reached. At this point the
direction of traversal is reversed, and the second step of phase
calculation commences. This time, the top-most child is not
allowed to consume any data (though it may produce some). The next
child only executes as many phases as it can, while only using
data already buffered between it and the child above it (it's not
allowed to borrow). Then the next child executes, etc. This is
repeated until the bottom-most child is reached. The number of
phases executed by each child in both steps is added up, and the
phases are inserted in order (all phases of every child together,
in order, iterating from top-most child down to bottom-most
child). This constitutes one complete phase of the {\pipeline}.

For a {\splitjoin}, the process is similar, but the children are
first executed bottom up starting with {\joiner}, then the
operator children (order of which is irrelevant) and finally the
{\splitter}. After, the children are executed from top to bottom
(excluding the {\splitter}), consuming only data already available
to them in {\Channels}.

Scheduling of {\feedbackloops} is again similar to the above
algorithms.  The children's phases are executed in order of
({\splitter}, body child, {\joiner}) for the first step, and (body
child, {\splitter}, loop child) for the second step.  The
{\splitter} executes exactly one time on its first iteration.  The
body child and the {\joiner} execute just enough times to provide
data for the {\splitter} to perform its execution. Then the body
child, {\splitter} and the loop child are executed as many times
as possible with the data available to them on their {\Input}
{\Channels}.

The one big difference between {\feedbackloop} and the other
operators ({\pipeline} and {\splitjoin}) is that in scheduling a
{\feedbackloop}, the {\joiner} is {\emph{not}} allowed to borrow
elements from $out_L$ {\Channel}.  That is in the trace table, the
$out_L$ entry is never allowed to become negative.  The reason for
this is that {\feedbackloops} are cyclical structures, and
allowing the {\joiner} to borrow elements from $out_L$ would cause
a full cycle of borrowing, leading to possible deadlock.

\begin{table*}[t] \centering  \scriptsize
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{data in {\Channel}} & \multicolumn{4}{c|}{\parbox{1in}{\centering phase executions left}} & \parbox{0.5in}{\centering child considered} & \parbox{0.6in}{\centering phases executed} & \parbox{0.6in}{\centering {\pipeline} consumption} \\
\cline{1-8} $in_A$ & $out_A$ & $in_B$ & $out_B$ & split & A & B & join & & & \\
\hline 0 (0) & 0 (0) & 0 (0) & 0 (0) & 0 & 0 & 1 & 0 & join & - & $[0\ 0\ 0]$ \\
\hline 0 (0) & 0 (0) & 0 (0) & 0 (0) & 0 & 0 & 1 & 0 & A & - & $[0\ 0\ 0]$ \\
\hline 0 (0) & 0 (0) & 0 (0) & 0 (0) & 0 & 0 & 1 & 0 & B & $A^i_{B,0}$ & $[0\ 0\ 0]$ \\
\hline 0 (0) & 0 (0) & 0 (1) & 0 (0) & 0 & 0 & 0 & 0 & split & split & $[3\ 3\ 0]$ \\
\hline 2 (0) & 0 (0) & 1 (0) & 0 (0) & 0 & 0 & 0 & 0 & A & $A^i_{A,0}$ & $[0\ 0\ 0]$ \\
\hline 0 (0) & 1 (0) & 1 (0) & 0 (0) & 0 & 0 & 0 & 0 & B & - & $[0\ 0\ 0]$ \\
\hline 0 (0) & 1 (0) & 1 (0) & 0 (0) & 0 & 0 & 0 & 0 & join & - & $[0\ 0\ 0]$ \\
\hline 0 (0) &  1 (0) &  1 (0) &  0 (0) & \multicolumn{7}{|c|}{init phase 0 done, init done} \\
\hline 0 (0) & 1 (0) & 1 (0) & 0 (0) & 2 & 2 & 1 & 2 & join & join & $[0\ 0\ 4]$ \\
\hline 0 (0) & 0 (0) & 1 (0) & -3 (3) & 2 & 2 & 1 & 2 & A & - & $[0\ 0\ 0]$ \\
\hline 0 (0) & 0 (0) & 1 (0) & -3 (3) & 2 & 2 & 1 & 1 & B & $A_{B,0}$ & $[0\ 0\ 0]$ \\
\hline 0 (0) & 0 (0) & -1 (2) & 3 (0) & 2 & 2 & 0 & 2 & split & $\{2split\}$ & $[6\ 6\ 0]$ \\
\hline 4 (0) & 0 (0) & 1 (0) & 3 (0) & 0 & 2 & 0 & 2 & A & $\{2A_{A,0}\}$ & $[0\ 0\ 0]$ \\
\hline 0 (0) & 2 (0) & 1 (0) & 3 (0) & 0 & 0 & 0 & 0 & B & - & $[0\ 0\ 0]$ \\
\hline 0 (0) & 2 (0) & 1 (0) & 3 (0) & 0 & 0 & 0 & 1 & join & join  & $[0\ 0\ 4]$ \\
\hline 0 (0) &  1 (0) &  1 (0) &  0 (0) & \multicolumn{7}{|c|}{phase 0 done, steady state schedule done} \\
\hline
\end{tabular}
\caption[Execution of Minimal Latency Scheduling Algorithm on a
{\splitjoin}]{Execution of Minimal Latency Scheduling Algorithm on
{\splitjoin} from Figure \ref{fig:steady-state}(b). In the ``data
in {\Channel}'' columns, the first value represents the actual
number of data in the {\Channel}, which can be negative due to
data borrowing. The second value is the minimal number of data
items borrowed from the {\Channel}.}
\label{tbl:min-lat-sj}
\end{table*}

Table \ref{tbl:min-lat-sj} contains a trace of execution of our
algorithm on the sample {\splitjoin} from Figure
\ref{fig:steady-state}(b). Below is the phased schedule for the
{\splitjoin}. Note that this example produces a phased single
appearance schedule.

\begin{displaymath} \scriptsize
P_{sj} = \left\{
\begin{array}{c}
T_{sj} = \left\{
\begin{array}{c}
A_{sj,0} = \left\{ \{\{2 split\}\{2A\}B\{2 join\}\}, \left[\begin{array}{c} 6 \\ 6 \\ 8 \end{array}\right]\right\} \\
\end{array}\right\}, \\
I_{sj} = \left\{ A^i_{sj,0} = \left\{
\{split\ A^i_{A,0}\ A^i_{B,0}\}, \left[\begin{array}{c} 3 \\ 3 \\ 0 \\
\end{array}\right]\right\}
\right\}, \\
c_{sj} = \left[ \begin{array}{c} 6 \\ 6 \\ 8 \end{array} \right],
c^i_{sj} = \left[ \begin{array}{c} 3 \\ 3 \\ 0 \end{array}
\right],
\end{array}
\right\}
\end{displaymath}
