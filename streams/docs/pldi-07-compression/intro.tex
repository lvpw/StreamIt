\section{Introduction}

With the emergence of data-intensive applications such as digital
film, medical imaging and geographic information systems, the
performance of next-generation systems will often depend on their
ability to process huge volumes of data.  For example, each frame of a
digital film requires approximately 2 megabytes, implying that a
fully-edited 90-minute video demands about 300 gigabytes of data for
the imagery alone~\cite{ibm-video}.  Industrial Light and Magic
reports that, in 2003, their processing pipeline output 13.7 million
frames and their internal network processed 9 petabytes of
data~\cite{ilm-interview}.  The U.S. Geological Survey had archived
over 13 million frames of photographic data by the end of 2004, and
estimates that 5 years is needed to digitize 8.6 million additional
images~\cite{usgs}.  In all of these situations, the data is highly
compressed to reduce storage costs.  At the same time, extensive
post-processing is often required for adding captions, watermarking,
resizing, compositing, adjusting colors, converting formats, and so
on.  As such processing logically operates on the uncompressed format,
the usual practice is to decompress and re-compress the data whenever
it needs to be modified.

%YouTube manages about 45 terabytes of video data~\cite{wsj-youtube},
%with 65,000 videos uploaded daily~\cite{youtube}.  Microsoft
%TerraServer holds upwards of 22 terabytes of image data, serving 69
%gigabytes per day~\cite{terraserver}.

In order to accelerate the process of editing compressed data,
researchers have identified specific transformations that can be
mapped into the compressed domain---that is, they can operate directly
on the compressed data format rather than on the uncompressed
format~\cite{chang95survey,mandal95survey,smith95survey,wee02survey}.
In addition to averting the cost of the decompression and
re-compression, such techniques greatly reduce the total volume of
data processed, thereby offering large savings in both execution time
and memory footprint.

However, existing techniques for operating directly on compressed data
have two limitations.  First, they focus on lossy compression formats
(e.g., JPEG, MPEG) rather than lossless compression formats, which are
important for the applications cited previously.  Second, they rely on
specialized and ad-hoc techniques for translating individual
operations into the compressed domain.  For example, for DCT-based
spatial compression formats (JPEG, Motion-JPEG), researchers have
developed separate algorithms for resizing~\cite{dugad01,mukherjee02},
edge detection~\cite{shen96b,shen96}, image
segmentation~\cite{feng03}, shearing and rotating inner
blocks~\cite{shen98}, and arbitrary linear combinations of
pixels~\cite{smith96b}.  Techniques extending to DCT-based temporal
compression (MPEG) include captioning~\cite{nang00},
reversal~\cite{vasudev98}, distortion detection~\cite{dorai00},
transcoding~\cite{smith98}, and others~\cite{wee02survey}.  For
run-length encoded images, algorithms have been devised for efficient
transpose and rotation~\cite{misra99,shoji95}.  A compressed audio
format has been invented that allows direct modification of pitch and
playback speed~\cite{levine98}.  While these techniques are powerful,
they remain inaccessible to most application programmers because they
demand intricate manipulation of the underlying compression format.
It is difficult for non-experts to compose existing compressed-domain
operations into a complete program, let alone translate a new and
unique operation into the compressed domain.

This paper presents a technique for automatically mapping complete
user-defined programs into the compressed domain.  The technique
applies to stream programs: a restricted but practical class of
applications that perform regular processing over long data sequences.
Stream programming captures the essential functionality needed by
image, video, and signal processing applications while exposing the
flow of data to the compiler.  Our transformation targets LZ77, a
lossless encoding utilized by ZIP, and immediately applies to simpler
formats such as Apple Animation, Microsoft RLE, and Targa.  Lossless
compression is widely used in computer animation and digital video
editing in order to avoid accumulating compression artifacts.  By
providing a programmable solution, our technique enables a large class
of transformations to be customized by the user and directly applied
to the compressed data.

The key idea behind our technique can be understood in simple terms.
In LZ77, compression is achieved by indicating that a given part of
the data stream is a repeat of a previous part of the stream.  If a
program is transforming each element of the stream in the same way,
then any repetitions in the input will necessarily be present in the
output as well.  Thus, while new data sequences need to be processed
as usual, any repeats of those sequences do not need to be transformed
again.  Rather, {\it the repetitions in the input stream can be
  directly copied to the output stream}, thereby referencing the
previously-computed values.  This preserves the compression in the
stream while avoiding the cost of decompression, re-compression, and
computing on the uncompressed data.  In this paper, we extend this
simple idea to a broader class of programs: those which input and
output multiple data items at a time, and those which split, combine,
and reorder the data in the stream.

We implemented a subset of our full technique in the StreamIt
compiler, which automatically generates plugins for two popular video
editing tools: MEncoder and Blender.  We evaluated the technique for a
variety of pixel transformations (brightness, contrast, color
inversion) as well as video compositing (overlays and mattes).  Using
a suite of 12 videos (screencasts, computer animations, digital
television content) in Apple Animation format, computing directly on
compressed data offers a speedup roughly proportional to the
compression factor.  For pixel transformations, speedups range from
3.1x to 235x, with a median of 19x; for video compositing, speedups
range from 1.0x to 35x, with a median of 7.4x.

In the general case, compressed processing techniques may need to
partially decompress the input data to support the behavior of certain
programs.  Even if no decompression is performed, the output may
benefit from an additional re-compression step if new redundancy is
introduced during the processing (for example, increasing image
brightness can whiteout parts of the image).  This effect turns out to
be minor in the case of our experiments.  For pixel transformations,
output sizes are within 0.1\% of input sizes and often within 10\%
(median case) of a full re-compression.  For video compositing, output
files maintain a sizable compression ratio of 8.8x (median) while full
re-compression results in a ratio of 13x (median).

To summarize, this paper makes the following contributions:
\begin{itemize}

\item An algorithm for mapping an arbitrary stream program, written in
  the cyclo-static dataflow model, to operate directly on lossless
  LZ77-compressed data (Sections 2-3).

\item An analysis of popular lossless compression formats and the
opportunities for direct processing on each (Section 4).

\item An experimental evaluation in the StreamIt compiler,
  demonstrating that automatic translation to the compressed domain
  can speedup realistic operations in popular video editing tools.
  Across our benchmarks, the median speedup is 16x (Section 5).

\end{itemize}

The paper concludes with related work (Section 6) and conclusions
(Section 7).
