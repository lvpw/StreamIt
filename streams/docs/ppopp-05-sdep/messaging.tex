\section{Precise Event Handling}
\label{sec:language}

In this section we describe how $\sdep$ information can be
incorporated into the semantics of a language feature that provides
precise delivery of control messages in stream programs.  Our goal is
to improve performance as well as programmer productivity.

The Synchronous Dataflow (SDF) domain is well-suited for applications
that have regular, high-bandwidth communication patterns.  However, in
realistic streaming applications there are also irregular,
low-bandwidth control messages that are used to adjust parameters in
various parts of the stream.  For example, a downstream actor might
detect a high signal-to-noise ratio and send a message to the
communications frontend to increase the amplification.  Or, an actor
at the top of the stream graph might detect an invalid checksum for a
packet, and send a message downstream to invalidate the effects of
what has been processed.  Other examples of control messages include:
periodic channel characterization; adaptive beamforming; initiating a
handoff ({\it e.g.,} to a new network protocol); marking the end of a
large data segment; and responding to user inputs, environmental
conditions, or runtime exceptions.

Generally speaking, control messages are sent at infrequent and
irregular intervals; however, once they are generated they might be
tightly constrained with respect to their delivery.  For example, the
message invalidating the effects of a given packet has to be delivered
exactly in sync with the front of the packet itself; otherwise, valid
data will be lost or invalid data will be allowed to pass.

\subsection{Teleport Messaging}
\label{sec:teleport}

Teleport messaging is a language construct that makes use of $\sdep$
to achieve precise timing of control messages.  It is included as part
of the StreamIt language~\cite{streamitcc}.  This section describes
the semantics of teleport messaging; the next section compares it
against other means of implementing messages for a frequency hopping
radio application.

In StreamIt, there are two distinct kinds of communication between
filters during steady state execution: 1) high-bandwidth dataflow over
the FIFO channels in the graph, and 2) low-bandwidth messaging between
pairs of filters\footnote{\small Messaging is possible whenever there
is a downstream path from either filter to the other.  Filters running
in parallel cannot send messages.}.  In order for filter $A$ to send a
message to filter $B$, the following steps need to be taken:
\begin{itemize}

\item $B$ declares a message handler that is invoked when a
message arrives.  For example:
{\small
\begin{verbatim}
handler increaseGain(float amount) {
  this.gain += amount;
}
\end{verbatim}
}
Message handlers are akin to normal functions, except that they
cannot access the input/output channels and they do not return values.

\item A parent stream containing $A$ and $B$ declares a variable
of type {\tt portal<} $T_B$ {\tt >} that can forward messages to
any actor of type $T_B$.  The parent adds $B$ to the portal and passes
the portal to $A$ during initialization.

\item To send a message, $A$ invokes the handler method on the portal
from within its steady state work function. The handler invocation
includes a range of latencies specifying when the message should be
delivered. The following illustrates an example.\\
{\small
\begin{verbatim}
work pop 1 {
  float val = pop();
  if (val < THRESHOLD) {
    portalToB.increaseGain(0.1) [2:3];
  }
}
\end{verbatim}}
This code sends an {\tt increaseGain} message to {\tt portalToB} with
minimum latency 2 and maximum latency 3.

\end{itemize}
The most interesting aspect of teleport messaging are the semantics
for the message latency.  Because there are many legal orderings of
actor executions, there does not exist a notion of ``global time'' in
a stream graph.  The only common frame of reference between
concurrently executing actors is the series of data items that is
passed between them.  The $\sdep$ function captures the data
dependences in the graph and provides a natural means of defining a
rendezvous point between two actors.

Intuitively, the message semantics can be thought of in terms of
attaching tags to data items.  If $A$ sends a message to downstream
filter $B$ with a latency $k$, then this could be implemented by
tagging the items that $A$ outputs $k$ iterations later.  These tags
propagate through the stream graph; whenever an actor inputs an item
that is tagged, all of its subsequent outputs are tagged.  Then, the
message handler of $B$ is invoked immediately before the first
invocation of $B$ that inputs a tagged item.  In this sense, the
message has the semantics of traveling ``with the data'' through the
stream graph, even though it does not have to be implemented this way.

The intuition for upstream messages is similar.  Consider that $B$ is
sending a message with latency $k$ to upstream actor $A$ in the stream
graph.  This means that $A$ will receive the message immediately
before the last invocation of its work function that produces an item
affecting the output of $B$'s $k$th firing, counting the current
firing as 0.  As before, we can also think of this in terms of $A$
tagging items and $B$ observing the tags.  In this case, the latency
constraint says that $B$ must input a tagged item before it finishes
$k$ additional executions.  The message is delivered immediately
before the latest firing in $A$ during which tagging could start
without violating this constraint.

\begin{table*}[t]
\begin{center}
{\small
\begin{tabular}{|r|c|c|} \hline
~ & {\bf Negative latency} & {\bf Positive latency} \\ \hline
{\bf Message travels downstream} & latency in schedule must not be too small & no constraint \\ \hline
{\bf Message travels upstream} & illegal & latency in schedule must not be too big \\ \hline
\end{tabular}}
\caption{\small Scheduling constraints imposed by messages.}
\label{tab:messcons}
\end{center}
\vspace{-12pt}
\end{table*}

The following definition leverages the $\sdep$ formalism to give a
precise meaning to message timing.

\begin{definition}(Message delivery)
Consider that $S$ sends a message to receiver $R$ with latency range
$[k_1:k_2]$ and that the message is sent during the $n$th invocation
of $S$'s work function.  Then the message handler can be invoked in
$R$ immediately before its work function has fired ${\cal M}(S, R,
k_1, k_2, n)$ times, where ${\cal M}$ is constrained as follows.

There are two cases\footnote{\small In a feedback path, both cases might apply.  In this event, we assume the message is being sent upstream.}:
\begin{enumerate}

\item There is a path in the stream graph from $S$ to $R$.  Then
${\cal M}$ obeys the following constraints:
\[
\begin{array}{l}
\sdepf{S}{R}({\cal M}(S, R, k_1, k_2, n)) \ge n+k_1\\
\sdepf{S}{R}({\cal M}(S, R, k_1, k_2, n)) \le n+k_2
\end{array}
\]

\item There is a path in the stream graph from $R$ to $S$.  Then
${\cal M}$ obeys the following constraints:
\[
\begin{array}{l}
{\cal M}(S, R, k_1, k_2, n) \ge \sdepf{R}{S}(n + k_1)\\
{\cal M}(S, R, k_1, k_2, n) \le \sdepf{R}{S}(n + k_2)
\end{array}
\]
\end{enumerate}
\end{definition}

It is instructive to note that messaging can place constraints on the
execution schedule.  The different categories of constraints are
illustrated in Figure~\ref{tab:messcons}.  A negative-latency
downstream message has the effect of synchronizing the arrival of the
message with some data that was previously output by the sender ({\it
e.g.,} for the checksum example mentioned earlier).  The latency
requires the downstream actor not to execute too far ahead, or else it
might process the data before the message arrives.  This translates to
a constraint on the minimum latency between the sender and receiver
actors in the schedule of the program.

Similarly, a positive latency upstream message places a constraint on
the maximum latency between the sender and receiver.  Again the
receiver must be throttled so that it does not get too far ahead
before the message arrives.

An upstream message with negative latency is impossible to deliver,
because the data dependences imply that the target iteration has
already passed when the message was sent.  Conversely, a downstream
message with positive latency imposes no constraint on the schedule,
as the sender has not yet produced the data that is synchronized with
the message.

Calculating a legal sequence of actor firings that respects all of the
message constraints in a stream graph is an interesting problem.  In
the case where there are no overlapping constraints (i.e., two
sender/receiver pairs do not span overlapping intervals of the stream
graph), a simple modification to pull scheduling suffices to satisfy
the constraints.  First, note that a pull schedule will always satisfy
constraints imposed by upstream messages; because upstream (receiving)
actors execute as little as possible per execution of the downstream
(sending) actor, a message can be forwarded to the receiver
immediately after sending.  The receiver can then store the message
and process it at the appropriate iteration.  For downstream messages,
the pull scheduler is modified to always execute one iteration of the
upstream (sending) actor before any execution of the downstream
(receiving) actor that would exceed the latency range.  If the
upstream actor needs more inputs to fire, then they can always be
generated by actors that are further upstream (via a recursive call to
the pull scheduling algorithm).

For the general case of overlapping constraints, it is possible that a
set of message constraints admits no feasible schedule even though
each constraint is feasible in isolation.  Because overlapping
constraints can be detected at compile time, a given compiler may
choose to prohibit overlapping constraints altogether.  A discussion
of the issues involved appears in a thesis~\cite{karczma-thesis} by
one of the authors.

As described in Section~\ref{sec:evaluation}, our compiler uses a
simple implementation of messaging in which each sender or receiver
executes in its own thread and waits for possible messages at
appropriate iterations.  This approach eludes the need to produce a
serial ordering of the actors at compile time.

%\input{constrained}

\subsection{Case Study}

To illustrate the pros and cons of teleport messaging, we implemented
a spread-spectrum frequency hopping radio frontend~\cite{harada02} as
shown in Figure~\ref{fig:fhr-streamit}.  A frequency hopping radio is
one in which the receiver switches between a set of known frequencies
whenever it detects certain tones from the transmitter.  The frequency
hopping is a good match for control messages because the hopping
interval is dynamic (based on the data in the stream); it spans a
large section of the stream graph (there is an FFT between the
demodulator and the hop detector); and it requires precise delivery of
messages.  The delivery must be precise both to meet real-time
requirements (as the transceiver will leave the current frequency
soon), and to ensure that the message falls at a logical packet
boundary; if the frequency change is out of sync with the Fast Fourier
Transform (FFT) stage, then the FFT will muddle the spectrum of the
old and new frequency bands.

A StreamIt version of the radio frontend with teleport messaging
appears in Figure~\ref{fig:freq1}.  The Freq\_Hopping\_Radio pipeline
creates a portal and adds the RFtoIF actor as a receiver (lines 45 and 48 respectively).  The portal
is passed to the Check\_Freq\_Hop stage, where four parallel detectors
send messages into the portal if they detect a hop to the frequency
they are monitoring (lines 32-35).  The messages are sent with a latency of 4 to
ensure a timely transition.  To make sense of the latency, note that
$\sdepf{RFtoIF}{D}(n) = 64*n$ for each of the detector actors $D$.
This comes about because the FFT stage consumes and produces 64 items;
each detector fires once per set of outputs from the FFT, but RFtoIF
fires 64 times to fill the FFT input.  Because of this $\sdep$
relationship, messages sent from the detectors to RFtoIF are
guaranteed to arrive only at iterations that are a multiple of 64.
This satisfies the design criterion that a given FFT stage will not
operate on data that was demodulated at two separate frequencies.

Another version of the frequency hopping radio appears in
Figures~\ref{fig:fhr-manual} and~\ref{fig:freq2}.  This version is
functionally equivalent to the first, except that the control messages
are implemented manually by embedding them in the data stream and
introducing a feedback loop.  Because the number of items transfered
around the loop must be constant from one iteration to the next, a
data item is sent whether or not there is a message as part of the
algorithm; a special value of 0 represents that there is not a message
on the given iteration (in some other programs, if a special value is
not available, a structure may be passed through the stream with a
boolean flag indicating whether or not a message is present).  The
RFtoIF filter checks the values from the loop on every iteration and
processes them as a message if they are non-zero.  The I/O rate of the
RFtoIF filter has been scaled up to ensure that the messaging
information is received at intervals of 64 iterations (as in the
version with portals).  To achieve the desired messaging latency, a
number of items are enqueued on the feedback path prior to execution.

Yet another way to approximate the behavior of messaging is with a
direct function call from the detector to the RFtoIF stage.  (Though
such a call is disallowed in StreamIt, it could be an option in a
different programming model.)  While this approach is simple, it does
not have any timing guarantees.  There is no way for the sender to
know when in the course of the target's execution the message will be
received.  This could cause problems both for algorithm development
and for reliability / predictability of software.

\vspace{12pt}
\subsection{Discussion}

We believe that teleport messaging offers several benefits compared to
a manual implementation of equivalent functionality.  While embedding
messages in the data stream is equally precise, this involves several
tedious and error-prone changes, not only to the stream graph but also
to the steady state execution code within the actors.  In particular,
the manual derivation of the loop delay, adjustment of the actor I/O
rates, and implicit interleaving of data items with control messages
has a negative impact on the readability and maintainability of the
code.  Teleport messaging provides the same level of precision, but
with the simplicity of a method call.

Teleport messaging also has advantages from a compiler standpoint.  By
separating the data-intensive code from the control-oriented code, the
common case of the steady state actor execution is not sacrificed for
the uncommon case of message processing.  There are no ``dummy items''
serving as placeholders in the static-rate channels.  In addition, by
exposing the message latency as part of the language, the compiler can
infer the true dependences between filter firings and reorder the
execution so long as the message constraints are respected.  The
actual message delivery can be implemented in the most efficient way
for the given architecture.

\input{code-examples}

A final benefit of teleport messaging is the clean interface provided
by the portals.  Since a portal can have multiple receivers, it is
straightforward to send a message that is delivered synchronously to
two actors in parallel streams.  For example, consider a vocoder (an
encoder for voice signals) that is separately manipulating the
magnitude and phase components of a signal.  If something triggers an
adjustment to the speech transformation ({\it e.g.,} the speaker
requests a change of pitch) then the mask needs to be updated at the
same time relative to data in both parallel streams.  A portal that
contains both components seamlessly provides this functionality.
Finally, portals are useful as an external programming interface; an
application can export a portal based on an interface type without
exposing the underlying actor implementation.

