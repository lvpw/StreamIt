ppopp final
-----------

- layout

- try printing pdf, make sure speedup graph is ok

- spelling

- comment / highlight FIR code?

x initialize W throughout (esp. on the left)

x make lines in figure wider so you can see ... 

n "filters" vs "actors"?

n "execution" vs. "firing"?

x coeff vs. weight

x check SDEP -> \sdep

x remove italics from e.g and i.e.

x frequency hopping vs. frequency-hopping

x define upsream, downstream?

x sanitize bibliography

x unresolved references

x cite Brook, Cg

x adjust streamit diffs to other languages at end of related work

x issue with functionality of _pullSchedule_ The "for all input
channels" allows for more than one possible execution.  (Or do you
mean to make it evaluate left-to-right edges in order?)  As-is, the
authors need to state that all pull-schedules produce the same count,
which I believe they do.  E.g., after step 4 or 5 in the example,
actor B could fire -- this would produce a different execution
(PullSchedule is not a function), but the number of A's in those
executions will all be equal.  I believe this all fixable, but it
relies critically on what constructs are in the dataflow graph, since
something with nondetermism could make the greedy algorithm find a
non-minimal

x could still be more explicit in in explaining [n:m] latency
  "message sends are annotated with a range [n:m] that means...]

x though this model does not allow messaging between parallel
  components, it could be possible to time such
  things using a common ancestor (or sequence of ancestors)

x limitations / future work

  n there is a problem if messages sent during initialization schedule

  n add con of messaging: modify the I/O rate of a black-box actor in
    between the message source and destination, and it automatically
    changes the granularity of message behavior.  could imagine
    system where it is instead declared at high level what a parent assumes
    about child?  would need to think about this.

  x hierarchical messaging system so that containers do not have to
    know about children.

  x we only deal with non-overlapping constraints
    x say this in intro?

x I don't like "applications of sdep" section
  x move to another section, summarize in conclusion, or delete

x fix conclusion to emphasize messaging, not sdep

x constraints section
  x give example of infeasible constraints

  x more comprehensible notion of constraints (end of 5.1)
  x overlapping constraints
  x intervals of the stream graph

x example of calculating delivery time

x talk about compositionality of sdep?
  SDEP_C<-A (n) = SDEP_C<-B ( SDEP_B<-A (n))

x In section 5.1, the term "work function" comes up, which has not
been discussed previously.  What are the restrictions on these work
functions?

x rewrite parts of intro

x acknowledge grant numbers	

 x In definition 2, I had trouble with the phrase "the message
handler can be invoked in R immediately before its work function has
fired M(...) times".  Should that be "can only be invoked"?  I think
some kind of if-and-only-iff is needed here, since you want to invoke
them inside (never outside) the specific time period, but they also
have to be invoked sometime in that perior, don't they?  As a
notational point, the use of the notation "M(...)" as if it is some
function, which is clearly is not (it's only constrained to have some
range of values) is also bothersome.  I think it would be cleaner to
define the minimum and maximum values separately, which should both be
well-defined, and then put constraints on where the message handler is
invoked.

n more details on portal implementation
    4.3 The third paragraph. "The actual message delivery can be
    implemented in the most efficient way for the given architecture"
    I think that many readers, including me, are equally interested in
    the implementation of the portal. Thus, it is very instructive if
    you describe (perhaps in a separate section rather than here)
    things like how the portal is implemented in the system you used
    in 4.4 and what code is generated to satisfy the latency range
    specified.

n mention complexity of scheduling algorithm?
n mention inverse SDEP in the latency section?
n clean up second bullet of what happens with messaging (4.5)?

*********************************************************************************

general
-------
x describe how messaging is actually implemented (credits, SDEP, etc)
x rewrite abstract and intro
  - explain about SDF, StreamIt
  - explain roundrobin splitters, joiners
  - explain "actor" , "I/O rate" and "phase" of actor
x explain numbers in figure 1 (separated by commas)
x Figure 2: explain how to follow chart, what each line means
x explain what [n:m] notation means 
  x added explanation to example when first introduced
  - could still be more explicit somewhere:
    "Message sends are annotated with a range [n:m] that means..."
x fix "automatic" wraparound in figure caption
x define what "simulateCurrentPhase" means in Figure 1
  - fixed by comment and renaming of call

technical things
----------------
x say something about determining whether message constraints are feasible

n see if there is anything worth integrating in constrained.tex (on constrained messaging)
x resolve timing issue in radio, as raised by reviewer
x fix jasper's off-by-one errors.  Might be the same as reviewer comment:
 x There are two equations in the paragraph after Theorem 1. The second
   term of the RHS of the first equation is "SDEP A<-E (n-1 mod |S ^ E|)",
   but this should be written as "SDEP A<-E (n mod |S ^ E|)"? The same
   goes for the second equation.
x resolve SDEP bugs suggested by reviewer below

specific reviewer comments
--------------------------
- How often did you perform frequency hops?

- 4.3 The third paragraph. "The actual message delivery can be
  implemented in the most efficient way for the given architecture" I think
  that many readers, including me, are equally interested in the
  implementation of the portal. Thus, it is very instructive if you
  describe (perhaps in a separate section rather than here) things like
  how the portal is implemented in the system you used in 4.4 and what
  code is generated to satisfy the latency range specified.

n Section 4.1: After the bullet list, you might mention as you describe the
  semantics of message latency that this is much like a logical clock,
  well-known in distributed systems.

x Section 6: I didn't understand the comparison of Pugh and Rosser. "..we
  calculate the last interation of an actor which affects another."
  What is "another" referring to? "..last invocation that was
  relevant."  Is this the last invocation of the actor in question, or
  one of its predecessors? Being a bit more precise might help to
  clarify the connection here.

x What is the impact in terms of bandwidth of sending less data
  (than the manual messaging model)? Why is parallelism so much more
  important for your out-of-band messaging? 

x 3 The last paragraph. "so we would conclude that SDEP A<-B (1) = 4." I
  think that SDEP A<-B (1) = 5.

x 4.1 Definition 2. In Case 2,
  "SDEP A<-B (n+k_1)" -> "SDEP B<-A (n+k_1)" 
  "SDEP A<-B (n+k_2)" -> "SDEP B<-A (n+k_2)"

x 4.2 Figure 4. Where is the latency rage [4:6] specified in the code?

x 4.2 Figure 4. To make the paper self-contained, I think that you should
  explain a bit about StreamIt, in particlar, the roundrobin construct
  for splitter and joiner.

x Figure 1: the labels at the top and bottom of each node are not adequately
  explained. 

x Algorithm 1 has several problems. The recursive call to pullSchedule only
  has one actual parameter, yet the procedure itself has 2 parameters. Moreover,
  the auxiliary procedure simulateCurrentPhase(X) is not explained,
  and the semantics of the "o" operator are left unspecified. The two
  occurrences of "o" look slightly different, which is probably the
  result of a typo.

x section 4.2: What does the term FFT refer to? 

x section 4.3: What is a vocoder?

x Address concern: why should frequency-hopping radio app be tested on
  a high-performance distributed cluster. Wouldn't embedded hardware
  be more appropriate?  This makes me wary of the possible
  performance gains on realistic hardware (i.e. lower-speed, less
  parallelism).

x Section 3: It would be useful to point out initially that \mathcal{S} \in
  \Phi. Also, in Algorithm 1, you are missing the n parameter to the
  recursive call of pullSchedule(). I presume this should be the
  number of tokens needed to on c_i.

x Also in 4.1, I was confused for a while about whether or not
  out-of-band messages would affect the scheduling of synchronous
  streams. I came
  to believe the answer is yes, but I'm not sure how this would work,
  since the definition of SDEP relied on a particular pull
  schedule---how is this schedule affected by possible changes due to
  required latencies? Some stronger clarifications on the relation of
  message delivery and SDF scheduling would be helpful. Finally, the
  last sentence of 4.1 feels a little misleading: I believe you are
  meaning to say that a positive latency will not impact SDF
  scheduling, but not that positive latencies for downstream messages
  are illegal (while negative ones are for upstream messages).

x Section 4.4: Why was this cluster chosen as the benchmarking platform? I'd
  think you'd want to look at embedded hardware. More analysis please! In
  particular, 

original submission todo list
-----------------------------
- mention complexity of scheduling algorithm
- describe structured streams in streamit
- mention inverse SDEP in the latency section
- clean up second bullet of what happens with messaging (4.5)
n maybe superscript the n^th 
x make the label on the enqueue of the loop back path a little bigger
x incorporate EPS figs in apps
  x trim them, make sure width ends up the same
  x draw arrows maybe in illustrator
x the feedback edge in the graph of the manual radio should be darker
x change A and B to {S}ender and {R}eceiver
x come up with a name for messaging -- Warp or something.  It's
  impossible to write about
