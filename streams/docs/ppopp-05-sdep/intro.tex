\section{Introduction}

Algorithms that operate on streams of data are becoming increasingly
pervasive across a broad range of applications, and there is evidence
that streaming media applications already consume a substantial
fraction of the computation cycles on consumer
machines~\cite{kirkpatrick97,Rix98,dief97,conte97}.  Examples of
streaming workloads can be found in embedded systems (e.g., sensor
nets and mobile phones), as well as in desktop machines (e.g.,
networking and multimedia) and high-performance servers (e.g., HDTV
editing consoles and hyper-spectral imaging).

As high performance remains a critical factor for many streaming
applications, programmers are often forced to sacrifice readability,
robustness, and maintainability of their code for the sake of
optimization.  One notoriously difficult aspect of stream programming,
from both a performance and programmability standpoint, is in
reconciling regular streaming data flow with irregular control
messages.  While the high-bandwidth flow of data is very predictable,
realistic applications also include unpredictable, low-bandwidth
control messages for adjusting parameters in various parts of the
system.  Such control messages often have strict timing constraints,
which complicates their implementation on a parallel system.

Consider for example a frequency hopping radio (FHR), which mirrors
how CDMA-based cell phone technology works.  In FHR, a transmitter and
a receiver switch between a set of known radio frequencies, and they
must do so in synchrony with respect to a stream boundary. That is, a
receiver must switch its radio frequency at an exact point in the
stream (as indicated by the transmitter) in order to follow the
incoming signal.  Such a receiver is challenging to implement in a
distributed environment because different processors might be
responsible for the radio frontend and the frequency hop detection.
When a frequency hop is detected, the detector must send a message to
the frontend that is timed precisely with respect to the data stream,
even though the two components are running on different processors
with independent clocks.

Other instances of control messaging have a similar flavor.  A
component in a communications frontend might detect an invalid
checksum for a packet, and send a precisely-timed message downstream
to invalidate the effects of what has been processed.  Or, a
downstream component might detect a high signal-to-noise ratio and
send a message to the frontend to increase the amplification.
Additional examples include: periodic channel characterization;
adaptive beamforming; initiating a handoff (e.g., to a new
network protocol); marking the end of a large data segment; and
responding to user inputs, environmental conditions, or runtime
exceptions.

There are two common implementation strategies for control messages
using today's language and compiler tools.  First, the message can be
embedded in the high-bandwidth data stream, perhaps as an extra field
in a data structure.  Application components check for the presence of
messages on every iteration, processing any that are found.  This
scheme offers precise timing across distributed components, as the
control message has a well-defined position with respect to the data
stream.  However, it adds complexity and runtime overhead to the
steady-state data processing, and it requires a direct high-bandwidth
connection between sender and receiver.

A second implementation strategy is to perform control messaging
``out-of-band'', via a new low-bandwidth connection or a remote
procedure call.  While this avoids the complexity of embedding
messages in another data stream, it falls short in terms of timing
guarantees.  In a distributed environment, each processor has its own
clock and is making independent progress on its part of the
application.  The only common notion of time between processors is the
data stream itself.  Though extra synchronization can be imposed to
keep processors in check, such synchronization is costly and can
needlessly suppress parallelism.  Also, the presence of dynamic
messaging can invalidate other optimizations which rely on static
communication patterns.

This paper presents a new language construct and supporting compiler
analysis that allows the programmer to declaratively specify control
messages.  Termed ``teleport messaging'', this feature offers the
simplicity of a method call while maintaining the precision of
embedding messages in the data stream.  The idea is to treat control
messages as an asynchronous method call with no return value.  When
the sender calls the method, it has the semantics of embedding a
placeholder in the sender's output stream.  The method is invoked in
the receiver when the receiver would have processed the placeholder.
We generalize this concept to allow messages both upstream and
downstream, and with variable latency.  By exposing the true
communication pattern to the compiler, the message can be delivered
using whatever mechanism is appropriate for a given architecture.  The
declarative mechanism also enables the compiler to parallelize and
reorder application components so long as it delivers messages on
time.

Our formulation of teleport messaging relies on a restricted model of
computation known as Synchronous Dataflow, or SDF~\cite{LM87-i}.  As
described in Section~\ref{sec:sdf}, SDF expresses computation as a
graph of communicating components, or {\it actors}.  A critical
property of SDF is that the input and output rate of each actor is
known at compile time.  Using this property, we can compute the
dependences between actors and automatically calculate when a message
should be delivered.  We develop a stream dependence function,
$\sdep$, that provides an exact, complete, and compact representation
of this dependence information; we use $\sdep$ to specify the
semantics of teleport messaging.

Teleport messaging is implemented as part of the StreamIt compiler
infrastructure~\cite{streamitcc}.  The implementation computes $\sdep$
information and automatically targets a cluster of workstations.
Based on a case study of a frequency hopping radio, we demonstrate a
49\% performance improvement due to the scheduling benefits of
teleport messaging.  As described in Section~\ref{sec:constraints},
our implementation limits certain sender-receiver pairs to be in
distinct portions of the stream graph; if overlapping messages are
sent with conflicting latencies, it may be impossible to schedule the
delivery.  This constrained scheduling problem remains an interesting
problem for future work.

The rest of this paper is organized as follows.  In the rest of this
section, we describe our model of computation and give a concrete
example of teleport messaging.  Section~\ref{sec:sdep} defines the
stream dependence function, and Section~\ref{sec:calc-sdep} shows how
to calculate it efficiently.  Section~\ref{sec:teleport} gives the
semantics for teleport messaging, and Section~\ref{sec:casestudy}
describes our case study and implementation results.  
%Other
%applications for $\sdep$ appear in Section~\ref{sec:others-apps},
Related work appear in Section~\ref{sec:related-work}, while
conclusions and future work appear in Section~\ref{sec:conclusion}.

%% In  this  paper, we focus on the 
%% computation paradigm embodied by Synchronous  Dataflow~\cite{LM87-i},
%% a popular  model  that  is well suited for  streaming codes.  
%% In this model, computation is represented  as a structured graph consisting
%% of {\it actors} connected by communication channels.

% which describes the
% ordering constraints of actor firings in an SDF graph.  This
% dependence information is similar to a program slice, which has a rich
% body of work surrounding it~\cite{hrb88pdg,pugh97slice,tip95slice}.
% Like program slicing, $\sdep$ can also facilitate debugging and
% program analysis.  However, unlike program slicing, one can leverage
% the static properties of SDF 
% Our work  is presented  in the context  of Synchronous  Dataflow (SDF)
% which  is a popular  model of  computation~\cite{LM87-i} that  is well
% suited for  streaming codes. In  SDF, computation is represented  as a
% graph consisting of {\it  actors} connected by communication channels;
% the actors consume  and produce a constant number  of items from their
% input and output  channels every time they execute.   SDF is appealing
% because it is ameable to static scheduling and optimization.
% However, the challenge
% comes when there is a dynamic, unpredictable event in the stream; for
% instance, an actor detects a low signal-to-noise ratio and sends a
% signal to the frontend to increase the amplification.  How should the
% control message be delivered?  The problem is further complicated when
% there is a constraint on the timing of the message.  With the abundant
% parallelism in stream programs, how can concurrent actors have a
% common frame of reference with respect to time?
%% This paper makes two contributions towards this goal.  First, we
%% formulate a stream dependence function, $\sdep$, which describes the
%% ordering constraints of node executions in a concurrent stream graph.
%% This dependence information is similar to a program
%% slice~\cite{tip95slice,hrb88pdg} in procedural programs and carries
%% with it many of the applications of slicing, including debugging and
%% program analysis.  However, unlike slicing, we restrict our input
%% domain to a Synchronous Dataflow (SDF) ~\cite{LM87-i} graph, which is
%% a natural representation for many signal processing applications.  We
%% leverage the static properties of SDF to compute $\sdep$ exactly at
%% compile time and to store the dependence information compactly.

%% The second contribution of this paper is a novel language construct
%% that uses $\sdep$ to provide simple and precise event handling in
%% stream programs.  In addition to the high-bandwidth data flow of
%% streaming applications, there are also low-bandwidth control messages
%% that adjust parameters in the stream graph; for example, toggling the
%% gain to suite the signal-to-noise ratio, or steering a microphone
%% array to follow a moving target.  Control messages are dynamic and
%% irregular, which makes them hard to integrate with a Synchronous
%% Dataflow model.  Most of all, they often have a constraint on the
%% timing of their delivery, which further complicates the programming
%% model because concurrent actors have no common frame of reference with
%% respect to time.
				   
%% We define a messaging construct that utilizes data dependences 

\subsection{Model of Computation}
\label{sec:sdf}

Our model of computation is Cyclo-Static Dataflow~\cite{BELP96}, a
generalization of Synchronous Dataflow~\cite{LM87-ii}.  This is a
popular model of computation that is well suited for signal processing
applications. Computation is represented as a graph of {\it actors}
connected by FIFO communication channels.  Each actor follows a set of
execution steps, or phases, that consume a constant number of items
from the input channels and produce a constant number of items onto
the output channels.  The number and ordering of phases is known at
compile time, and their execution is cyclic (that is, after executing
the last phase, the first phase is executed again).  SDF is appealing
because every actor has a fixed input and output rate, making the
stream graphs amenable to static scheduling and
optimization~\cite{LM87-i}.

In this paper, we use the StreamIt programming
language~\cite{streamitcc} to describe the connectivity of the
dataflow graph as well as the internal functions of each actor.  Our
technique is general and should apply equally well to other languages
and systems based on Synchronous or Cyclo-Static Dataflow.  In
StreamIt, each actor (called a {\it filter} in the language) has one
input channel and one output channel.  An execution step consists of a
call to the ``work function'', which contains general-purpose code.
During each invocation, an actor consumes ({\it pops}) a fixed number
of items from the input channel and produces ({\it pushes}) a fixed
number of items on the output channel.  It can also {\it peek} at
input items without consuming them from the channel.

Actors are assembled into single-input, single-output stream graphs
(or {\it streams}) using three hierarchical primitives.  A {\it
pipeline} arranges a set of streams in sequence, with the output of
one stream connected to the input of the next.  A {\it splitjoin}
arranges streams in parallel; incoming data can either be duplicated
to all streams, or distributed using a round-robin splitter.
Likewise, outputs of the parallel streams are serialized using a
round-robin joiner.  Round-robin splitters (resp. joiners) execute in
multiple phases: the $i$th phase pushes (resp. pops) a known number of
items $k_i$ to the $i$th stream in the splitjoin.  Finally, a {\it
feedbackloop} can be used to introduce cycles in the graph.

\subsection{Illustrating Example}

Figure~\ref{fig:fir-orig-code} illustrates a StreamIt version of an
FIR (Finite Impulse Response) filter.  A common component of digital
signal processing applications, FIR filters represent sliding window
computations in which a set of coefficients is convolved with the
input data.  This FIR implementation is
%
\input{fir-example}
\noindent 
%
very fine-grained; as depicted in Figure~\ref{fig:fir-orig-diagram},
the stream graph consists of a single pipeline with a {\tt Source}, a
{\tt Printer}, and 64 {\tt Multiply} stages---each of which contains a
single coefficient (or {\it weight}) of the FIR filter.  Each {\tt
Multiply} actor inputs a {\tt Packet} consisting of an input item and
a partial sum; the actor increments the sum by the product of a
weight and the {\it previous} input to the actor.  Delaying the
inputs by one step ensures that each actor adds a different input to
the sum.  While we typically advocate a more coarse-grained
implementation of FIR filters, this formulation is simple to
parallelize (each actor is mapped to a separate processor) and
provides a simple illustratoin of our analysis.

The problem addressed by this paper is as follows.  Suppose that the
actors in the FIR program are running in parallel and the {\tt Source}
detects that the weights should be adjusted (e.g., to suite the
current operating conditions).  Further, to guarantee stability, every
output from the system must be obtained using either the old
weights or the new ones, but not a mixture of the two.  This
constraint precludes updating all of the weights at the same
instant, as the partial sums within the pipeline would retain evidence
of the old weights.  Rather, the weights must be changed one
actor at a time, mirroring the flow of data through the pipeline.
What is a simple and efficient way to implement this behavior?

One way to implement this functionality is by manually tagging each
data item with a flag, indicating whether or not it marks the
transition to a new set of weights.  If it does, then the new set of
weights is included with the item itself.  While this strategy (shown
in Figures~\ref{fig:fir-manual-code} and~\ref{fig:fir-manual-diagram})
is functional, it complicates the {\tt Packet} structure with two
additional fields---a {\tt newWeights} flag and a {\tt weights}
array---the latter of which is meaningful only when {\tt newWeights}
is true.  This scheme muddles steady-state dataflow with event
handling by checking the flag on every invocation of {\tt Multiply}
(line 41 of Figure~\ref{fig:fir-manual-code}).  It is also very
inefficient in StreamIt because arrays are passed by value; a new
optimization would be needed to compress each {\tt Packet} structure
when the {\tt weights} field is unused.

This paper proposes an alternate solution: teleport messaging.  The
idea behind teleport messaging is for the {\tt Source} to change the
weights via an asynchronous method call, where method invocations in
the target actors are timed relative to the flow of data in the
stream.  As shown in Figure~\ref{fig:fir-message-code}, the {\tt
Multiply} actor declares a message handler that adjusts its own weight
(lines 40-42).  The {\tt Source} actor calls this handler through a
{\it portal} (line 25), which provides a clean interface for messaging
(see Section~\ref{sec:teleport}).  As depicted in
Figure~\ref{fig:fir-message-diagram}, teleport messaging gives the
same result as the manual version, but without corrupting the data
structures or control flow used in the steady-state.  It also exposes
the true information flow to the compiler, allowing the compiler to
deliver the message in the most efficient way for a given
architecture.

%% The rest of this paper is devoted to making the above notions more
%% general and more precise.  In particular, it is natural to use
%% teleport messaging to send messages upstream---against the flow of
%% data---which is hard to achieve manually.  We also incorporate a
%% natural concept of latency in message delivery.  But first, to make
%% the timing guarantees precise, we formulate a stream dependence
%% analysis that can be used to describe the timing of messages.  This
%% formulation extends teleport messaging to any pair of actors that are
%% connected by a path in the stream graph.
