\subsection{Experimental Evaluation}
\label{sec:evaluation}

We use a cluster of workstations as our evaluation testbed. Our
StreamIt compiler automatically partitions an input program into a set
of threads that are mapped to the workstations and then run concurrently.
We chose a cluster-based evaluation for two reasons. 
First, a number of streaming applications run on the server side
(e.g., cell phone base stations, radar processing, HDTV editing) and
require large computational resources. Second, clusters provide a
simple abstraction for distributed and parallel computing---multiple program
counters, and distributed memories---which 
is at the heart of emerging multi-core and tile-based architectures
for embedded, desktop, and server computing.
In our experiments, we are primarily interested in quantifying two
performance metrics: throughput and communication overhead.
For the purpose of this paper, throughput is defined
as the number of outputs produced per unit of time, and communication
overhead reflects the quantity of data transmitted between
workstations.


The teleport messaging-based implementation of the frequency hopping
radio was compiled into 28 threads
whereas the alternate version---relying on a feedback-loop---was
compiled into 32 threads.  Each thread implements a
specific subset of the stream graph  and is assigned to one of
sixteen 750Mhz Pentium~III workstations, each with a
256~Kb cache.  The machines are interconnected using a fully switched
high speed network, and the threads intercommunicate via
dedicated  TCP/IP connections. 

Our compiler maps threads to workstations using an algorithm that aims
to reduce the overall application bottleneck while
maximizing the throughput of the output filter (i.e., most downstream
actor) in the stream graph.

\begin{figure}[t]
\psfig{figure=throughput-graph.eps,width=3.3in}
\caption{\small Throughput as a function of the number of workstations
in the cluster. 
\protect\label{fig:fhr-throughput}}
\end{figure}

In Figure~\ref{fig:fhr-throughput}, we report the measured throughput 
($y$-axis) for various cluster sizes ranging from one to sixteen
interconnected workstations. Note that due to the limited parallelism in the
two implementations of the frequency hopper, cluster configurations
with more than five workstations lead to negligible gains in
terms of throughput. From the data, we can observe that teleport
messaging achieves a maximal throughput that is 49.8\% better than its
feedback-loop counterpart. Furthermore, a detailed analysis of the
results shows that teleport messaging reduces the communication
overhead by 35\%.

The current teleport messaging implementations assumes that actors
can only receive messages from a unique sender.
In our implementation, each actor maintains an iteration number which
serves to synchronize message delivery. A message from a sender $A$ to
a receiver $B$ is tagged with the iteration number in $B$ when the
message must be processed. The iteration number is calculated by the
sender using the $\sdep$ and $\sdep^{-1}$ relations, and the specified
message latency.

Furthermore, actors that receive messages with negative latencies from
an upstream sender, or positive latency messages from a downstream sender, 
can only execute as many iterations as allowed by a
{\it credit} received from the sender; the compiler automatically
schedules the exchange  of credits between actors when it derives an
execution of the stream graph. Our communication layer guarantees that
messages are received before credits and hence processed during the
appropriate execution step.
For messages sent downstream with zero or positive latencies, we
do not use the credit system, and assume that the messages arrive as fast or
faster than the data items exchanged between the two actors. This
assumption is reasonable since a downstream filter can not get 
ahead of an upstream actor.

                                                                                                     
