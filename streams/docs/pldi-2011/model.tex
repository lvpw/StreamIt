\section{Model of Computation}

The semantics of the model of computation employed in this
paper are built upon synchronous dataflow (SDF)~\cite{leeSDF}.
Computation is described by composing independent processing nodes
into networks.  The processing nodes, termed {\it actors} or {\it
  filters}, communicate via uni-directional FIFO data channels.  Each
filter defines an atomic execution step that is performed repeatedly.
The term {\it synchronous} denotes that a filter will not fire unless
all of its input are available.  Synchronous dataflow requires that
the number of items produced and consumed by a filter during each
atomic execution step can be determined statically.  This allows the
compiler to perform static scheduling of and transformations on the
stream graph.

The model we use for this paper is a general model that is agnostic
of input language.  Although the model was inspired by the StreamIt
programming language, the model can represent aspects of other
streaming programming languages such as Brook~\cite{brook04},
StreamC/KernelC~\cite{imagine03ieee}, and SPUR~\cite{spur05samos}.
Consider a directed graph $G = (V, E)$ corresponding to a streaming
application. $F \in V$ is a filter in the application and $(f, g) \in
E$ is an edge in the graph that denotes communication from $f$ to $g$
using a FIFO channel.  Edges are also termed {\it channels}. Note that
there can exist multiple edges between 2
filters. Figure~\ref{fig:streamgraph} gives an example stream graph
for a FM radio with an equalizer.  The figure also includes details
for two of the filters; the details are explained below.

The filter is the basic unit of execution in our model.  Each filter
is described by multiple rate declarations, data reorganization
patterns, and compute functions.  Basically, each filter defines the
number of items it consumers and produces each time it is executed.
The execution of a stream graph is represented by a {\it schedule} of
a graph $G$.  A schedule gives a multiplicity for each filter $F \in
V$ that denotes how many times to fire filter $F$.  When a schedule is
executed, each filter fires when it has buffered enough input to
satisfy its input requirement (a filter will not fire more times than
given by a schedule).  In our notation, a schedule is represented by
the variable $\Sigma$, and it denotes a mapping from filters to
non-negative integers. The multiplicity of filter $F$ in schedule
$\Sigma$ is denoted by $M(\Sigma, F)$.

In the streaming domain, input items continuously enter the
application, with application output continuously produced as items
flow from producer filters to consumer filters.  To model this
behavior statically, a schedule can be calculated such that all
filters fire in the schedule, and the schedule can be repeated
indefinitely.  Since our model of execution adheres to SDF, a {\it
  steady-state} schedule, $S$, of a graph can be statically calculated
such that the quantity of items on each of the filter's input buffer
and output buffer remains unchanged by the complete execution of the
schedule~\cite{lee87}.  Thus this schedule can be repeated
indefinitely because buffers to not grow or shrink in size.
Steady-state execution of the graph entails repeating the steady-state
schedule for as much input as is expected.  Due to this, execution of
the stream graph is conceptually wrapped in an outer loop that
continuously executes the steady-state schedule.
Figure~\ref{fig:pipeline-example}(a) gives an example of a
steady-state schedule calculated for a graph that consists of three
filters in a pipeline.

Our model does not deal with arbitrary schedules of filter executions.
The steady-state schedule, $S$, is explicitly represented.
Furthermore, an {\it initialization} schedule, $I$, is explicitly
represented that enables the steady-state schedule in the presence of
peeking.  Thus, in our model $\Sigma \in \{I, S\}$.  More details on
the initialization schedule are given below.

For each filter, $F \in V$, a {\it work} function, $W_F$, is defined.
The work function defines the atomic execution step for each filter.
When a filter fires, its work function is called once, consuming the
items and producing the items statically defined.  For each filter, $F
\in V$, a {\it prework} function, $W_F^P$, can also be defined.
Prework describes a computation step that executes once at the first
firing of a filter.  The prework function is executed only once per
execution of the {\it application}; it describes any special
initialization behavior required for a filter.  Many filters do not
require a prework function that differs from the work function.  If a
filter does not define a prework function, the work function is called
on the first execution of the filter.  We denote a function with the variable
$\mathcal{F} \in \{W^p, W\}$.   We sometimes leave out
the subscript that denotes the filter if it is clear which work or
prework function is intended.  

For each filter $F \in V$ we define the following:
\begin{itemize}

\item $o(\mathcal{F}, F)$, the number of items dequeued from $F$'s
input buffer per firing of $\mathcal{F}$.  This quantity is termed the
{\it pop} rate.

\item $e(\mathcal{F}, F)$, 1 $+$ the greatest index that is read (but
  not necessarily dequeued) from $F$'s input buffer per firing of
  $\mathcal{F}$.  This quantity is termed the {\it peek} rate.

\item $u(\mathcal{F}, F)$, the number of items enqueued to $F$'s
output buffer per firing of $\mathcal{F}$.  This quantity is termed
the {\it push} rate.

\end{itemize}

\noindent If $F$ does not define a prework function, $e(W^P_F, F) = o(W^P_F, F) =
u(W^P_F, F) = 0$.  A filter with peek rate greater than pop rate is
termed a {\it peeking} filter.  Figure~\ref{fig:pipeline-example}(b)
demonstrates a peeking filter.  The figures shows that the window of
items read for consecutive firings of the filter overlap.  For the
work function execution, a peeking filter $F$ requires $e(W_F, F)$
items to be on its input channel(s) to fire.  $F$ will consume
(dequeue) the first $o(W_F, F)$ items after the firing of the work function.

An initialization schedule is required if peeking is present in a
graph to enable the calculation and execution of a steady-state
schedule~\cite{karczma-thesis}.  After the initialization schedule
executes, each filter $F$ is guaranteed to have at least $e(W_F, F) -
o(W_F, F)$ items in its input buffer. The initialization schedule is
required to calculate a steady-state schedule for a graph with an $F$
such that $e(W_F, F) - o(W_F, F) > 0$ (if the prework function peeks
then an initialization schedule is also
required)~\cite{karczmarek-lctes03}.
Figure~\ref{fig:pipeline-example}(c) gives a legal steady-state
schedule for the pipeline that enables the peeking in filter
$\mt{Filter}_2$.  During application execution, the initialization schedule
is executed once followed by an infinite repetition of the
steady-state schedule.  Figure~\ref{fig:timeline} shows how the
execution of the initialization and steady-state schedules unfold for
the pipeline example of Figure~\ref{fig:pipeline-example}.  The number
of items remaining on $F$'s input channel(s) after execution of the
initialization schedule is represented by $C(F)$.  In the case of
Figure~\ref{fig:timeline}, $C(\mt{Filter}_2) = 2$ and
$C(\mt{Filter}_1) = C(\mt{Filter}_3) = 0$.


A filter may have multiple incoming edges and/or multiple outgoing
edges.  Let \textsc{Out}$(F)$ represent the set of output edges of $F$, and 
let \textsc{In}$(F)$ represent the set of input edges of $F$.
Filter inputs are organized into a single internal FIFO buffer for the
filter to read according to an {\it input distribution pattern}, and
filter outputs are distributed from a single internal output FIFO
buffer according to an {\it output distribution pattern}.   
The input distribution pattern is represented as:

\[ \mt{ID}(\Sigma, F) \in (\mathbb{N} \times E)^n = ((w_1,e_1), (w_2,
e_2), ..., (w_n, e_n))\]

\noindent Where $n$ is the width of the input distribution pattern.
The input distribution describes the round robin joining pattern for
organizing the input data into the filter's single internal FIFO
buffer, where $w_i$ items are received from edge $e_i$ before
proceeded to the next edge, $e_{i+i}$.

The output distribution pattern describes both round-robin
splitting and duplication in a single structure:

\[ \mt{OD}(\Sigma, F) \in (\mathbb{N} \times (P(E)-\emptyset))^m = ((w_1,d_1), (w_2,
d_2), ..., (w_n, d_n))\]

\noindent Each $d_i$ is called the {\it dupset} of weight $i$.  The
dupset $d_i$ specifies that $w_i$ items be duplicated to the edges
of the dupset.  Each tuple denotes that $w_i$ output items of $F$ are
duplicated to the edges of $d_i$ before moving on to the next tuple.

For both the $\mt{OD}$ and the $\mt{ID}$, the variable
$\Sigma$ specifies the schedule which the distribution pattern
describes, either $I$ for initialization, or $S$ for steady-state.
The input and output distributions are repeated as needed for the
schedule that is being executed.  The start of each steady-state
iteration resets the input and output distributions to the first tuple.
Figure~\ref{fig:dist-example} highlights an example of a filter with
both multiple inputs and multiple outputs, and how the distribution
patterns translate into scattering and gathering.

Let $\mt{RO}(F_1, F_2, \Sigma)$ be the ratio of output items $F_1$
splits along the edge $(F_1, F_2)$ to the total number of items that
$F_1$ produces in the schedule $\Sigma$.  This can be calculated from
$F_1$'s output distribution pattern for $\Sigma$.  For example, in
Figure~\ref{fig:dist-example}, $\mt{RO}(F_3, F_4) = 1$ and
$\mt{RI}(F_3, F_5) = \frac{1}{3}$.  Conversely, let $\mt{RI}(F_1, F_2,
\Sigma)$ be the percentage of total input items that $F_2$ receives
from $F_1$ for $\Sigma$.  For example, in
Figure~\ref{fig:dist-example}, $\mt{RI}(F_2,F_3) = \frac{2}{3}$.

Let $s(\mathcal{F}, F)$ denote the execution time (in cycles, with
respect to a given real or conceptual machine) of function
$\mathcal{F}$ of filter $F$.  For many of the static scheduling
techniques covered in this thesis, we calculate a static estimation of
the total amount of work for a filter $F$ in the steady-state, $s(W,
F) \cdot M(S, F)$.  We often use the term {\it work estimation} to
denote this quantity.

We define one final property of a filter.  For filter $F$, given that
there is sufficient input data and output data will be ordered
properly, if it is required to execute firing $i$ of $F$ before firing
$i+1$, then $F$ is termed {\it stateful}.  If firing $i$ of $F$ can be
executed before (or in parallel with) $i+1$, then $F$ is termed a {\it
  stateless} filters.  This section does not define how the
computation of the work and prework functions is specified.  In the
next section we will see that for StreamIt, a work function is
imperative code that can include (field) variables live across firings
of the filter.  In this case, a filter $F$ is stateless if $F$ does
not write to a field variable that is read during a subsequent firing.
This will create a loop-carried dependence and prevent
data-parallelization.

In general, if all of the quantities defined above can be statically
determined for graph $F \in G$, then we say $G$ is {\it static}.
Static graphs match closely the original formulation of synchronous
dataflow (with the exception of peeking and prework).  The scheduling,
partitioning, and mapping techniques presented in this thesis are
enabled by and operate on static graphs.
