We have already alluded to execution scaling in previous
sections. As the instruction cache model shows, increasing the number
of consecutive firings of the same actor leads to lower instruction
cache miss rates. However, scaling increases the data buffers that are
maintained between actors. Thus it is prudent that we account for the
data working set requirements as we scale a steady state.

\begin{figure}[t]
\begin{center}
\framebox{\parbox{3.25in}{
\mbox{} ~~~~// {\it Returns a scaling factor for steady state $S$} \\
\mbox{} ~~~~// - $c$ is the data cache size\\
\mbox{} ~~~~// - $\alpha$ is the fraction of $c$ dedicated for I/O\\%$(0 < \alpha)$\\
\mbox{} ~~~~// - $p$ is the desired percentile of all actors to be\\
\mbox{} ~~~~// satisfied by the chosen scaling factor $(0 < p \le 1$)\\
\mbox{} ~~~~{\bf calculateScalingFactor}($S$, $c$, $\alpha$, $p$) \{\\
\mbox{} ~~~~~~~{\bf create} array $M$ of size $|S|$ and {\bf initialize} all entries to 0\\
\mbox{} ~~~~~~~{\bf for} $i$ = 1 to $|S|$ \{ \\
\mbox{} ~~~~~~~~~~$w = S[i]$\\
\mbox{} ~~~~~~~~~~// calculate effective cache size\\
\mbox{} ~~~~~~~~~~$c = \alpha \times (c - \mt{State}(w))$\\
%\mbox{} ~~~~~~~~~~// estimate I/O requirements\\
%\mbox{} ~~~~~~~~~~$d = \mt{IO}(X)$\\
\mbox{} ~~~~~~~~~~// account for the I/O requirements and\\
\mbox{} ~~~~~~~~~~// calculate largest possible scaling for $w$\\
\mbox{} ~~~~~~~~~~$M[i] = {\bf round}(c~/~\mt{IO}(w, 1))$\\
\mbox{} ~~~~~~~\} \\
\mbox{} ~~~~~~~{\bf sort} $M$ into ascending numerical order\\
\mbox{} ~~~~~~~$i = \lfloor~(1-p) \times |S|~\rfloor$ \\
\mbox{} ~~~~~~~return $M[i]$\\
\mbox{} ~~~~\}
}}
\end{center}
\nocaptionrule
\caption{Algorithm for calculating the scaling factor.}
\label{fig:scaling-algo}
\end{figure}

Our approach is to scale the entire steady state by a single
scaling factor, with the constraint that only a small percentage
of the actors overflow the data cache. Our two-staged algorithm is
outlined in Figure~\ref{fig:scaling-algo}.

First, the algorithm calculates the largest possible scaling factor
for every actor in the steady state. To do this, it calculates the
amount of data produced by each actor firing and divides the available
data cache size by this data production rate. In addition, the
algorithm can toggle the effective cache size to account for various
eviction policies.

Second, it chooses the largest
factor that allows a fraction ($p$) of the steady state actors to be
scaled safely (i.e., the cache is adequate for their I/O
requirements).  For example, the algorithm might calculate
$M_\texttt{A} = 10$,
$M_\texttt{B} = 20$, 
$M_\texttt{C} = 30$, and 
$M_\texttt{D} = 40$, for four actors in some steady state. Thus,
scaling actor \texttt{A} beyond 10 consecutive iterations will cause
its dynamic I/O requirements to exceed the data cache. Therefore, the
largest $M$ that allows $p=90\%$ of the actors to be
scaled without violating the cache constraints is 10.
Similarly, to allow for the safe scaling of $p=75\%$ of the actors, the
largest factor we can choose is 20.

In our implementation, we use the 90-10 heuristic. In other words, we
set $p=90\%$. We empirically determined this value via a series of
experiments using our benchmark suite. Specifically, we varied the
scaling factor across a wide range of values,
and measured the resulting performance. We found that
increasing the threshold beyond 90\% leads to marginal
performance gains. Due to the limited space we cannot include all of
the collected data. However, the interested reader can find
the results in a thesis by one of the authors~\cite{janis-thesis}.


Note that our algorithm adjusts the effective cache size that is
reserved for an actor's 
dynamic working set (i.e., data accessed via {\tt pop} and {\tt
push}). This adjustment allows us to control the fraction of the cache
that is used for reading and writing data---and affords some
flexibility in targeting various cache organizations.  For example,
architectures with highly associative and multilevel caches may benefit
from scaling up the effective cache size (i.e., $\alpha > 1$), whereas
a direct mapped cache that is more prone to conflicts may benefit from
scaling down the cache (i.e., $\alpha < 1$). In our implementation, we
found $\alpha=2/3$ to work well. However, we note that the optimal
choice for the effective cache size is a complex function of the 
underlying cache organization and possibly the application as well; 
this is an interesting issue that warrants further investigation.
