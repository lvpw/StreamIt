\subsection{Data Cache}

The results in Figure~\ref{fig:scaling-data} show that scaling can
reduce the running time of a program, but ultimately, it degrades
performance. In this section, we provide a basic analytical model that
helps in reasoning about the relationship between scaling and the data
cache miss rate. 

We distinguish between two types of data working sets. The static data
working set of an actor represents state, e.g., \texttt{weights} in
the FIR example (Figure~\ref{fig:fir}).  The dynamic data
working set is the data generated by the work function and pushed onto
the output channel. Both of these working sets impact the data cache
behavior of an actor.

Intuitively, the presence of state suggests that it is
prudent to maximize that working set's temporal locality. In this
case, scaling positively improves the data cache performance. To see
that this is true, we can define a data miss rate ($\mt{DMR}$) based on
a derivation similar to that for the instruction miss rate, replacing
$C_I$ with $C_D$ in Equation~\ref{eq:imiss}, and $I(a)$ with
$\mt{State}(a)$ when calculating the reuse distance. Here, $C_D$
represents a constant proportional to the data cache size, and
$\mt{State}(a)$ represents the total size of the static data in the
specified actor. 

Execution scaling however also increases the I/O requirements of a
scaled actor. Let $\mt{pop}$ and $\mt{push}$ denote the declared pop and push rates
of an actor, respectively.  The scaling of an
actor by a factor $m$ therefore increases the pop rate to $m\times o$
and the push rate to $m\times u$. Combined, we represent the dynamic
data working set of an actor $a$ as $\mt{IO}(a, m) =
m\times(o+u)$. Therefore, we measure the data reuse distance ($\mt{DRD}$)
of an execution $S$ with scaling factor $m$ follows:
\[
  \mt{DRD}(S^m,i)=\sum_{a} \mt{State}(a) + \mt{IO}(a,m)
\]
over all distinct actors $a$ occurring in $\mt{phase}(S^m,i)$.  While
this simple measure double-counts data that is both produced and
consumed within a phase, such duplication could be roughly accounted
for by setting $IO'(a,m) = IO(a,m) / 2$.

We can determine if a
specific work function will result in a data cache miss (on its next
firing) by evaluating the following step function:
\begin{equation}
\label{eq:dmiss}
  \mt{DMISS}(S^m,i) =
    \begin{cases}
      0& \text{if $\mt{DRD}(S^m,i) \leq C_D$; hit: no cache refill,}\\
      1& \text{otherwise; miss: (some) cache refill.}
    \end{cases}
\end{equation}
Finally, to model the data miss rate ($\mt{DMR}$):
\begin{eqnarray}
  \label{eq:dmrM}
  \mt{DMR}(S^m) &=& \frac{1}{|S^m|}\sum_{i=1}^{|S^m|} \mt{DMISS}(S^m,i).
\end{eqnarray}

It is evident from Equation~\ref{eq:dmrM} that scaling can lead to
lower data miss rates, as the coefficient $1/|S^m| = 1/(m \times |S|)$
is inversely proportional to $m$.  However, as the scaling factor $m$
grows larger, more of the $\mt{DMISS}$ values transition from $0$ to
$1$ (they increase monotonically with the I/O rate, which is
proportional to $m$).  For sufficiently large $m$, $\mt{DMR}(S^m) =
1$.  Thus, scaling must be performed in moderation to avoid negatively
impacting the data locality.

Note that in order to generalize the data miss rate equation so that it properly
accounts for the dynamic working set, we must consider the amount of
data reuse within a phase. This is because any actor that fires within
$\mt{phase(S,i)}$ might consume some or all of the data
generated by $S[i]$. The current model is simplistic, and leads to
exaggerated I/O requirements for a phase. We also do not model the
effects of cache conflicts, and take an ``atomic'' view of cache
misses (i.e., either the entire working set hits or misses).

%These observations suggest that in order to maximize the locality of
%any static data in an actor, the compiler should not only favor short
%phases, but also, it should schedule actor executions so that
%producer-consumer pairs appear within the same phase. Our cache-aware
%optimizations, described in the next section, exploits these
%observations.
