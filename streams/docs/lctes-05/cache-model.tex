\section{Cache Model for Streaming}
\label{sec:cache-model}

From a caching point of view, it is intuitively clear that once a
actor's instruction working set is fetched into the cache, we can
maximize instruction locality by running the actor as many times as
possible.  This of course assumes that the total code size for
all actors in the steady state exceeds the capacity of the
instruction cache.
%% In
%% Figure~\ref{fig:ssi-single} we show a representative breakdown of the
%% code size per actor in a steady state execution of a StreamIt
%% implementation of a Fast Fourier Transform (FFT).
For our benchmarks, the total code size for a steady state
ranges from 2~Kb to over 135~Kb (and commonly exceeds 16~Kb). Thus, while individual actors may have a
small instruction footprint, the total footprint of the actors in a
steady state exceeds a typical instruction cache size.
From these observations, it is evident that we must {\it scale} the
execution of actors in the steady state in order to improve temporal
locality. In other words, rather than running a actor $n$ times per
steady state, we scale it to run $m \times n$ times.
%
%% (e.g., the loop bound for \verb+A_work+ is
%% changed to $\texttt{m} \times 4$ in the example shown earlier). 
We term $m$ the {\it scaling factor}.

The obvious question is: to what extent can we scale the execution of
actors in the steady state? The answer is non-trivial because
scaling, while beneficial to the instruction cache behavior, may
overburden the data cache as the buffers between actors may grow to
prohibitively large sizes that degrade the data cache
behavior. Specifically, if a buffer overflows the cache, then
producer-consumer locality is lost.

In this section we describe a simple and intuitive cache model to
estimate the instruction and data cache miss rates for a steady state
sequence of actor firings. The model serves as a foundation for
reasoning about the cache aware optimizations introduced in this
paper. We develop the model first for the instruction cache, and then
generalize it to account for the data cache.

\input{cache-model-instruction}
\input{cache-model-data}

%% Also complicating matters is the amount of state a actor must retain
%% from one execution of its work function to the next. In the FIR
%% example shown earlier, the state is proportional to the size of
%% the coefficient array (i.e., \texttt{weights}). Thus any scaling of
%% actor firings is further constrainted by the state.


%% We do not account for the cold start
%% effects of an execution trace. This does not affect our model, and can
%% be easily remedied if necessary. To see why this is so, consider the
%% following two execution traces which represent the executions of work
%% functions for two actors \texttt{A} and \texttt{B}:
%% $T_1 = \texttt{A}_1\texttt{A}_2\texttt{B}_3\texttt{B}_4$
%% and 
%% $T_2 = \texttt{A}_1\texttt{B}_2\texttt{A}_3\texttt{B}_4$.
%% In both 
%% traces, the number of cold starts is the same (i.e., two in all).


%% Note that different execution orderings for
%% the same stream graph lead to traces of the same size and hence the
%% comparisons are fair. 

%% Our model predicts that scaling the multipicitly of the actors always
%% leads to lower miss rates. An actor is fired a greater number of times in 
%% the steady state before transitioning to the next actor in the
%% schedule. Specifically, scaling a steady state $S$ by a multiplicity
%% factor $m$ 
%% Hence for example, a program that would generate a trace
%% $T_2$
%% might be as follows:
%% {\small
%% \begin{verbatim}
%% run_steady_state() {
%%   for (i = 0; i < 1; i++) A_work();
%%   for (i = 0; i < 1; i++) B_work();
%% }
%% \end{verbatim}}
%% \noindent but a scaled program that increases temporal locality is as follows:
%% {\small
%% \begin{verbatim}
%% run_steady_state() {
%%   for (i = 0; i < m * 1; i++) A_work();
%%   for (i = 0; i < m * 1; i++) B_work();
%% }
%% \end{verbatim}}
%% \noindent where \texttt{m} (the multiplicity factor) is an integer greater than
%% one.
%% From our model, it is easy to see that scaling reduces the number
%% of misses for a given actor $f$ from $K$ to $\frac{K}{\texttt{m}}- 1$, where $K =
%% \sum \mt{IMS}(f)$ for all occurrences of $f$ in  the execution trace.

%% \subsection{Remarks}

%% Note  that we can  combine the  instruction and  data cache  miss rate
%% equations to yield one measure that estimates the temporal locality of
%% a streaming computation.

%% Also note that while we use  an execution trace to describe our model,
%% it  is possible  to  calculate  the instruction  and  data cache  miss
%% metrics at  compile time.  To do so,  we can leverage  the hierarchical
%% StreamIt   representation  that  dictates   the  ordering   of  actor
%% executions.


%% Intuitively, the dynamic data accounts for the size of the buffer
%% required for writing new values, as well as the size of the buffer
%% necessary for reading values. It also adjusts for any
%% producer-consumer locality since the output buffer of one actor is
%% also the input buffer of its neighbor in the stream graph. What this
%% measure tells us is that we can scale the execution of a actor as
%% long as its buffer requirements (for reading and writing) do not
%% exceed the cache, and furthermore, as long as the input-output rates
%% between producer consumer pairs are not grossly mismatched. Managing the buffer 
%% requirements is important and motivates a series of cache
%% optimizations discussed in the next section. In the case of rate
%% mismatch, the metric tells us that the effective cache size is
%% reduced, or in other words, the data that is left over after a
%% producer-consumer firing must be preserved until a future occurrence of
%% the same pair of actors. This translates to lower data locality and
%% degrades cache performance.

%% Mathematically, the dynamic data measure is defined as:
%% \begin{eqnarray}
%%   \nonumber
%%   Y(f_i) &=&\sum_{f_k} min(W(f_k), U(f_k) \times A(f_k)) + \\
%%   \nonumber
%% 	   &&\sum_{f_k} min(R(f_k), O(f_k) \times A(f_k)) - \\
%%   \nonumber
%%          &&\sum_{f_s} min(R(f_s), O(f_s) \times A(f_s))
%% \end{eqnarray}
%% with $W(f)$ equal to the output buffer size reserved for writing data, $R(f)$
%% equal to the input buffer size reserved for reading data, $U(f)$ equal to the
%% {\tt push} rate (in bytes) of the actor, $O(f)$ equal to the {\tt pop} rate (in
%% bytes) of the actor, and $A(f)$ equal to the number of occurrences of
%% actor $f$ in $phase(f_i)$. Also note that the $Y(f_i)$ is defined
%% over all distinct occurrences of $f_k$ in the phase, and $f_s$
%% represents the actor that consumes the data produced by $f_k$ (i.e.,
%% it is $f_k$'s successor in the stream graph, and $f_k$--$f_s$
%% constitute a producer-consumer pair; $f_s$ is unique unless $f_k$ is a splitter).

%% The first summand in the equation above quantifies the address space
%% accessed for writing data. It is  equal to the lesser of the buffer size
%% reserved for output, and the total number of items produced by
%% the actor (i.e., the push rate multiplied by the number of times the
%% work function fired in the phase); this avoids over estimating the
%% address space when an exceedingly large buffer is reserved but only a
%% portion of it is used for writing in a phase.

%% The second term in the equation quantifies the referenced address space 
%% for reading the input data. This term is also the lesser of two
%% values: the input buffer size, and the total number of bytes referenced for
%% reading data.

%% The third term avoids double counting since the output buffer of one
%% actor is also the input buffer of its successor in the stream
%% graph. The term quantifies the amount of data that is consumed by the
%% producer's successor (which therefore releases a portion of the
%% address space for use by other actors in the phase).
