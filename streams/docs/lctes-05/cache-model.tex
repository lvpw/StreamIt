\section{Cache Model for Streaming}
\label{sec:cache-model}

From a caching point of view, it is intuitively clear that once a
filter's instruction working set is fetched into the cache, we can
maximize instruction locality by running the filter as many times as
possible.  This of course assumes that the total code size for
all filters in the steady state exceeds the capacity of the
instruction cache (which is commonly the case).
%% In
%% Figure~\ref{fig:ssi-single} we show a representative breakdown of the
%% code size per filter in a steady state execution of a StreamIt
%% implementation of a Fast Fourier Transform (FFT).
For our benchmarks, the total code size for a steady state
ranges from 2~Kb to over 135~Kb. Thus, while individual filters may have a
small instruction footprint, the total footprint of the filters in a
steady state exceeds a typical instruction cache size.
From these observations, it is evident that we must {\it scale} the
execution of filters in the steady state in order to improve temporal
locality. In other words, rather than running a filter $n$ times per
steady state, we increase its {\it multiplicity} to $M \times n$ 
times.
%% (e.g., the loop bound for \verb+A_work+ is
%% changed to $\texttt{M} \times 4$ in the example shown earlier). 
We term $M$ the {\it multiplicity factor}.

The obvious question is: to what extent can we scale the execution of
filters in the steady state? The answer is non-trivial because
scaling, while beneficial to the instruction cache behavior, may
overburden the data cache as the buffers between actors may grow to
prohibitively large sizes that degrade the data cache
behavior. Specifically, if a buffer overflows the cache, then
producer-consumer locality is lost.

In this section we describe a simple and intuitive cache model to
estimate the instruction and data cache miss rates for a steady state
sequence of actor firings. The model serves as a foundation for
reasoning about the cache-aware optimizations introduced in this
paper. We develop the model first for the instruction cache, and then
generalize it to account for the data cache.

\input{cache-model-instruction}
\input{cache-model-data}

%% Also complicating matters is the amount of state a filter must retain
%% from one execution of its work function to the next. In the FIR
%% example shown earlier, the state is proportional to the size of
%% the coefficient array (i.e., \texttt{weights}). Thus any scaling of
%% actor firings is further constrainted by the state.

%% We do not account for the cold start
%% effects of an execution trace. This does not affect our model, and can
%% be easily remedied if necessary. To see why this is so, consider the
%% following two execution traces which represent the executions of work
%% functions for two filters \texttt{A} and \texttt{B}:
%% $T_1 = \texttt{A}_1\texttt{A}_2\texttt{B}_3\texttt{B}_4$
%% and 
%% $T_2 = \texttt{A}_1\texttt{B}_2\texttt{A}_3\texttt{B}_4$.
%% In both 
%% traces, the number of cold starts is the same (i.e., two in all).


%% Note that different execution orderings for
%% the same stream graph lead to traces of the same size and hence the
%% comparisons are fair. 

%% Our model predicts that scaling the multipicitly of the actors always
%% leads to lower miss rates. An actor is fired a greater number of times in 
%% the steady state before transitioning to the next actor in the
%% schedule. Specifically, scaling a steady state $S$ by a multiplicity
%% factor $M$ 
%% Hence for example, a program that would generate a trace
%% $T_2$
%% might be as follows:
%% {\small
%% \begin{verbatim}
%% run_steady_state() {
%%   for (i = 0; i < 1; i++) A_work();
%%   for (i = 0; i < 1; i++) B_work();
%% }
%% \end{verbatim}}
%% \noindent but a scaled program that increases temporal locality is as follows:
%% {\small
%% \begin{verbatim}
%% run_steady_state() {
%%   for (i = 0; i < M * 1; i++) A_work();
%%   for (i = 0; i < M * 1; i++) B_work();
%% }
%% \end{verbatim}}
%% \noindent where \texttt{M} (the multiplicity factor) is an integer greater than
%% one.
%% From our model, it is easy to see that scaling reduces the number
%% of misses for a given filter $f$ from $K$ to $\frac{K}{\texttt{M}}- 1$, where $K =
%% \sum \mt{IMS}(f)$ for all occurrences of $f$ in  the execution trace.

%% \subsection{Remarks}

%% Note  that we can  combine the  instruction and  data cache  miss rate
%% equations to yield one measure that estimates the temporal locality of
%% a streaming computation.

%% Also note that while we use  an execution trace to describe our model,
%% it  is possible  to  calculate  the instruction  and  data cache  miss
%% metrics at  compile time.  To do so,  we can leverage  the hierarchical
%% StreamIt   representation  that  dictates   the  ordering   of  filter
%% executions.
