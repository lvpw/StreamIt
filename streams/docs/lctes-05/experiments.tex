\section{Experimental Evaluation}
\label{sec:evaluation}

\begin{table}[t]
\nocaptionrule
\center
\label{tab:benchmarks}
{\scriptsize
\begin{tabular}{|l|l|c|} \hline
\hspace{-2pt}{\bf Benchmark}&\hspace{-2pt}{\bf Description}& \hspace{-6pt} {\bf \# of Actors} \hspace{-6pt} \\ \hline \hline
\hspace{-2pt}\texttt{bitonic	} &\hspace{-2pt}bitonic sort of 64 integers	&	972 \\ \hline
\hspace{-2pt}\texttt{fir	      } &\hspace{-2pt} finite impulse response (128 taps)&	132 \\ \hline
\hspace{-2pt}\texttt{fft-fine	} &\hspace{-2pt}fine grained 64-way FFT	&	267 \\ \hline
\hspace{-2pt}\texttt{fft-coarse}\hspace{-2pt} &\hspace{-2pt}coarse grained 64-way FFT	&	26 \\ \hline
\hspace{-2pt}\texttt{3gpp	} &\hspace{-2pt}3GPP Radio Access Protocol	&	105 \\ \hline
\hspace{-2pt}\texttt{beamformer} & \hspace{-2pt}beamformer with 64 channels and 1 beam & 197 \\ \hline
\hspace{-2pt}\texttt{matmult	} & \hspace{-2pt}matrix multiplication	&	48 \\ \hline
\hspace{-2pt}\texttt{fmradio	} & \hspace{-2pt}FM Radio with 10-way equalizer	&	49 \\ \hline
\hspace{-2pt}\texttt{filterbank}\hspace{-2pt} & \hspace{-2pt}filterbank program (8 bands, 32 taps / filter)	&	53 \\ \hline
\hspace{-2pt}\texttt{filterbank2}\hspace{-3pt}& \hspace{-2pt}independent filterbank (3 bands, 100 taps / filter) &	37 \\ \hline
\hspace{-2pt}\texttt{ofdm	 }& \hspace{-2pt}Orthogonal Frequency Division Multiplexor~\cite{spectrumware}	&	16 \\ \hline
\end{tabular}
}
\caption{Evaluation benchmark suite.}
\end{table}

In this section we evaluate the merits of the proposed cache-aware
optimizations and buffer management strategies.  We use three
different architectures: a 137MHz StrongARM-1110, a 600~MHz Pentium~3
and a 1.3~GHz Itanium~2. The StrongARM results reflect performance for
an embedded target; it has a 16Kb L1 instruction cache, an 8Kb L1 data
cache, and no L2 cache.  The StrongARM also has a separate 512-byte
minicache (not targeted by our optimizations).  The Pentium~3 and
Itanium~2 reflect desktop performance; they have a 16Kb L1 instruction
cache, 16Kb L1 data cache, and 256Kb shared L2 cache.

Our benchmark suite (see Table~\ref{tab:benchmarks}) consists of
eleven StreamIt applications. They are compiled with the StreamIt
compiler which applies the optimizations described in this paper, as
well as aggressive loop unrolling (by a factor of 128 for all
benchmarks) to facilitate scalar replacement
(Section~\ref{sec:buffer}).  The StreamIt compiler outputs a
functionally equivalent C program that is compiled with \texttt{gcc}
(v3.4, -O3) for the StrongARM and for the Pentium~3 and with
\texttt{ecc} (v7.0, -O3) for the Itanium~2.  Each benchmark is then
run five times, and the median user time is recorded.

As the StrongARM does not have a floating point unit, we converted all
of our floating point applications (i.e., every application except for
{\tt bitonic}) to operate on integers rather than floats.  In
practice, a detailed precision analysis is needed in converting such
applications to fixed-point.  However, as the control flow within
these applications is very static, we are able to preserve the
compuation pattern for the sake of benchmarking by simply replacing
every floating point type with an integer type.

We also made an additional modification in compiling to the StrongARM:
our execution scaling heuristic scales actors until their output
fills 100\% of the data cache, rather than 2/3 of the data cache as
described in Section~\ref{sec:cache-opt}.  This modification accounts
for the 32-way set-associative L1 data cache in the StrongARM.  Due to
the high degree of associativity, there is a smaller chance that the
actor outputs will repeatedly evict the state variables of the
actor, thereby making it worthwhile to further fill the data cache.
This observation leads yields up to 25\% improvement on some
benchmarks.

\begin{figure}[t]
\vspace{-6pt}
\nocaptionrule
\begin{minipage}{3.35in}
\centering
\psfig{figure=arm1.eps,width=3.4in}
\vspace{-14pt}
\caption{Summary of results on a StrongARM.\protect\label{fig:arm-perf}}
~ \\

\psfig{figure=p3-1.eps,width=3.4in}
\vspace{-14pt}
\caption{Summary of results on a Pentium~3\protect\label{fig:p3-perf}}
~ \\

\psfig{figure=i2-1.eps,width=3.4in}
\vspace{-14pt}
\caption{Summary of results on an Itanium~2\protect\label{fig:i2-perf}}
~ \\
\end{minipage}
\end{figure}

\paragraph*{Overall Speedup} 
The overall speedups offered by our techniques are illustrated in
Figure~\ref{fig:arm-perf} (StrongARM), Figure~\ref{fig:p3-perf}
(Pentium~3), and \ref{fig:p3-perf} (Itanium~2).  These graphs have two
bars: one for ``full fusion'', in which all actors are fused into a
single function (with scalar replacement), and one labelled {\tt
CAF+scaling+buffer}, representing all of our optimizations (cache
aware fusion, execution scaling, and buffer optimizations) applied
together.  We include the comparison to full fusion because it
represents a simple approach for eliminating function call overhead
and optimizing across actor boundaries; while this fusion strategy
utilizes our scalar replacement optimization, it remains oblivious to
instruction and data locality.  Performance is normalized to
unoptimized StreamIt, in which no actors are fused (but there is
still unrolling by 128).  On the right of each graph, both arithmetic
and geometric means are shown for the benchmarks.  We usually speak in
terms of the arithmetic means as they are more intuitive (and also
yield more conservative speedups), though we refer to the geometric
mean for an unambiguous reference on close results.

On StrongARM (Figure~\ref{fig:arm-perf}), our cache optimizations
offer a 249\% average speedup over the baseline and a 162\% average
speedup over full fusion.  Scalar replacement is responsible for much
of the gains that both strategies offer over the baseline.  Cache
optimizations always perform better than the baseline, and they
perform better than full fusion in all cases except for \texttt{3gpp},
where they yield a 45\% slowdown.  This slowdown is due to
conservative code size estimation: the compiler predicts that the
fused version of \texttt{3gpp} will not fit into the instruction
cache, thereby preventing fusion.  However, due to optimizations by
{\tt gcc}, the final code size is smaller than expected and does fit
within the cache.  While such inaccuracies could be improved by adding
feedback between the output of {\tt gcc} and our code estimation, each
fusion possibility would need to be evaluated separately as the fusion
boundary affects the impact of low-level optimizations (and thus the
final code size).

The speedups offered by cache optimizations over a full fusion
strategy are more modest for the desktop processors: 34\% average
speedup on Pentium~3 (Figure~\ref{fig:p3-perf}) and essentially zero
speedup (6\% by the arithmetic mean, -8\% by the geometric mean) on
Itanium~2 (Figure~\ref{fig:i2-perf}).  Out of the 11 benchmarks, cache
optimizations perform as well or better than full fusion for 7
benchmarks on the Pentium~3 and 5 benchmarks on the Itanium~2.
Performance on any architecture is a tradoeff between two factors: 1)
the benefit of data and instruction locality, and 2) the benefit of
fusion, which reduces memory accesses due to improved register
allocation across actor boundaries.  Compared to the StrongARM, the
Pentium~3 and Itanium~2 offer an L2 cache (as well as a larger L1 data
cache), thereby lessening the impact of locality-enhancing cache
optimizations.  However, the fusion benefit remains a significant
factor; for example, using Intel VTune on the Pentium~3, we measured
that full fusion offers a 50\% reduction in memory accesses over the
cache-optimized version.  This effect may be pronounced in Itanium~2
due to the larger number of registers on that architecture (128
general, 128 floating point).  While fusion benefits are also present
on StrongARM, cache optimizations are more important on that processor
due to the large penalty for cache misses.

The cache-aware fusion algorithm can be adjusted to account for the
caching hierarchy on desktop machines.  Specifically, if cache-aware
fusion is modified to allow fused actors to consume up to one half of
the L2 cache (rather than L1 cache), then the performance of cache
optimizations is closer or equal to full fusion, for cases it was
previously trailing on Pentium~3 or Itanium~2 (data not shown).  The
only benchmark that is negatively impacted by this change is {\tt
ofdm}, where two large actors are fused despite a very low
communication to computation ratio, thereby lessening the impact of
eliminated memory accesses while nonetheless worsening the instruction
locality.

\begin{figure}[t]
\vspace{-6pt}
\nocaptionrule
\begin{minipage}{3.35in}
\centering
\psfig{figure=arm2.eps,width=3.4in}
\vspace{-14pt}
\caption{Impact of each optimization on a StrongARM.\protect\label{fig:arm-perf2}}
~ \\

\psfig{figure=p3-2.eps,width=3.4in}
\vspace{-14pt}
\caption{Impact of each optimization on a Pentium~3.\protect\label{fig:p3-perf2}}
~ \\

\psfig{figure=i2-2.eps,width=3.4in}
\vspace{-14pt}
\caption{Impact of each optimization on an Itanium~2.\protect\label{fig:i2-perf2}}
~ \\
\end{minipage}
\end{figure}

\paragraph*{Impact of Each Optimization}
To better understand the overall speedups, we assess the individual
performance impact of execution scaling, cache aware fusion, and
buffer management optimizations.  These results are illustrated in
Figures~\ref{fig:arm-perf2}, \ref{fig:p3-perf2} and~\ref{fig:i2-perf2}
for the StrongARM, Pentium~3, and Itanium~2, respectively.  There are
four bars per benchmark; as in the previous analysis, all performance
is normalized to unoptimized StreamIt (with 128-way unrolling).  The
first bar, labeled {\tt scaling}, applies only execution scaling.  The
second bar, labeled {\tt CAF}, applies only cache-aware fusion,
including scalar replacement across the fused actors.  The third bar,
labeled {\tt CAF+scaling}, first applies cache-aware fusion with
scalar replacement and then applies execution-scaling to the
granularity-adjusted actors.  The fourth bar, labeled {\tt
CAF+scaling+buffer}, additionally applies buffer management
optimizations (detailed later); this bar is equivalent to the ``best''
cache-optimized performance illustrated in Figures~\ref{fig:arm-perf},
\ref{fig:p3-perf}, and \ref{fig:i2-perf}.

Execution scaling improves performance over unoptimized StreamIt, with
average speedups of 145\% for StrongARM, 58\% for Pentium~3, and 57\%
for Itanium~2.  The 90-10 heuristic works quite well
(see~\cite{janis-thesis} for full details) and there is only one
instance where scaling results in a performance degradation. The
granularity-adjusted \texttt{3gpp} on StrongARM has a 17\% slowdown
due to scaling (compare {\tt CAF} to {\tt CAF+scaling} in
Figure~\ref{fig:arm-perf}).  This is possibly due to items in flight
between the granularity-adjusted actors overwriting the state of an
executing actor in the data cache.  Since StrongARM has no L2 cache
then such eviction can be quite expensive.

Independently, cache-aware fusion also improves performance by 84\% on
StrongARM, 101\% on Pentium~3 and a 103\% on the Itanium~2.
Cache-aware fusion degrades performance only for
\texttt{filter}-\texttt{bank2} (by 6\% on StrongARM). When we combine
cache-aware fusion with execution scaling, the performance
consistently improves.  The speedup of \texttt{CAF+scaling} over
baseline is 241\% on StrongARM, 146\% on Pentium~3 and 144\% on
Itanium~2.

However, after coarsening the actors with cache-aware fusion, scaling
results in less additional speedup than it did relative to the
baseline.  The speedup of \texttt{CAF+scaling} over \texttt{CAF} is
86\% for StrongARM, 22\% for Pentium~3 and only 20\% for Itanium~2.
This is because some actors are implicity scaled by fusion to match
input/output rates of succesive actors within a fused block.

Note that the \texttt{ofmd} benchmark does not benefit from fusion or
scaling. This is because \texttt{ofmd} has few actors, some of which
consume and produce a total of 16-66Kb data; consequently, execution
scaling does not apply.  Also there is limited oportunity to fuse
actors within \texttt{ofmd}, as there are actors that have an
instruction size of 9Kb and fusing them with other actors would
exceed the instruction cache.

The last bar, {\tt CAF+scaling+buffer}, illustrates the benefit of
buffer management optimizations for filters that peek.  As detailed in
Section~\ref{sec:buffer}, such filters demand specialized buffer
management, as they reuse items on the input tape across successive
iterations.  In our benchmark suite, peeking occurs only in the last
four applications (i.e., \texttt{fmradio}, \texttt{filterbank},
\texttt{filterbank2} and \texttt{ofdm}).  Thus, the {\tt CAF+scaling}
bar is equivalent to {\tt CAF+scaling+buffer} for all benchmarks from
{\tt bitonic} through {\tt matmult}.

The buffer optimization applied in Figures~\ref{fig:arm-perf},
\ref{fig:p3-perf}, and \ref{fig:i2-perf} is termed {\tt cutpeek}.  In
this optimization, the cache-aware fusion algorithm is modified so
that two adjacent filters are never fused if the downstream filter
performs any peeking (i.e., it has $\mbox{\it peek} > \mbox{\it
pop}$).  Following execution scaling, the live items are copied to the
start of the buffer once per scaled execution (using the copy-shift
strategy, Section~\ref{sec:fusion-opt}), thereby reducing the overhead
of copying live items.  Optimized buffer management offers the largest
gains for {\tt filterbank2}: 45\% on StrongARM, 38\% on Pentium~3, and
36\% on Itanium~2.  This is due to a large peek rate (100 items) in
addition a large scaling factor of 300 that amortizes the cost of
copying items.

While we found that {\tt cutpeek} is, on average, the best buffer
management strategy across the three platforms, we also evaluated
several others.  For {\tt fmradio}, up to a 5\% improvement is offered
by a strategy called {\tt peekscale}, in which (prior to other
optimizations) filters with $\mbox{\it peek}>\mbox{\it pop}$ are
scaled to perform several executions at once.  As scaling is
performed, the {\it pop} and {\it peek} rates increase but $\mbox{{\it
peek}} - \mbox{\it pop}$ remains constant; scaling continues until the
{\it pop} rate is at least $4*(\mbox{{\it peek}} - \mbox{{\it pop}})$.
Like {\tt cutpeek}, this serves to amortize the overhead of copy-shift
buffer management, but it can also hamper the execution scaling
optimization.  Because certain other actors in the graph must fire
more frequently to compensate for the scaled peeking filter, there is
less room for global execution scaling.  The only other case where
{\tt peekscale} is the best strategy is for {\tt ofdm} on StrongARM,
where there is a 7\% improvement over {\tt cutpeek}.  Finally, on the
{\tt filterbank} benchmark, highest performance results from
unoptimized copy-shift buffer management.  We also evaluated buffer
management using modulation for indexing, but this did not yield the
best performance for any benchmark or platform.
