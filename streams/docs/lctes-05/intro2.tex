\section{Introduction}

Efficiency and high performance are of central importance within the
embedded domain.  As processor speeds continue to increase, the memory
bottleneck remains a primary impediment to attaining performance.
Current practices for hiding memory latency are invariably expensive
and complex.  For example, superscalar processors resort to
out-of-order execution to hide the latency of cache misses.  This
results in large power expenditures (unfit for embedded systems) and
also increases the cost of the system.  Compilers have also employed
computation and data reordering to improve locality, but this requires
a heroic analysis due to the obscured parallelism and communication
patterns in traditional languages such as C.

For performance-critical programs, the complexity inevitably
propagates all the way to the application developer.  Programs are
written to explicitly manage parallelism and to reorder the
computation so that the instruction and data working sets fit within
the cache.  For example, the inputs and outputs of a procedure might
be arrays that are specifically designed to fit within the data cache
on a given architecture; loop bodies are written at a level of
granularity that matches the instruction cache.  While manual tuning
can be effective, the end solutions are not portable.  They are also
exceedingly difficult to understand, modify, and debug.

The recent emergence of streaming applications represents an
opportunity to mitigate these problems using simple transformations in
the compiler.  Stream programs are rich with parallelism and regular
communication patterns that can be exploited by the compiler to
automatically tune memory performance.  Streaming codes encompass a
broad spectrum of applications, including embedded communications
processing, multimedia encoding and playback, compression, and
encryption.  They also range to server applications, such as HDTV
editing and hyper-spectral imaging.  It is natural to express a stream
program as a high-level graph of independent components, or {\it
actors}.  Actors communicate using explicit FIFO channels and can
execute whenever a sufficient number of items are available on their
input channels.  In a stream graph, actors can be freely combined and
reordered to improve caching behavior as long as there are sufficient
inputs to complete each execution.  Such transformations can serve to
automate tedious approaches that are performed manually using today's
languages; they are too complex to perform automatically in hardware
or in the most aggressive of C compilers.

%By calculating the input and
%output rates of each actor, the compiler can determine how long to
%execute each one so as to assure that the outputs fit within the data
%cache.  In addition, by running an actor multiple times in succession,
%instruction locality is enhanced.  

This paper presents three simple cache aware optimizations for stream
programs: {\it (i)} execution scaling, {\it (ii)} cache aware fusion,
and {\it (iii)} scalar replacement.  These optimizations represent a
{\it unified approach} that simultaneously considers the instruction
and data working sets.  We also develop a simple quantitative model of
caching behavior for streaming workloads, providing a foundation to
reason about the transformations.  Our work is done in the context of
the Synchronous Dataflow~\cite{LM87-i} model of computation, in which
each actor in the stream graph has a known input and output rate.  This
is a popular model for a broad range of signal processing and embedded
applications.

Execution scaling is a transformation that improves instruction
locality by executing each actor in the stream graph multiple times
before moving on to the next actor.  As a given actor usually fits
within the cache, the repeated executions serve to amortize the cost
of loading the actor from off-chip memory.  However, as our cache
model will show, actors should not be scaled excessively, as their
outputs will eventually overflow the data cache.  We present a simple
and effective algorithm for calculating a scaling factor that respects
both instruction and data constraints.

Prior to execution scaling, cache aware fusion combines adjacent
actors into a single function.  This allows the compiler to optimize
across actor boundaries.  Our algorithm is cache aware in that it
never fuses a pair of actors that will result in an overflow of the
instruction cache.

As actors are fused together, new buffer management strategies become
possible.  The most aggressive of these, termed scalar replacement,
serves to replace an array with a series of local scalar variables.
Unlike array references, scalar variables can be register allocated,
leading to large performance gains.  We also develop a new buffer
management strategy (called ``copy-shift'') that extends scalar
replacement to sliding-window computations, a domain where complex
indexing expressions typically hinder compiler analysis.

Our cache aware optimizations are implemented as part of StreamIt, a
language and compiler infrastructure for stream
programming~\cite{streamitcc}.  We evaluate the optimizations on three
architectures.  The StrongARM 1110 represents our primary target; it
is an embedded processor without a secondary cache.  Our other targets
are the Pentium~3 (a superscalar) and the Itanium~2 (a VLIW
processor).  We find that execution scaling, cache aware fusion, and
scalar replacement each offer significant performance gains, and the
most consistent speedups result when all are applied together.
Compared to unoptimized StreamIt code, our cache optimizations yield a
249\% speedup on the StrongARM, a 154\% speedup on the Pentium~3, and
a 152\% speedup on Itanium~2.  These numbers represent averages over
our streaming benchmark suite.

This paper is organized as follows.  Section~2 gives background
information on the StreamIt language.  Section~3 lays the foundation
for our approach by developing a quantitative model of caching
behavior for any sequence of actor executions.  Section~4 describes
execution scaling and cache aware scheduling.  Section~5 evaluates
buffer management strategies, including scalar replacement.  Section~6
contains our experimental evaluation of these techniques in the
StreamIt compiler.  Finally, Section~7 describes related work and
Section~8 concludes the paper.

%% In summary, this paper presents a unified optimization strategy for
%% improving instruction and data locality.  Its contributions are:
%% \begin{itemize}
%% \item A cache model for stream computing that provides a quantitative
%% estimate of the caching performance for any sequence of actor
%% executions (Section 3).
%% \item A cache aware scheduling heuristic that judiciously scales the
%% execution frequency of actors to improve instruction and data locality,
%% while not overflowing the data cache (Section 4).
%% \item A cache aware partitioning policy that judiciously fuses
%% adjacent actors into a single component, enabling local optimizations,
%% while not overflowing the instruction cache (Section 4).
%% \item An optimized buffer management policy, termed ``copy-shift with
%% execution scaling'', which out-performs a traditional rotating buffers
%% in a micro-benchmark analysis (Section 5).
%% \item An evaluation of these techniques using the
%% StreamIt compiler and a streaming benchmark suite of eleven programs.
%% Compared to unoptimized StreamIt, the cache optimizations deliver an
%% average speedup of 249\% on a StrongARM, 154\% on a Pentium~3 and
%% 152\% on an Itanium~2 (Section 6).
%% \end{itemize}

%% In traditional C programs, the compiler is very good at generating
%% code within a function to keep resources busy when there is available
%% ILP.  However, with current language abstractions, it is extremely
%% difficult for the compiler to do large-scale reordering of different
%% parts of the program to match a given data cache.  This is because a)
%% the coarse-grained data dependences are not exposed, limiting
%% reordering opportunities, b) data access patterns are obscured, making
%% it hard to predict what should be cached, and c) the communication
%% pattern and bandwidth between components is obscured, often through
%% shared memory.
%%
%% As a result, programmers have turned to manually optimizing a
%% streaming runtime system to fit a given cache hierarchy.  Procedures
%% are written in terms of a given block size that leads to good data
%% caching behavior.  This block size is propagated throughout the
%% program and becomes inseparable with the underlying algorithm.  When
%% the architecture changes, the code must be re-engineered to suite the
%% new caching system.  It also becomes very complex from the
%% programmer's standpoint, as the high-level algorithm is lost in the
%% details.
%%
%% The recent emergence of streaming applications (examples) represents a
%% new opportunity for the compiler to achieve good use of the cache
%% hierarchy using simple high-level transformations.  A stream program
%% is often represented as a graph of independent actors, where all
%% communication between actors occurs over explicit FIFO queues.  Actor
%% executions can be reordered so long as they respect the data
%% dependences over the communication channels.  By calculating the input
%% and output rates of each actor, the compiler can determine how long to
%% execute each one so as to assure that the working set remains in the
%% cache.  In addition, by running each actor for as long as possible at
%% a given time, instruction locality is enhanced.  These transformations
%% serve to automate tedious approaches that have must be performed
%% manually using today's languages and compilers.
