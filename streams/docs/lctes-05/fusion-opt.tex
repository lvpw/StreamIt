\section{Buffer Management}
\label{sec:buffer}

A salient characteristic of stream programs is the use of FIFO
channels to communicate between parallel components.  Such channels
make explicit the communication between filters, allowing execution to
proceed in parallel or out-of-order so long as items are produced
before they are consumed.  FIFO channels also provide a natural
abstraction for the programmer, as complex modules can be assembled
from a set of small, reusable components.  For these reasons, it is
important to optimize the performance of communication channels.  An
efficient implementation enables a high-level abstraction for
composing filters without sacrificing performance.

Buffer management in StreamIt is more involved than some other stream
languages, due to the {\tt peek} operation.  The {\tt peek} operation
allows a filter to access an item on its input channel without
removing the item from the channel (removal is done via the {\tt pop}
operation).  The {\tt peek} functionality is very important for
components such as FIR (Finite Impulse Response) filters that access
data over a sliding window.  Because a given data item is accessed by
multiple iterations of the filter, there must be a persistent buffer
that stores items across executions.  In the context of a
uniprocessor, efficient buffer management translates to efficient
maintenance and addressing of this buffer in memory.  On a parallel
system, buffers can also be implemented using network links.

In this section, we explore two basic strategies for buffer management
in stream programs.  The first strategy, termed {\it modulation},
implements a traditional circular buffer that is indexed by wraparound
head and tail pointers.  The second strategy, termed {\it copy-shift},
is a new approach that elides modulo operations by shifting the buffer
contents after each operation.  We demonstrate that, while a naive
implementation of copy-shift can be up to 2X slower than modulation,
optimizations that utilize execution scaling can boost the performance
of copy-shift to be roughly 50\% faster than modulation.

\input{fusion-fig1}

Our study is done in the context of a synthetic benchmark, shown in
Figure~\ref{fig:code-orig}.  
%As depicted in Figure~\ref{fig:code-graph}, 
The benchmark is a pipeline consisting of a simple source and an FIR
filter.  On each iteration, the source pushes a single item.  The FIR
filter calculates a weighted sum over {\tt PEEK} items of the input,
then pops a single item from the channel.  In our experiments, we vary
the {\tt PEEK} value from 1 to 121 items.

\input{fusion-fig2}

\subsection{Modulation}

Figure~\ref{fig:code-modulation} illustrates a fused version of the
benchmark using modulation for buffer management.  For simplicity, we
illustrate each buffer management strategy as a source-to-source
transformation in StreamIt.  Each fused filter contains a {\tt
prework} function in which the source filter executes several times to
prime the communication channel with initial items, as well as a {\tt
work} function that represents the steady-state execution.

The modulation scheme uses a traditional circular-buffer approach.
Three variables are introduced: a {\tt BUFFER} to hold all items
transfered between the filters, a {\tt push\_index} to indicate the
buffer location that will be written next, and a {\tt pop\_index} to
indicate the buffer location that will be read next (i.e., the
location corresponding to {\tt peek(0)}).  The communication
primitives are translated as follows: {\small
\begin{verbatim}
push(val); ==>  BUFFER[push_index] = val;
                push_index = (push_index + 1) % BUF_SIZE;

pop();     ==>  pop_index = (pop_index + 1) % BUF_SIZE;

peek(i)    ==>  BUFFER[(pop_index + i) % BUF_SIZE]
\end{verbatim}}
\noindent The StreamIt compiler converts the modulo operations to
bitwise-and operations by scaling the buffer to a power of two.  Note
that if there are no {\tt peek} operations, then the buffer will be
empty following each execution of the downstream filter.  In this
case, the indices can be reset to zero at the start of each execution
and the modulo operations can be eliminated.  However, in our example
the FIR filter does perform peeking, so the modulo operations are
needed.

{\bf Experimental setup.}  Figures~\ref{fig:buf-p3}
and~\ref{fig:buf-itanium} illustrate the performance of various buffer
management strategies on a Pentium~3 and an Itanium~2, respectively.
The figures illustrate the execution time per output for the synthetic
benchmark (Figure~\ref{fig:code-orig}) across a range of {\tt PEEK}
values.  Performance is normalized to the modulation strategy with
{\tt PEEK=1}.  To ensure a fair comparison with the scalar replacement
optimization (Section~\ref{sec:scalar-replacement}), all loops in the
original filter are fully unrolled.

{\bf Evaluation.}  The time required for the modulation strategy
increases linearly with the peek rate.  This is expected, as there is
a constant overhead per peek operation.

\subsection{Copy-Shift}

The copy-shift strategy, illustrated in Figure~\ref{fig:copy-shift},
is a novel approach that shifts the live items to the front of the
buffer at the beginning of each execution.  Because each execution
starts writing and reading to the buffer at the same location, there
is no need for the indices to wraparound and the modulo operations can
be eliminated.  This savings is compounded by additional optimizations
enabled by the copy-shift approach, as described in the subsequent
sections.

However, the cost of this strategy comes in the copying operations: at
the start of each execution, $(\mbox{\it peek} - \mbox{\it pop})$
items are copied from the persistent {\tt BUFFER} to the beginning of
a local {\tt TEMP\_BUFFER}.  All subsequent operations reference the
{\tt TEMP\_BUFFER}, and the live items are copied back to the {\tt
BUFFER} upon completion.  While these two variables could also be
combined into a single buffer, keeping them separate results in a
smaller live data set when the filter is not executing.

The communication primitives are translated as follows:
{\small
\begin{verbatim}
push(val); ==>  TEMP_BUFFER[push_index] = val;
                push_index = push_index + 1;

pop();     ==>  pop_index = pop_index + 1;

peek(i)    ==>  TEMP_BUFFER[pop_index + i]
\end{verbatim}}
\noindent Compared to the modulation scheme, the copy-shift strategy
references the {\tt TEMP\_BUFFER} and does not perform modulo
operations.

{\bf Evaluation.}  As shown in Figures~\ref{fig:buf-p3}
and~\ref{fig:buf-itanium}, the unoptimized copy-shift strategy is the
slowest strategy that we evaluate.  Though the cost per peek operation
is smaller than the modulation scheme, the copying overhead per
iteration also grows with the peek rate and cancels out any savings;
overall, copy-shift performs up to 2X slower than modulation.  The
following sections describe optimizations that can justify taking the
copy-shift approach.

\subsection{Copy-Shift with Scalar Replacement}
\label{sec:scalar-replacement}

The first optimization enabled by the copy-shift scheme is dubbed {\it
scalar replacement}.  In contrast to the modulation scheme, the
copy-shift approach can result in array operations that access the
same location on every execution of the filter.  The idea behind
scalar replacement is to fully unroll the loops in the filter, thereby
resolving each array index to an integer literal.  Then, since each
location is fully resolved at compile time, an $n$-length array can be
replaced by a set of $n$ scalar variables: one for each item in the
buffer.  This transformation is illustrated in
Figure~\ref{fig:code-scalar-replace}.

Scalar replacement offers several performance benefits.\\ Scalar
variables can be register allocated, and as local variables they are
subject to a range of dataflow optimizations (constant propagation,
copy propagation, dead code elimination, etc.).  Replacing array
operations with scalars also eliminates array index calculations.
Despite these benefits, scalar replacement is nearly impossible to do
in a general-purpose language such as C because array contents might
be aliased with other pointers.  StreamIt arrays represent values that
are independent in memory, thereby facilitating this optimization.

Note that scalar replacement can only be applied when array indices
can be resolved to compile-time constants.  In the presence of unpredictable
control flow within a filter, or if the loops are too large to fully unroll,
then scalar replacement does not apply.

{\bf Evaluation.}  Compared to an unoptimized copy-shift strategy,
Figures~\ref{fig:buf-p3} and~\ref{fig:buf-itanium} illustrate that
scalar replacement offers modest gains on our synthetic benchmark.  On
the Pentium~3, improvements range from 14\% to 26\%, while on the
Itanium~2, improvements range from 3\% to 38\% (depending on the peek
rate).  We conjecture that scalar replacement is more critical for
small filters that perform only a few operations.  Due to the high
communication-to-computation ratio in such filters, there could be
large gains from register-allocating and copy-propagating the
temporary variables.

\subsection{Copy-Shift with Execution Scaling}

A final optimization of the copy-shift strategy uses execution scaling
to dramatically decrease the overhead associated with copying the
buffer contents on each iteration.  In any filter, the number of items
inspected on one execution that need to be saved for the next
execution is $(\mbox{\it peek} - \mbox{\it pop})$.  This represents
the number of items copied by the copy-shift scheme.  However, this
cost can be amortized by scaling the number of executions of the
downstream filter in the fused code.  By enclosing the body of the
filter in a loop, the $\mbox{\it peek}$ and $\mbox{\it pop}$ rates can
be made arbitrarily large, while $(\mbox{\it peek} - \mbox{\it pop})$
remains constant.

Thus, execution scaling reduces the fraction of time spent copying to
an arbitrarily small level.  In our study, we scale the executions of
a filter until $(\mbox{\it peek} - \mbox{\it pop}) \leq \frac{1}{4}
\mbox{\it pop}$.  In the synthetic benchmark, this implies that each
filter body executes 16 times before the buffer contents are shifted.
The code resulting from this transformation is shown in
Figure~\ref{fig:code-scaling}.

Note that due to the large loops introduced by execution scaling, it
cannot be used in combination with scalar replacement.  If the loops
were unrolled to resolve the array indices, there could be a negative
impact on the instruction cache.

{\bf Evaluation.}  As shown in Figure~\ref{fig:buf-p3}, the copy-shift
approach with execution scaling performs significantly better than
modulation on the Pentium~3.  Speedups range from 38\% to 65\%, with a
geometric mean of 52\%.  This makes sense, as each peek operation is
cheaper due to the eliminated modulo operations (implemented as
bitwise-and in the modulation scheme), while the overhead from copying
is reduced to a fraction of the original copy-shift approach.  

The gains are also sizable on the Itanium~2, as shown in
Figure~\ref{fig:buf-itanium}.  Speedups range from 11\% to 86\% (where
{\tt PEEK=1}), with a geometric mean of 34\%.  Keep in mind that these
are speedups of copy-shift with scaling over modulation; the speedups
over unoptimized copy-shift are even greater.

\subsection{Summary}

We conclude that copy-shift with execution scaling is the best buffer
management strategy for filters that utilize peeking.  This is
somewhat surprising because the unoptimized copy-shift strategy has
large overheads that result in a slowdown relative to a circular
buffer with modulation.  However, by leveraging the flexibility of the
parallel stream graph to perform execution scaling, the overheads are
amortized and there are significant speedups (geometric means of 52\%
on the Pentium~3 and 34\% on the Itanium~2) over a plain circular buffer
strategy.

%% \subsection{Reusing Intermediate Storage Variables}

%% Once loops have been unrolled, filters have been fused into a
%% partition and arrays have been replaced with scalar variables we can
%% find approximations of live ranges for the new variables and use this
%% information to reduce stack space required by the partition's work
%% function.

%% We replace the scalar variables that have been created as a result of
%% destroying arrays with a minimal number of variables, where minimal
%% number is the maximum number of overlapping live ranges at any point
%% in the work function of the fused partition.

%% The above optimization improves data access locality.

