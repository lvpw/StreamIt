\section{Introduction}

Stream computing represents an increasingly widespread class of
applications. In streaming codes, there is an abundance of parallelism that
is easier to extract compared to traditional desktop workloads (e.g.,
pointer-based computing). As a result, the extraction of parallelism
in streaming codes does not require heroic efforts, and thus,
processors can deliver higher performance with significantly lower
power costs. This is especially important since
leading microprocessor companies have realized that modern general
purpose architectures are near their  performance limits for  the
amount of power they consume. Thus, the future will place a greater
emphasis on exploiting the properties of streaming workloads in
conventional von~Neumann architectures.

Streaming is a model of computation that uses sequences of data
and computation kernels to expose concurrency and locality for
efficiency~\cite{wss}. In general purpose processors, improving locality 
translates to an effective management of the memory hierarchy at all
levels, including the register file. In this paper, we present a
methodology for compiling streaming codes to general purpose,
cache-based architectures. We first introduce a simple model for
reasoning effectively about the caching behavior of streaming
workloads. This model serves as a foundation for several {\it cache aware
optimizations} that are geared toward the concomitant increase of instruction
and data {\it temporal locality}. These
optimization lead to significantly better utilization of the memory
system, and as such, they improve the performance of unoptimized code
by 249\% for our streaming benchmark suite on an embedded StrongARM 
processor.   They also offer performance gains on desktop machines:  
a speedup of 154\% on a Pentium~3 and a speedup of 152\% on an Itanium~2,
compared to unoptimized code.

%they deliver performance gains ranging from 11\%
%to 99\% for our streaming benchmark suite.

The context for our work is StreamIt, an architecture independent
language that is engineered for streaming
applications~\cite{streamitcc}. It adopts the 
Cyclo-Static Dataflow~\cite{BELP96} model of computation which is a
generalization of Synchronous Dataflow~\cite{LM87-i} (SDF).  
SDF is a popular  model that  is well suited for
streaming codes. In SDF, computation is represented as a graph
consisting of {\it  actors} connected by communication channels; the
actors consume  and produce a constant number  of items from their
input and output  channels every time they execute. SDF is appealing
because it is amenable to static scheduling and
optimization. An overview of StreamIt appears in Section~\ref{sec:streamit}.

From a general purpose architecture's point of view, actors represent
computation kernels, and the communication between actors represents
data buffers that must be streamed to and from the processor. Thus
the size of an actor and the order of actor executions are critical
properties that impact the performance of the instruction cache.  

In this paper, we introduce {\it execution scaling} as a simple and
effective optimization to improve instruction locality. Scaling 
increases the number of consecutive executions of an actor to
amortize the cost of fetching its instructions into the cache (usually
an expensive operation). The locality improvements translate to better
performance. However, as our cache model will show
(Section~\ref{sec:cache-model}), we  cannot arbitrarily scale the
execution frequency of an actor. This is because actors produce data
that is buffered, and therefore, we must also consider the amount of
data an actor produces and consumes if we are to adequately manage the
data cache.  This paper is one of the first to present a unified
optimization methodology that simultaneously considers instruction and
data locality for mapping streaming computation to cache-based
architectures (Section~\ref{sec:cache-opt}). 

In terms of improving the data cache behavior, the compiler schedules
actor firing such that the producer-consumer locality is
preserved. Furthermore,  the compiler applies  {\it cache aware fusion}
to fuse together two or more actors (forming a coarser grained
execution unit). The fusion allows for better register allocation as we
can sometimes perform {\it scalar replacement}: arrays used to buffer
data between the actors are broken down into a set of local scalar
variables. Our compiler fuses actors only when the resulting actor
fits within the instruction cache, as otherwise fusion would
negatively impact instruction locality. Fusion also allows for
various competing implementations for managing the buffers between the
fused actors.  This paper investigates several implementation
alternatives (for buffer management) and evaluates their performance
(Section~\ref{sec:buffer}).

%% The methodology for fusing actors leverages a distinguishing StreamIt
%% characteristic, namely, the hierarchical organization of
%% the stream graph. Furthermore, the algorithm for fusing actors applies
%% for the various topologies allowed by StreamIt.
%% It also considers another distinguishing characteristics of StreamIt,
%% namely the {\tt peek} operation whereby an actor may inspect data
%% items in its input buffer without consuming them until some future
%% execution. While peeking is a powerful language feature, it does pose
%% some challenges to the compiler and the cache optimizations. Peeking
%% also impacts the choice for the best buffer management strategy, as our
%% study will show.

%% the comment about p3 and itanium not being embedded architectures
%% is out of the blue! need a better transition.
For our experiments, we use three different processors: a StrongARM
1110, a Pentium~3, and an Itanium~2.  The StrongARM represents an
embedded processor in which cache optimizations are very important due
to the absence of a secondary cache.  The other processors are
general-purpose desktop machines, the Pentium~3 representing
performance on a superscalar out-of-order processor and the Itanium~2
representing a VLIW processor.  Execution scaling gives a 145\%
speedup over unoptimized StreamIt for our benchmark suite on the
StrongARM, a 58\% speedup on the Pentium~3 and a 57\% speedup on the
Itanium~2. Cache aware fusion with scalar replacement leads to a 84\%
speedup on the StrongARM, a 101\% speedup on the Pentium~3, and a
103\% speedup on the Itanium~2.

While both execution scaling and fusion lead to frequent performance
gains, we combine the two to deliver a more consistent performance
improvement. When we do so, we can achieve an average speedup of 241\%
on a StrongARM, 146\% on a Pentium~3, and 144\% on 
an Itanium~2. An optimized buffer management strategy in
combination with scaling and fusion gives the best possible performance.
Section~\ref{sec:evaluation} describes our evaluation
methodology and presents our experimental analysis.

In summary, this paper makes the following contributions:
\begin{itemize}
\item A cache model for stream computing that provides a quantitative
estimate of the caching performance for any sequence of actor
executions (Section 3).
\item A cache aware scheduling heuristic that judiciously scales the
execution frequency of actors to improve instruction and data locality,
while not overflowing the data cache (Section 4).
\item A cache aware partitioning policy that judiciously fuses
adjacent actors into a single component, enabling local optimizations,
while not overflowing the instruction cache (Section 4).
\item An optimized buffer management policy, termed ``copy-shift with
execution scaling'', which out-performs a traditional rotating buffers
in a micro-benchmark analysis (Section 5).
\item An evaluation of these techniques using the
StreamIt compiler and a streaming benchmark suite of eleven programs.
Compared to unoptimized StreamIt, the cache optimizations deliver an
average speedup of 249\% on a StrongARM, 154\% on a Pentium~3 and
152\% on an Itanium~2 (Section 6).
\end{itemize}

%% The remainder of the paper is organized as
%% follows. Section~\ref{sec:streamit} describes StreamIt and introduces
%% our motivating example.  Section~\ref{sec:cache-model} introduces our
%% cache model for reasoning about the performance of a streaming
%% computation. Section~\ref{sec:cache-opt} describes our cache aware
%% optimizations, and Section~\ref{sec:buffer} describes the optimization
%% enabled by fusion. Section~\ref{sec:evaluation} describes our
%% evaluation methodology and present our experimental
%% analysis. Section~\ref{sec:related-work} discusses related work and
%% concludes the paper.

% The remainder of the paper is organized as follows. Section~\ref{sec:streamit}
% describes StreamIt and introduces our motivating example.
% Section~\ref{sec:cache-model} introduces our cache model for 
% reasoning about the performance of a streaming
% computation. Section~\ref{sec:cache-opt} describes our cache aware
% optimizations, and Section~\ref{sec:buffer} describes the 
% optimization enabled by fusion. Section~\ref{sec:evaluation} describes
% our evaluation methodology and present our experimental
% analysis. Sections~\ref{sec:related-work}~and~\ref{sec:conclusion}
% discuss related work and concludes the paper.



