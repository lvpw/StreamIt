\vspace{-5pt}
\section{Related Work}

Printz's ``signal flow graphs'' includes nodes that performed a
sliding window~\cite{printz91thesis}.  Printz presents an
implementation strategy translating the sharing requirement of data
parallelization of peeking filters into communication.  However, his
approach does not alter the steady-state of the graph to reduce
sharing to neighboring fission products, and it does not further alter
the steady-state to reduce sharing to under a given threshold.
Instead, Printz's technique parallelizes the communication required by
sharing via a pipelined sequence of transfers between neighboring
cores on the iWarp machine.  Finally, Printz did not implement his
strategy; instead his evaluation relied on an model of a parallel
architecture.

% In the Warp project, the AL language~\cite{tseng89thesis,tseng90} had
% a window operation for use with arrays.  The AL compiler targeted the
% Warp machine and translated loops into systolic computation.  The AL
% compiler did alter the blocking of distributed loop iterations to
% reduce the sharing requirement for the sharing of the sliding window
% among iterations of the loop.  However, this was in the context of an
% imperative language with parallel arrays and considered a different
% application class: matrix computation. 

The Brook language requires a programmer to explicitly represent the
communication of sliding windows by specifying that a filter reads
overlapping portions of the input. Liao et al. map Brook to multicore
processors by leveraging the affine partitioning
model~\cite{liao06brook}. However, from our understanding, they do not
block across steady-states of the application, meaning the application
needs to be described at a granularity coarse enough to block
iterations of the filter to be parallelized across cores. In other
words, the sizes of the streams between producer and consumer need to
be large enough to enable parallelization and blocking to reduce
sharing caused by sliding windows.  Our framework frees the programmer
from having to consider buffers and granularity, enabling portable,
reusable, and malleable code.

Stream reuse frameworks~\cite{stream-reuse} seek to optimize explicit
copying of data items to distinct streams.  Stream reuse reduces
memory bandwidth when multiple different filters read overlapping
sections of the same input (possibly on different iterations). Stream
reuse does explicitly capture stream-stencil reuse in the context of
Brook or StreamC/KernelC.  After a filter is data parallelized, a
stream reuse framework may capture the sharing caused by sliding
window input between product filters, however, the framework will not
introduce blocking across steady-states to reduce the sharing
requirement.

Other languages have included the notion of a sliding window.  The ECOS
graphs language allows actors to specify how many items are read but
not consumed~\cite{huang_ecos_1992}; the Signal language allows access
to the window of values that a variable assumed in the
past~\cite{le_guernic_signal--data_1986}; and the SA-C language
contains a two-dimensional windowing
operation~\cite{draper_compiling_2001}.  However, to the best of our
knowledge, translation systems for these languages do not utilize
sliding windows to improve parallelism and reduce inter-core
communication.

Our key contribution over previous work in both streaming computation
and traditional scientific frameworks is that we automatically adjust
the granularity of the computation in the presence of non-affine
dependencies to decrease inter-core communication, and that we give
expressions for calculating the destination(s) of data items.