\section{Introduction}

The domain of stream programs is important because it stands at the
intersection of trends in applications and architectures.  Stream
programming naturally represents applications such as audio, video,
digital signal processing, and data analysis; applications that are
increasingly prevalent as computing moves towards data-centric
applications and to the mobile and embedded space.  Also, by virtue of
their structure -- a graph of independent computational nodes (termed
{\it filters}) with explicit and regular communication -- stream
programs are a natural fit for exploiting coarse-grained parallelism
suitable for multicore architectures.  The interest in streaming
applications has spawned a number of streaming languages that target
the streaming domain, including StreamIt~\cite{streamitcc},
Brook~\cite{brook04}, Cg~\cite{cg03},
SPUR~\cite{spur05samos}, Spidle~\cite{spidle03}, Lime~\cite{lime10},
and SPL~\cite{spl09}.

In a stream program, each filter defines an atomic execution step that
repeats for many iterations; each execution step discards a number of
data items from the filter's input edge.  Often, a filter does not
discard all the data items that it read for the current execution
step, requiring these inspected (but not discarded) items for a future
iteration of the filter.  This type of filter is described as
performing a sliding window computation on its input. Sliding window
computations are prevalent in stream programs.  Examples of sliding
window computations include FIR filters; moving averages and
differences; error correcting codes; motion estimation; and network
packet inspection.  A recent study of a large streaming benchmark
suite written in the StreamIt programming language finds that 17 of
the 30 real-world benchmarks include at least one filter that performs
a sliding window computation~\cite{streamit-suite}.

A goal of stream programming is to directly expose to the software
layer the necessary information to enable automatic management of
coarse-grained parallelism.  Stream programs expose multiple forms of
parallelism; however, data parallelism is the most attractive.  Data
parallelism exists when a filter is stateless and can thus be
replicated.  Data parallelism provides load-balanced and limitless
parallelism (as long as input data is available).  A filter that is
stateful, and cannot be data-parallelized, becomes a limit to
parallelization scalability, as the work of that filter cannot be
divided; the most load-intensive stateful filter becomes a bottleneck.

Figure~\ref{fig:fir-nopeeking} shows how to implement a sliding window
FIR filter via state carried between iterations of a filter in the
form of a circular buffer.  This implementation is extremely difficult
for the compiler to analyze and reason about.  If the compiler cannot
precisely reason about the state of a filter, it will be prevented
from data-parallelizing the filter, and this state could represent the
parallelization bottleneck.  If a compiler cannot data-parallelize them,
sliding window computations would become the bottleneck in 11 of the
17 real-world benchmarks in the StreamIt Benchmark Suite that contain
sliding windows~\cite{streamit-suite}.

 Some programming languages (e.g., Brook, Lime,
StreamIt, and IBM SPL) go so far as to include idioms that directly
represent sliding window computation, allowing the programmer to
specify, for each filter, the size of the window and the number of
items discarded after an execution of the filter.
Figure~\ref{fig:fir-peeking} shows how language extensions of the
StreamIt programming language elegantly expose sliding windows for
compiler analysis and optimization.  The {\tt peek} and {\tt pop}
declarations of the {\tt work} function declare the number of items
inspected and the number of items dequeued, respectively, from the
 input edge for each firing of the filter.

This paper presents a compiler framework for data-parallelizing
filters that perform sliding window computations.  The framework is
enabled by languages that expose sliding windows so that the
properties of the sliding window can be calculated statically.
Data-parallelizing a filter is performed via a transformation termed
{\it fission} (verb form {\it fiss})~\cite{streamit-asplos}.  Fission
is the process of data-parallelizing a stateless filter by duplicating
the filter a certain number of ways, assigning duplicates to distinct
cores, and correctly distributed input data to and collecting output
data from the duplicates.  The duplicated filters are referred to as
{\it products}.  When a sliding window is present, fission is
accomplished by duplicating certain input items since they are
required by multiple products.  This duplication translates into
inter-core communication, a limiting factor for scalability when
targeting multicore architectures.

Previous approaches duplicate each input data item to all products,
with products ignoring (decimating) items that are not
needed~\cite{streamit-asplos}.  We will show that this strategy limits
scalability for multicores by requiring too much inter-core
communication.  In contrast, our strategy precisely routes each input
item to the minimal set of product filters that requires the item.
% Unlike previous work, our techniques are defined on
% multiple input and multiple output filters, removing the need to
% introduce synchronization filters that serialize data before and
% after the product filters.  

Our techniques operate on {\it static-rate} stream graphs, meaning
that the number of items produced, the number of items consumed, and
the number of items inspected by each filter can be determined
statically.  Because of this property, a steady-state schedule of
filter firings can be calculated that does not grow buffers and can be
executed indefinitely~\cite{lee87}.  Our techniques are conscious of
the spatial locality between producers and consumers.  Our framework
includes techniques that can determine when spatial locality can be
increased by altering the steady-state schedule.  When applicable, our
techniques can reduce the overall sharing (and thus inter-core
communication) requirement to below a threshold percent of the total
input communication for each sliding window filter that is
data-parallelized. 

\begin{figure}[t]
\centering
\subfigure[]{\includegraphics[width=3.3in]{figures/fir-nopeeking.pdf}\label{fig:fir-nopeeking}}
\subfigure[]{\includegraphics[width=3.3in]{figures/fir-peeking.pdf}\label{fig:fir-peeking}}
\caption[Two implementations of an FIR filter.]{\label{fig:fir-code}
  Two StreamIt implementations of an FIR filter:
   (a) the non-peeking version implemented via a
  stateful circular buffer; and (b) the peeking version. Only steady-state implementation is
  given.}
\vspace{-10pt}
\end{figure}

The framework presented is defined on an execution model that is
agnostic of source language.  For evaluation, we have implemented our
techniques in the context of the StreamIt compiler
infrastructure~\cite{gordon-asplos06}.  Our transformations are guided
by the parallelization management techniques presented
in~\cite{gordon-asplos06}.  We employ 3 real-world benchmarks that
include sliding window computation from the StreamIt Benchmark
Suite~\cite{streamit-suite}.  We demonstrate the effectiveness of our
techniques by comparing them to previously published techniques on two
multicore architectures: a 16-core SMP shared-memory multicore and the
64-core distributed-memory Tilera TILE64.  We show that our techniques
are required to achieve scalable parallelization on both
architectures, achieving a 6.7x mean speedup on the 16-core SMP and a
1.8x mean speedup on the 64-core TILE64 over a
previously published technique.

% This paper makes the following contributions:
% \begin{itemize}
%   % \myitem{Motivation for Exposing Sliding Windows in Stream
%   %   Languages}: Without exposing sliding windows in the language, it
%   % requires heroic effort by the compiler to analyze the access patterns
%   % of such a filter. Without success, the compiler will not be able to
%   % data-parallelize these filters.  This will prevent robust 
%   % parallelization scalability for streaming applications.

%   \myitem{Generalized Fission of Sliding Window Filters}: We present a
%   transformation that fisses sliding window filters with multiple
%   input and multiple outputs.  The technique also supports filters
%   that with multiple schedules of execution.  General fission defines
%   a precise pattern of communication of input data to the products
%   that can be reasoned about by our other techniques.

%   \myitem{Sharing Reduction}: We are the first to present a technique
%   that decides when it is possible to decrease the amount of sharing
%   between fission products by altering the steady-state of a stream
%   graph, thus decreasing inter-core communication.  The technique
%   considers each sliding window filter of the stream graph,
%   and when possible, reduces the sharing requirement to below a given
%   threshold percent of the total input of the filter. 

%   \myitem{Data Parallelization of Stream Graph}: We present a
%   framework for data-parallelizing all of the filters of a stream
%   graph employing the fission transformation on individual filters and
%   applying sharing reduction when possible.  This framework optimizes
%   for spatial locality and enables the compiler to automatically and
%   effectively manage parallelization across varying multicore
%   architectures.

%   \myitem{Enable Robust Parallelization Scaling for Multicores}: For
%   streaming applications with sliding window computation, previously
%   published data-parallelization transformations do not scale for our
%   target multicores. Our techniques enable robust parallelization
%   scalability by reducing inter-core communication.  We achieve a 17x
%   mean parallelization speedup for a 16-core SMP and a 62.3x mean
%   parallelization speedup for the 64-core TILE64 across our benchmarks.

%\end{itemize}

% For sliding window filters, some (or all) input items are required by
% multiple iterations.  Without explicit support for sliding window
% computations, shared items must be manually saved for a later
% iteration.  Figure~\ref{fig:fir-nopeeking} demonstrates a manual
% implementation of a sliding window.  This implementation is very
% difficult for a compiler to analyze and parallelize.  Obscuring the
% data parallelism in sliding window computations limits parallelization
% scalability.  For our benchmarks, ChannelVocoder, Filterbank, and
% FMRadio, stateful implementations of sliding window filters limits
% theoretical parallelization scalability to 18, 37, and 14 cores, respectively.
% Figure~\ref{fig:fir-peeking} gives an implementation of an FIR filter
% that utilizes StreamIt's peeking idiom.  The behavior of the sliding
% window is described by the pop and peek rates.  The window is accessed
% via the {\tt peek(i)} expression.  Exposing sliding windows in this
% manner enables the techniques presented in this paper thus enabling
% scalable parallelization to at least 64-cores.

\section{Perspective on our Framework}

\begin{figure}[t]
\centering
\includegraphics[width=3.3in]{figures/early-example.pdf}
\caption{(a) Flexible skeleton implementation of a streaming
  application in an imperative language focusing on sliding window
  operation $f$. (b) The elements of {\tt f\_in} inspected by
  iterations of $f$ for the first two steady-states.
\label{fig:comparison}}
\vspace{-10pt}
\end{figure}

To appreciate the contributions of our framework, let us discuss the
program transformation in the context of an imperative language and a
traditional optimizing compiler.  Consider a sliding window filtering
operation $f$ embedded in a streaming application.  $f$ is repeatedly
called as long as input is available.  Figure~\ref{fig:comparison}(a)
shows a flexible skeleton implementation of a streaming application
with an embedded $f$.  An outer ({\tt while}) loop iterates as long as
input data is available; $f$ is wrapped in an inner ({\tt for}) loop
that executes for {\tt M} iterations.  The combination of these loops
captures that fact that it is often natural for a filter to operate on
a subset of inputs at a time (e.g., a frame, sampling window). An
array ({\tt f\_in}) implements the input edge of $f$.  Copy-shift
operations move items that are shared at the boundary of
steady-states. 

Figure~\ref{fig:comparison}(b) illustrates the sharing of input items
across executions of $f$ within a steady-state (an iteration of the
{\tt while} loop) an across steady-states.  Notice that within a
steady-state, iterations of $f$ read overlapping portions of {\tt
  f\_in}.  Traditional automatic parallelizing compilers will be able
to data parallelize the inner {\tt for} loop as there are no true
dependences between iterations (there are no writes to {\tt f\_in}).
Blocking could be performed to minimize communication, as
communication is introduced when successive iterations are mapped to
distinct cores.  

There is a problem if there are not enough iterations of $f$ within a
steady-state to spread across the cores of the machine and overcome
the cost of communicating the sharing. It would require heroic
compiler analysis and program understanding to parallelize and block
$f$ across iterations of the outer steady-state loop.  The granularity
of the implementation obscures this: {\tt f\_in} has a finite size and
the presence of the copy-shift dependences.  A super compiler would
have to (i) unroll the {\tt while} loop, (ii) perform array expansion
on {\tt f\_in} comprehending the copy-shift dependences, (iii)
redistribute the iterations counts of all loops (corresponding to all
filters).

An alternative implementation could implement {\tt f\_in} as a
circular buffer, eliminating the copy-shift operations. However, the
modulo operation of each array access would confuse a dependence
analysis framework.

As we will demonstrate, our techniques are able to parallelize and
block the inner scheduling loop across iterations of the outer
steady-state loop in the presence of non-affine dependences when the
application is represented in our streaming computation model.  The
transformation is performed by altering the steady-state loop
granularity appropriately.  This has the effect of expanding all edge
arrays, redistributing filter iterations, and recalculating data
routing.  Our framework also includes closed-form formulas for
calculating an appropriate granularity for the steady-state such that
sharing does not overwhelm the benefits of parallelization.  These
capabilities enable a programmer to write an application and its
filters at a natural granularity that is independent of
parallelization considerations enabling portable, reusable, and
malleable code (see Figure~\ref{fig:fir-peeking}).  Furthermore,
fine-grained implementations of filters enables the compiler to
perform guided filter fusion that balances data and instruction cache
behavior~\cite{sermulins-lctes05}.
