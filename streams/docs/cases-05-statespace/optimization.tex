\mysection{Optimization}
\label{sec:optimization}

We consider two types of optimizations.  The first is to remove
extraneous state variables from the linear state space representation.
This reduces the memory allocation for a program and reduces the
number of loads and stores executed, which are typically slow and
power-hungry operations. It also eliminates computations that involve
the removed states.  The second optimization is to reduce the
parametrization of a state space representation, by changing the
representation to one with more zero and one entries in its
matrices. This directly eliminates computations, as the compiler
statically evaluates $0 \cdot x = 0$ and $1 \cdot x = x$ rather than
performing a multiplication at runtime.

\mysubsection{State-Space Transformations}

For any state space equation pair, there are an infinite number of
transformations to an equivalent state space system.  These
transformations involve a change of basis of the state vector
$\vec{\mathbf{x}}$ to $\mathbf{T} \vec{\mathbf{x}}$, where
$\mathbf{T}$ is an invertible matrix. Consider the state update
equation $\vec{\dot{\mathbf{x}}} = \mathbf{A} \vec{\mathbf{x}} +
\mathbf{B} \vec{\mathbf{u}}$. Multiplying the entire equation by
$\mathbf{T}$ yields:
\starteqnstar
\mathbf{T} \vec{\dot{\mathbf{x}}} = \mathbf{TA} \vec{\mathbf{x}} +
\mathbf{TB} \vec{\mathbf{u}}
\doneeqnstar
Since $\mathbf{T}^{-1} \mathbf{T} = \mathbf{I}$, we can write:
\starteqnstar
\mathbf{T} \vec{\dot{\mathbf{x}}} & = & \mathbf{TA}
(\mathbf{T}^{-1} \mathbf{T}) \vec{\mathbf{x}} + \mathbf{TB}
\vec{\mathbf{u}} ~~=~~ \mathbf{TA}
\mathbf{T}^{-1} (\mathbf{T} \vec{\mathbf{x}}) + \mathbf{TB} \vec{\mathbf{u}} \\
\vec{\mathbf{y}} & = & \mathbf{C} (\mathbf{T}^{-1} \mathbf{T})
\vec{\mathbf{x}} + \mathbf{D} \vec{\mathbf{u}} ~~~~~\hspace{1.1pt}~=~~ \mathbf{C}
\mathbf{T}^{-1} (\mathbf{T} \vec{\mathbf{x}}) + \mathbf{D}
\vec{\mathbf{u}}
\doneeqnstar
where we have introduced the output equation as well. Let
$\vec{\mathbf{z}} = \mathbf{T} \vec{\mathbf{x}}$.
$\vec{\mathbf{z}}$ is a new state vector related to the old state
vector $\vec{\mathbf{x}}$ by the change of basis $\mathbf{T}$.
Substituting into the equations above yields:
\starteqnstar
\vec{\dot{\mathbf{z}}} & = & \mathbf{TA} \mathbf{T}^{-1} \vec{\mathbf{z}} + \mathbf{TB} \vec{\mathbf{u}} \\
\vec{\mathbf{y}} & = & \mathbf{C} \mathbf{T}^{-1}\vec{\mathbf{z}}
+ \mathbf{D}\vec{\mathbf{u}}
\doneeqnstar

This is precisely the original state space equation pair,
with $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ transformed to
$\mathbf{T} \mathbf{A} \mathbf{T}^{-1}$, $\mathbf{T} \mathbf{B}$,
and $\mathbf{C} \mathbf{T}^{-1}$, respectively.

For a StreamIt state space representation $\cal{R}$, we must
determine how the other values change. The initialization update
equation is analogous to the steady state update equation,
so $\mathbf{A_{pre}}$ and $\mathbf{B_{pre}}$ are transformed to
$\mathbf{T} \mathbf{A_{pre}} \mathbf{T}^{-1}$ and $\mathbf{T}
\mathbf{B}$, respectively. Since the old state vector
$\vec{\mathbf{x}}$ is multiplied by $\mathbf{T}$, the old initial
state vector is multiplied by $\mathbf{T}$. The number of states,
inputs, and outputs is the same, so $s$, $o$, and $u$ are unchanged.

\mysubsection{State Removal}
\label{sec:state-removal}

There are two types of states that can be removed from a state space
system without changing its behavior: unreachable and unobservable
states.  Informally, unreachable states are unaffected by inputs and
unobservable states have no effect on outputs.  If there are two
redundant states in a filter, then both might be reachable and
observable as the program is written.  However, following a series of
transformations, one of the redundant states can be converted to an
unreachable or unobservable state, allowing it to be removed.

More formally, the $i$th state is reachable if and only if at least
one of the following is true:
\begin{enumerate}
\item The $i$th state is initialized to a non-zero value.  That is,
the $i$th entry of $\overrightarrow{\mathbf{initVec}}$ is non-zero or
$\exists j~\mbox{s.t.}~\mathbf{B_{pre}}[i,j] \ne 0$.

\item The $i$th state directly depends on a steady-state input.  That
is, $\exists j~\mbox{s.t.}~\mathbf{B}[i,j] \ne 0$.

\item The $i$th state directly depends on another reachable state.
That is, $\exists j \ne i~\mbox{s.t.}~\mathbf{A}[i,j] \ne 0$ and $j$ is a
reachable state.
\end{enumerate}
All states in the system are either reachable or unreachable.
Unreachable states always have a value of zero, as they are
initialized to zero and are never updated by a non-zero value (i.e.,
by a reachable state or an input).  Therefore, unreachable states can
be removed from the state space representation, since they will have
no effect on any other states or output values.

The $i$th state is observable if and only if at least one of the
following is true:
\begin{enumerate}
\item An output directly depends on the $i$th state.  That is,
$\exists j~\mbox{s.t.}~\mathbf{C}[i,j] \ne 0$.

\item Another observable state directly depends on the $i$th state.
That is, $\exists j \ne i~\mbox{s.t.}~\mathbf{A}[j,i] \ne 0$ and $j$
is an observable state.
\end{enumerate}
All states in the system are either observable or unobservable.  The
unobservable states are not used to update the observable states and
are not used to determine the outputs.  Therefore, all unobservable
states can be removed from a representation (regardless of their
initial values).

A simple algorithm exists to refactor the states of a system and
expose the unreachable and unobservable states~\cite{Mayne}.  For
unreachable states, the algorithm assumes that there is no
initialization stage, i.e., that $\overrightarrow{\mathbf{initVec}}$
and $\mathbf{B_{pre}}$ are zero.  We first describe the basic
algorithm and then extend it to handle the initialization stage.

To detect unreachable states, the algorithm performs row
operations\footnote{\smaller Performing a row operation operation on a
matrix is equivalent to left-multiplying it by some invertible matrix,
while performing a column operation is equivalent to right-multiplying
by some invertible matrix.} on the augmented matrix $\left [
\begin{array} {cc} \mathbf{A} & \mathbf{B} \end{array} \right ]$.  To 
maintain the proper input/output relationship of the system,
corresponding inverse column operations are performed on $\mathbf{A}$
and $\mathbf{B}$.  The row operations achieve a special type of
row-eschelon form.  In this form, the last non-zero entry in each row
is a 1 (called the ending 1) and the ending 1 in a given row is to the
left of the ending 1 in lower rows.  Once the augmented matrix is in
the desired form, row $i$ represents an unreachable state if there are
no non-zero entries past the $i$th column.  This naturally expresses
the constraint that the $i$th state does not depend on any input
(columns of $\mathbf{C}$) or on any possibly reachable state (later
columns of $\mathbf{A}$).  In the absence of an initialization stage,
all unreachable states identified in this way can be removed from the
system.

For unobservbale states, the same procedure is applied to the
augmented matrix $\left [\begin{array} {cc} \mathbf{A}^T &
\mathbf{C}^T \end{array} \right ]$.  In the eschelon form, row $i$
represents an unobservable state if there are no non-zero entries past
the $i$th column.  Intuitively, the rows of the transposed matrices
represent how a given state is used, rather than how it is calculated.
The identified states are unobservable because they are used neither
in the calcluation of an output (colums of $\mathbf{C}^T$) or in
possibly observable states (later columns of $\mathbf{A}^T$).  All of
these unobservable states can be removed from the system (even if they
are assigned an initial value).

To handle the initialization stage, a minor extension is needed.  If a
state is assigned a non-zero value during initialization, either as a
constant (a non-zero entry in $\overrightarrow{\mathbf{initVec}}$) or
from the input (a non-zero entry in $\mathbf{B_{pre}}$), the state
must be considered reachable.  Further, any dependent states must also
be considered reachable.  This classification can easily be performed
as post-processing step on the set of candidate unreachable states
identified by the algorithm above.  If any candidate is initialized to
a non-zero value or directly depends (via the $\mathbf{A}$ matrix) on
a state outside the set, then the candidate is removed from the set.
When the set is as small as possible, it contains nothing but genuine
unreachable states.

%% The general algorithm for identifying unreachable states works as
%% follows.  First, it uses the previous algorithm~\cite{Mayne} to
%% identify {\it candidate} unreachable states.  Suppose that there are
%% $k$ candidates.  The candidate states are moved to the top of the
%% state vector via a series of row operations.  Following this
%% reordering, the sub-matrix $\mathbf{A}[1:k;1:k]$ represents the
%% dependences between candidate unreachable states.  As detailed below,
%% this sub-matrix is converted to an upper-triangular form (i.e., all
%% entries below the diagonal are zero) while maintaining the
%% input/output relationship of the system.  Finally, the candidate
%% states are considered in reverse order (from $k$ to $1$).  The $i$th
%% state is declared reachable if at least one of the following is true:
%% \begin{itemize}
%% \item The $i$th state is initialized to a non-zero value.

%% \item The $i$th state directly depends on another reachable state.
%% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mysubsubsection{Expanding the Scope}

So far we have considered optimizations that affect $\mathbf{A}$,
$\mathbf{B}$, and $\mathbf{C}$. Since the optimizations are entirely
the result of state transformations, they do not affect $\mathbf{D}$,
which is independent of the choice of state space basis. By storing
every input as a state, however, all the entries of $\mathbf{D}$ are
moved into $\mathbf{A}$ and can then be changed by state
optimizations.

We have already discussed how to store inputs as states. When every
input is stored as a state, the new state-equation pair is:
\starteqnstar
\left [ \begin{array} {c} \vec{\dot{\mathbf{x}}} \\
\vec{\dot{\mathbf{x_{in}}}} \end{array} \right ] & = & \left [
\begin{array} {cc} \mathbf{A} & \mathbf{B} \\ \mathbf{0} &
\mathbf{0} \end{array} \right ] \left [ \begin{array} {c}
\vec{\mathbf{x}} \\ \vec{\mathbf{x}}_{{\mathbf{in}}} \end{array} \right ]
+ \left [ \begin{array} {c} \mathbf{0} \\ \mathbf{I} \end{array}
\right ] \vec{\mathbf{u}} \\
\vec{\mathbf{y}} & = & \left [ \begin{array} {cc} \mathbf{C} &
\mathbf{D} \end{array} \right ] \left [ \begin{array} {c}
\vec{\mathbf{x}} \\ \vec{\mathbf{x}}_{{\mathbf{in}}} \end{array} \right ]
+ \mathbf{0} \vec{\mathbf{u}}
\doneeqnstar

These states should be added before state-removal is
performed. It may seem counter-intuitive that we first add states,
then seek to remove them. However, the added states represent
computations involving $\mathbf{D}$, which were not considered
before. Removing some of these states results in reducing
computations involving $\mathbf{D}$.

\mysubsection{Parameter Reduction}
\label{sec:parameter-reduction}

After removing as many states as possible, additional computations can
be eliminated by transforming the state space system to one with the
fewest number of non-zero, non-one entries (termed parameters). If
$\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are completely filled,
there are $s*(s+o+u)$ parameters. Ackermann and Bucy
\cite{Ackermann/Bucy} show a general form for $\mathbf{A}$ and
$\mathbf{C}$ ($\mathbf{B}$ is excluded) to have at most $s*(o+u)$
parameters, assuming there are no unobservable or unreachable
states. They derive this form using system impulse responses. We
achieve this same form using row operations on the augmented matrix
$\left [
\begin{array} {cc} \mathbf{A}^T & \mathbf{C}^T \end{array} \right
]$. The desired form is:
\starteqnstar
\mathbf{A}^T &=& \left [ \begin{array} {ccccc} \mathbf{L_1} &
\mathbf{A_{12}} & \mathbf{A_{13}} & ... & \mathbf{A_{1u}} \\
\mathbf{0} & \mathbf{L_2} & \mathbf{A_{23}} & ... &
\mathbf{A_{2u}} \\ \mathbf{0} & \mathbf{0} & \mathbf{L_3} & ... &
\mathbf{A_{3u}} \\ ... & ... & ... & ... & ... \\ \mathbf{0} &
\mathbf{0} & \mathbf{0} & ... & \mathbf{L_u} \end{array} \right ] \\ ~ \\
\mathbf{C}^T &=& \left [ \begin{array} {ccccc} 1 & 0 & 0 & ... &
0 \\ 0 & 0 & 0 & ... & 0 \\ ... & ... & ... & ... & ... \\ 0 & 1 &
0 & ... & 0 \\ 0 & 0 & 0 & 0 & 0 \\ ... & ... & ... & ... & ... \\
0 & 0 & 0 & ... & 1 \end{array} \right ]
\doneeqnstar

The matrices $\mathbf{A_{ij}}$ are rectangular, and the matrices
$\mathbf{L_i}$ are square, but do not necessarily have the same
dimensions as each other. These matrices have the form:
\starteqnstar
\mathbf{A_{ij}} = \left [ \begin{array} {cccc} \hspace{-1pt}0 & 0 & ... & *
\hspace{-1pt}\\\hspace{-1pt} ... & ... & ... & ... \hspace{-1pt}\\\hspace{-1pt} 0 & 0 & ... & *\hspace{-1pt} \end{array} \right ] ~~
\mathbf{L_i} = \left [ \begin{array} {ccccc} \hspace{-1pt}0 & 0 & ...
& 0 & * \hspace{-1pt}\\\hspace{-1pt} 1 & 0 & ... & 0 & * \hspace{-1pt}\\\hspace{-1pt} 0 & 1 & ... & 0 & * \hspace{-1pt}\\\hspace{-1pt}
... & ... & ... & ... & ... \hspace{-1pt}\\\hspace{-1pt} 0 & 0 & ... & 1 & *\hspace{-1pt} \end{array}
\right ]
\doneeqnstar

The entries marked with a * are the parameters of the system.  This is
known as the observable canonical form of the system. In contrast, the
reachable canonical form defines $\mathbf{A}$ and $\mathbf{B}$ instead
of $\mathbf{A}^T$ and $\mathbf{C}^T$, and $\mathbf{C}$ may be filled
with parameters instead of $\mathbf{B}$.

Figure~\ref{fig:param} gives pseudocode for a a simple algorithm to
attain the form above.  The pseudocode does not include the necessary
inverse column operations that must go with all row operations.

\newcommand{\IND}{\begin{ALC@g}}
\newcommand{\UND}{\end{ALC@g}}

\begin{figure}[t]
\hspace{-0.05in}\parbox{3.2in}{
{\bf Reduce\_Parameters}(\mbox{$A, C$}) \{
\begin{algorithmic}
\STATE - $\mt{currRow} = 0$; \\ \vspace{6pt}
\STATE - $\mt{colA} = 0$; \\ \vspace{6pt}
\STATE - $\mt{colC} = 0$; \\ \vspace{6pt}
\STATE {\bf while} $(\mt{currRow} < \mt{totalRows})$ \{
\IND
\STATE - \parbox[t]{2.85in}{Find a non-zero entry in column $\mt{colC}$ at or below row $\mt{currRow}$ of $C^T$, and swap it with the entry in row $\mt{currRow}$} \\ \vspace{6pt}
\STATE - \parbox[t]{2.85in}{Set $C^T[\mt{currRow}, colC] = 1$ by scaling the row appropriately; make all entries above and below it zero by adding appropriate multiple of row $\mt{currRow}$ to other rows} \\ \vspace{6pt}
\STATE - $\mt{currRow} = \mt{currRow} + 1$ \\ \vspace{6pt}
\STATE - $\mt{colC} = \mt{colC} + 1$ \\ \vspace{6pt}
\STATE {\bf do} \{  \\ \vspace{6pt}
\IND
\STATE - \parbox[t]{2.75in}{Find a non-zero entry in column $\mt{colA}$ at or below row $\mt{currRow}$ of $A^T$, and swap it with the entry in row $\mt{currRow}$} \\ \vspace{6pt}
\STATE - \parbox[t]{2.75in}{Set $A^T[\mt{currRow}, colA] = 1$ by scaling the row appropriately; make all entries below it zero by adding appropriate multiple of row $\mt{currRow}$ to other rows} \\ \vspace{6pt}
\STATE - $\mt{currRow} = \mt{currRow} + 1$ \\ \vspace{6pt}
\STATE - $\mt{colA} = \mt{colA} + 1$ \\ \vspace{6pt}
\UND
\STATE \} {\bf while} a non-zero entry in column $\mt{colA}$ is found \\ \vspace{6pt}
\STATE - $\mt{colA} = \mt{colA}+1$ \\ \vspace{6pt}
\UND
\STATE \} \\ \vspace{6pt}
\end{algorithmic}
\}
} % end parbox
%% \begin{verbatim}
%% Reduce Parameters {
%%   currRow = 0; colA = 0; colC = 0;

%%   while(currRow < totalRows) {

%%    -find a non-zero entry in column colC at or below row currRow 
%%     of C{transpose}, and swap it with the entry in row currRow;

%%    -set C{transpose}[currRow,colC] = 1 by scaling the row appropriately;
%%     make all entries above and below it zero by adding appropriate 
%%     multiple of row currRow to other rows;

%%     currRow = currRow + 1;
%%     colC = colC + 1;

%%     do {
%%      -find a non-zero entry in column colA at or below row currRow 
%%       of A{transpose}, and swap it with the entry in row currRow;

%%      -set A{transpose}[currRow,colA] = 1 by scaling the row appropriately;
%%       make all entries below it zero by adding appropriate multiple 
%%       of row currRow to other rows;

%%       currRow = currRow + 1;
%%       colA = colA + 1;
%%     } while a non-zero entry in column colA is found

%%     colA = colA + 1;
%%   }
%% }
%% \end{verbatim}
\vspace{-6pt}
\caption{Algorithm for parameter reduction. \protect\label{fig:param}}
\end{figure}

    It is possible that one type of form has fewer parameters than the
other. Therefore, we perform the above algorithm on $\left [
\begin{array} {cc} \mathbf{A}^T & \mathbf{C}^T
\end{array} \right ]$ as noted to produce the observable form, and on $\left [
\begin{array} {cc} \mathbf{A} & \mathbf{B} \end{array} \right
]$ to produce the reachable form, and check which one has fewer
parameters.

\mysubsubsection{Staged Execution}

Using input state variables corresponds to executing a state space
block in two stages:
\begin{enumerate}
\vspace{\itemshrink} \item Put inputs into input state variables.

\vspace{\itemshrink} \item Execute the original block, using input states instead of
actual inputs.
\vspace{\itemshrink} \end{enumerate}

We can add additional stages by having multiple sets of input
states---$\vec{\mathbf{x}}_{{\mathbf{in1}}}$,
$\vec{\mathbf{x}}_{{\mathbf{in2}}}$, etc. The first set is saved in
the second set, the second set is saved in the third set, etc.
Suppose there are $k$ input sets. We can write our state space
equation pair as follows: \starteqnstar \left [\hspace{-1.4pt}
\begin{array} {c} \vec{\dot{\mathbf{x}}} \\
\vec{\dot{\mathbf{x}}}_{{{\mathbf{ink}}}} \\ ... \\
\vec{\dot{\mathbf{x}}}_{{{\mathbf{in2}}}} \\
\vec{\dot{\mathbf{x}}}_{{{\mathbf{in1}}}}
\end{array} \hspace{-1.4pt} \right] &\hspace{-6pt}=\hspace{-6pt}& \left [\hspace{-1.4pt} \begin{array} {ccccc}
\mathbf{A} & \mathbf{B} & \mathbf{0} & ... &
\mathbf{0} \\ \mathbf{0} & \mathbf{0} & \mathbf{I} & ... & \mathbf{0} \\
... & ... & ... & ... & ... \\ \mathbf{0} & \mathbf{0} &
\mathbf{0} & ... & \mathbf{I} \\ \mathbf{0} & \mathbf{0} &
\mathbf{0} & ... & \mathbf{0} \end{array} \hspace{-1.4pt} \right] \left [\hspace{-1.4pt}
\begin{array} {c} \vec{\mathbf{x}} \\ \vec{\mathbf{x}}_{{\mathbf{ink}}} \\ ...
\\ \vec{\mathbf{x}}_{{\mathbf{in2}}} \\ \vec{\mathbf{x}}_{{\mathbf{in1}}} \end{array} \hspace{-1.4pt} \right]
+ \left [\hspace{-1.4pt} \begin{array} {c} \mathbf{0} \\ \mathbf{0} \\ ... \\
\mathbf{0} \\ \mathbf{I} \end{array} \hspace{-1.4pt} \right]
\vec{\mathbf{u}} \\
\vec{\mathbf{y}} &\hspace{-6pt}=\hspace{-6pt}& \left [\hspace{-1.4pt} \begin{array} {ccccc} \mathbf{C} &
\mathbf{D} & ... & \mathbf{0} & \mathbf{0} \end{array} \hspace{-1.4pt} \right]
\left [\hspace{-1.4pt} \begin{array} {c} \vec{\mathbf{x}}
\\ \vec{\mathbf{x}}_{{\mathbf{ink}}} \\ ... \\ \vec{\mathbf{x}}_{{\mathbf{in2}}}
\\ \vec{\mathbf{x}}_{{\mathbf{in1}}} \end{array} \hspace{-1.4pt} \right] + \mathbf{0} \vec{\mathbf{u}}
\doneeqnstar

By itself, executing the work of a filter in stages does not result in
any gain in performance. However, minimally parameterizing the
resulting system may be more productive than minimally parameterizing
the one- or two-stage system.  The canonical forms of the previous
section do not in general minimally parameterize the system; hence
evaluating staged execution remains an area of future research.
