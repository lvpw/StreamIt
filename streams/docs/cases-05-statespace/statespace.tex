\mysection{State Space Analysis}
\label{sec:statespace}

Our analysis operates on a symbolic representation of linear state
space filters.  We analyze the code of each StreamIt filter to
determine whether or not it is state space; if so we initialize a data
structure, fill it with the appropriate values through a process
called \emph{extraction}, and associate the structure with the filter.
We provide a set of rules to combine state space representations of
filters in hierarchical StreamIt blocks---pipelines, splitjoins, and
feedback loops. Such a process results in a single state space
representation for the entire block.  We also describe how to
\emph{expand} a representation so that it can be combined with blocks
of mis-matching dimensions.

\mysubsection{Representation}

Our first task is to create a data structure that fully captures the
state space representation of a StreamIt filter.  We save a filter's
number of states, push rate, and pop rate in variables termed $s$,
$u$, and $o$, respectively. Our data structure also contains the
matrices $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$, and $\mathbf{D}$
with dimensions $s \times s$, $s
\times o$, $u \times s$, and $u \times o$, respectively. The
inputs to a filter are denoted by $\vec{\mathbf{u}}$ (length $o$), the
outputs by $\vec{\mathbf{y}}$ (length $u$), and the states by
$\vec{\mathbf{x}}$ (length $s$). Upon every execution of the filter,
we can update the state vector by the
formula $\vec{\dot{\mathbf{x}}} = \mathbf{A}\vec{\mathbf{x}} +
\mathbf{B}\vec{\mathbf{u}}$ and calculate the outputs by the formula $\vec{\mathbf{y}} =
\mathbf{C}\vec{\mathbf{x}} +
\mathbf{D}\vec{\mathbf{u}}$.  For convenience, we calculate the filter 
outputs before updating the
state vector. Since the states may have initial values other than
zero, we store these values as the vector
$\overrightarrow{\mathbf{initVec}}$ (length $s$).

Since we have not included a constant term in our model, we always set
one of the state variables to be the constant $1$. This variable is
not updated by any of the inputs or states (besides itself), and its
initial value is $1$, so it always remains that value. Any state or
output that depends on a constant term can now refer to a multiple of
the constant state variable instead.

As long as a filter's peek rate (denoted by $e$) equals its pop rate,
the data structure as currently described can fully represent the
filter. We must make additional modifications for a filter with a peek
rate greater than its pop rate. Note that such a filter still removes
$o$ items from its input tape upon every execution, but it accesses
$e-o$ additional items on its input tape. Therefore, our current data
structure would work as long as there is some way to access these
additional items.

We solve the problem of having a peek rate greater than a pop rate by
storing $e-o$ items from the input tape in the state vector
$\vec{\mathbf{x}}$.  Thus, when a filter executes, it can access all
$e$ items it needs: $o$ items from its input vector and $e-o$ items
from its state vector. These $e-o$ states must be updated by the
inputs and themselves---the specifics are covered in the next
section. We store the number of states used for inputs as the variable
$\mt{stored}$. This will be useful when combining representations.

When a filter is executed for the first time, it has access to the $o$
items in the input vector, but the $e-o$ states it needs have yet to
be copied from the input.  Therefore, we need to initialize the state
vector before iteratively computing the output and state update
equations.  We introduce a new matrix $\mathbf{B_{pre}}$ to perform
this initialization. Before the filter runs, it performs the state
update $\vec{\dot{\mathbf{x}}} =
\overrightarrow{\mathbf{initVec}} +
\mathbf{B_{pre}}\vec{\mathbf{u}}_\mathbf{pre}$. The initialization input
vector $\vec{\mathbf{u}}_\mathbf{pre}$ has length $o_{pre} =
e-o$. For now, $o_{pre}$ and $\mt{stored}$ have the same value, but
combining filters might result in $o_{pre}$ being greater than
$\mt{stored}$.

Putting these pieces together, a full representation consists of the
push and pop rates, the number of state variables, the number of
stored inputs, the four state matrices, an initial state vector, and
possibly an initialization state matrix and an initial pop rate. We
define a state space representation $\cal{R}$ as the tuple
$\langle$$u$, $o$, $s$, $\mt{stored}$, $\mathbf{A}$, $\mathbf{B}$,
$\mathbf{C}$, $\mathbf{D}$, $\overrightarrow{\mathbf{initVec}}$,
$\mathbf{B_{pre}}$, $o_{pre}$$\rangle$. When we introduce a
representation ${\cal R}_i$, each of its values in the ordered set
will be denoted with the index $i$ (for example $u_i$,
$\mathbf{A_i}$). For representations of filters that do not need the
initialization matrix, we write $\mathbf{B_{pre}} = \mt{null}$ and
$o_{pre} = 0$. In this case, the filter does not have any stored
inputs, so $\mt{stored} = 0$ as well.

Representations are initially created from StreamIt filters and
ultimately converted back to StreamIt filters. Between these steps,
however, representations of hierarchical StreamIt blocks can be
derived by combining the representations of their parts.  Thus, from
now on we will say that a representation refers to a block rather than
a filter. The exception is in the following section, where we discuss
how to create a representation from a StreamIt filter.
% Hence we explicitly refer to a filter rather than block
%representation in that section.

\mysubsection{Extraction}

We use a simple dataflow analysis to extract a state space
representation from the imperative code in a filter's work function.
While an alternate approach would be to allow the user to specify the
representation explicitly (as part of the program), StreamIt aims to
provide a unified development environment in which diverse filters are
implemented using a small set of primitives.  State space filters are
simple to implement using imperative StreamIt constructs, and in this
form they are immediately readable by programmers unfamiliar with the
state space formalism.

\looseness+1 Our dataflow analysis symbolically executes a single iteration of a
filter's work function, maintaining a vector pair representation for
each local variable and filter field that is encountered (together,
these are termed program variables). If the outputs and fields (i.e.,
states) all have vector pair representations, then the filter is
linear state space, and the vectors are used as rows of $\mathbf{A}$,
$\mathbf{B}$, $\mathbf{C}$, and $\mathbf{D}$.  Of course, many filters
do not fit the state space model; the optimizations developed in this
paper are selectively applied to the portions of the stream graph that
contain state space filters.
%See \cite{Lamb}
%for a treatment of the linear case.

\looseness+1 We attempt to find a vector pair ($\vec{\mathbf{v}}$,$\vec{\mathbf{w}}$) 
for each program variable $p$ such that $p = \vec{\mathbf{v}} \cdot
\vec{\mathbf{x}} + \vec{\mathbf{w}} \cdot \vec{\mathbf{u}}$, where  $\vec{\mathbf{x}}$ 
is the filter's state vector and $\vec{\mathbf{u}}$ is the filter's
input vector .  When $p$ is on the left hand side of an assignment
statement, terms from the right hand side are identified as states
(corresponding to entries of $\vec{\mathbf{x}}$) and inputs
(corresponding to entries of $\vec{\mathbf{u}}$).  The coefficients
from terms that match are used to fill the corresponding entries in
$\vec{\mathbf{v}}$ and $\vec{\mathbf{w}}$, as long as they are
constants. If any coefficient is not a constant, then $p$ is
non-linear.

The input vector $\vec{\mathbf{u}}$ is defined as $[\mt{peek}(e-o)
~\mt{peek}(e-o+1) ~... ~\mt{peek}(o-1)]$. The state vector
$\vec{\mathbf{x}}$ holds $e-o$ variables from the input tape
($\mt{peek}(0) ~... ~\mt{peek}(e-o-1)$), every filter field, and a
variable for the constant 1. We do not consider local variables for
the state vector, because their values are not saved across filter
executions.
%Therefore, their values should be resolved to
%constants at compile time. 
A filter field has the initial vector pair ($\left [ \begin{array}
{ccccc} 0 & ... & 1 & ... & 0 \end{array} \right ]$,
$\vec{\mathbf{0}}$), where the 1 corresponds to the field
itself.

If a vector pair is found for a given program variable $p$, then $p$
can be written as a linear combination of the inputs and state
variables, with the vector pair entries representing the weights.  The
final assignment to state variable $x_i$ by some program variable
$p_k$ indicates that the $i$th rows of $\mathbf{A}$ and $\mathbf{B}$
should be $\vec{\mathbf{v}}_\mathbf{k}$ and
$\vec{\mathbf{w}}_\mathbf{k}$, respectively. Similarly, if the $i$th
push statement uses program variable $p_k$, then the $i$th rows of
$\mathbf{C}$ and $\mathbf{D}$ should be $\vec{\mathbf{v}}_\mathbf{k}$
and $\vec{\mathbf{w}}_\mathbf{k}$, respectively.
% NOT TRUE!  Should be 1 for the constant itself
%For the constant state
%variable $1$, the corresponding rows of $\mathbf{A}$ and $\mathbf{B}$
%are all zeros.

We use the same procedure in the init function to find the initial
values for each filter field.  However, we do not need a vector
$\vec{\mathbf{w}}$ for the inputs, since there are no inputs to the
init function. The initial value for each stored input is zero, and
the initial value for the constant state is $1$.

Finally, consider the stored input states (call them
$\vec{\mathbf{x}}_\mathbf{s}$). They are updated by the inputs;
however, if $\mt{stored} > o$, then some of the input states must be
updated by other input states. In particular, the first $\mt{stored}-o$
input states are updated by the last $\mt{stored}-o$ input states, and the
remaining $o$ input states are updated by the $o$ inputs. The update
is described by the equation:
\begin{eqnarray}
\label{eq:one}
\vec{\dot{\mathbf{x}}}_\mathbf{s} = \left [
\begin{array} {cc} \mathbf{0} & \mathbf{I} \\ \mathbf{0} &
\mathbf{0} \end{array} \right ] \vec{\mathbf{x}}_\mathbf{s} + \left [
\begin{array} {c} \mathbf{0} \\ \mathbf{I} \end{array} \right ]
\vec{\mathbf{u}}
\end{eqnarray}

We also create an initialization matrix to put values from the input
tape into the input states:
\starteqnstar
\vec{\dot{\mathbf{x}}}_\mathbf{s} = \mathbf{0} +
\mathbf{I} \vec{\mathbf{u}}_\mathbf{pre}
\doneeqnstar

%Stored inputs are updated as shown on every execution step.
%Therefore, we use $\mathbf{A_s}$ and $\mathbf{B_s}$ to describe this
%update, where the values of these two matrices are given in
%Equation~\ref{eq:one}.

\newpage \mysubsubsection{Extraction Example}
\enlargethispage{-1\baselineskip}

Consider another IIR filter.  Unlike the example in
Section~\ref{sec:background}, this filter uses peeking to read
elements from the tape without consuming them.
\begin{singlespace}
\small
\begin{verbatim}
float->float filter IIR() {
    float curr;  // example of a filter field
    work push 1 pop 1 peek 3 {
      float temp; // example of a local variable
      temp = (peek(0) + peek(1) + peek(2))/6;
      curr = temp + curr/2;
      push(curr);
      pop();
    }
}
\end{verbatim}
\end{singlespace}
\vspace{-16pt}

\noindent The state vector is $\left [\hspace{-3.2pt}\begin{array} {c} \mt{peek}(0) \\ \mt{peek}(1) \\ curr \\ 1 \end{array} \hspace{-3.2pt}\right]$; the input vector is $\left [\hspace{-3.2pt}\begin{array} {c} \mt{peek}(2) \end{array} \hspace{-3.2pt}\right]$. The first program
variable encountered is $\mt{temp}$. It is given the vector pair
($\left [\begin{array} {cccc} 1/6 & 1/6 & 0 & 0 \end{array} \right ]$, $\left [ \begin{array} {c} 1/6 \end{array} \right ]$). The
variable $\mt{curr}$, as a state variable, has an initial vector pair:
($\left [\begin{array} {cccc} 0 & 0 & 1 & 0 \end{array} \right ]$, $\left [ \begin{array} {c} 0 \end{array} \right ]$). 
When $\mt{curr}$ is found in an assignment
statement, it is given a new vector pair, constructed as the vector
pair for $\mt{temp}$ plus $1/2$ times the old vector pair for $\mt{curr}$:
($\left [ \begin{array} {cccc} 1/6 & 1/6 & 1/2 & 0 \end{array}
\right ]$, $\left [ \begin{array} {c} 1/6 \end{array} \right ]$). 
The output is $\mt{curr}$, so it is given the same vector
pair. The final pair for $\mt{curr}$ represents its state update. The
stored inputs $\mt{peek}(0)$ and $\mt{peek}(1)$ are updated as in
Equation~\ref{eq:one}, and the constant $1$ updated to itself. Therefore,
we have:

\begin{minipage}{1.6in}
\starteqnstar
\mathbf{A} = \left [ \begin{array} {cccc} 0 & 1 & 0 & 0 \\ 0 &
0 & 0 & 0 \\ 1/6 & 1/6 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{array}
\right ]
\doneeqnstar
\end{minipage}
\begin{minipage}{1.4in}
\starteqnstar
\mathbf{B} = \left [ \begin{array} {c} 0 \\ 1 \\ 1/6 \\ 0
\end{array} \right ]
\doneeqnstar
\end{minipage}

\begin{minipage}{1.57in}
\starteqnstar
\mathbf{C} = \left [ \begin{array} {cccc} 1/6 & 1/6 & 1/2 & 0
\end{array} \right ]
\doneeqnstar
\end{minipage}
\begin{minipage}{1.4in}
\starteqnstar
\mathbf{D} = \left [ \begin{array} {c} 1/6 \end{array} \right ]
\doneeqnstar
\end{minipage}

\begin{minipage}{1in}
\starteqnstar
\overrightarrow{\mathbf{initVec}} = \left [ \begin{array} {c}
0 \\ 0 \\ 0 \\ 1 \end{array} \right ]
\doneeqnstar
\end{minipage}
\hspace{0.76in}
\begin{minipage}{1in}
\starteqnstar
\mathbf{B_{pre}} = \left [ \begin{array} {cc} 1 & 0 \\ 0 & 1
\\ 0 & 0 \\ 0 & 0 \end{array} \right ]
\doneeqnstar
\end{minipage} ~\\

The pop and push rates are both 1, and there are four states, so $o =
1$, $u = 1$, and $s = 4$.  There are two stored input states, so
$o_{pre} = 2$ and $\mt{stored} = 2$.

\mysubsection{Combination}

If all blocks within a given pipeline, splitjoin, or feedback loop
have state space representations, they can be combined into a single
representation using the rules developed in this section.  There are
two benefits to combining blocks.  First, combination can eliminate
redundant computations across blocks.  Second, combination exposes
optimization opportunities, as intra-block optimizations (described in
Section~\ref{sec:optimization}) can effectively be applied across
blocks by combining the blocks first.

\mysubsubsection{Pipeline}
\label{sec:pipeline}

Consider two blocks connected in a pipeline with representations
${\cal R}_1$ and ${\cal R}_2$.  We will derive the combined
representation of the two blocks, denoted by ${\cal R}$.  Suppose the
output rate of ${\cal R}_1$ equals the input rate of ${\cal R}_2$
($u_1 = o_2$). If this is not the case, we must expand one or both
blocks to have their input/output rates match ($u_1' =
o_{2}' = lcm(u_1,o_2)$). Block expansion is covered in
Section~\ref{sec:expansion}. Since the output of ${\cal R}_1$ (i.e.,
$\vec{\mathbf{y}}_\mathbf{1}$) is equivalent to the input of ${\cal
R}_2$ (i.e., $\vec{\mathbf{u}}_\mathbf{2}$), we can write:
\starteqnstar
\vec{\dot{\mathbf{x}}}_{{{\mathbf 1}}} & = & \mathbf{A_1}\vec{\mathbf{x}}_{{\mathbf 1}} + \mathbf{B_1}\vec{\mathbf{u}}_{{\mathbf 1}} \\
\vec{\dot{\mathbf{x}}}_{{{\mathbf 2}}} & = & \mathbf{A_2}\vec{\mathbf{x}}_{{\mathbf 2}} + \mathbf{B_2}\vec{\mathbf{y_1}} \\[1.5Ex]
\vec{\mathbf{y}}_{{\mathbf 1}} & = & \mathbf{C_1}\vec{\mathbf{x}}_{{\mathbf 1}} + \mathbf{D_1}\vec{\mathbf{u}}_{{\mathbf 1}} \\
\vec{\mathbf{y}}_{{\mathbf 2}} & = & \mathbf{C_2}\vec{\mathbf{x}}_{{\mathbf 2}} +
\mathbf{D_2}\vec{\mathbf{y}}_{{\mathbf 1}}
\doneeqnstar
Substituting for $\vec{\mathbf{y}}_{{\mathbf 1}}$ yields:
\starteqnstar
\vec{\dot{\mathbf{x}}}_{{{\mathbf 2}}} & = & \mathbf{A_2}\vec{\mathbf{x}}_{{\mathbf 2}} + \mathbf{B_2}(\mathbf{C_1}\vec{\mathbf{x}}_{{\mathbf 1}} + \mathbf{D_1}\vec{\mathbf{u}}_{{\mathbf 1}} \\
\vec{\mathbf{y}}_{{\mathbf 2}} & = & \mathbf{C_2}\vec{\mathbf{x}}_{{\mathbf 2}} +
\mathbf{D_2}(\mathbf{C_1}\vec{\mathbf{x}}_{{\mathbf 1}} +
\mathbf{D_1}\vec{\mathbf{u}}_{{\mathbf 1}})
\doneeqnstar
Which simplifies to:
\starteqnstar
\vec{\dot{\mathbf{x}}}_{{{\mathbf 2}}} & = & \mathbf{A_2}\vec{\mathbf{x}}_{{\mathbf 2}} + \mathbf{B_2}\mathbf{C_1}\vec{\mathbf{x}}_{{\mathbf 1}} + \mathbf{B_2}\mathbf{D_1}\vec{\mathbf{u}}_{{\mathbf 1}} \\
\vec{\mathbf{y}}_{{\mathbf 2}} & = & \mathbf{C_2}\vec{\mathbf{x}}_{{\mathbf 2}} +
\mathbf{D_2}\mathbf{C_1}\vec{\mathbf{x}}_{{\mathbf 1}} +
\mathbf{D_2}\mathbf{D_1}\vec{\mathbf{u}}_{{\mathbf 1}}
\doneeqnstar

Let $\vec{\mathbf{x}} = \left [ \begin{array} {c} \vec{\mathbf{x}}_{{\mathbf 1}} \\
\vec{\mathbf{x}}_{{\mathbf 2}} \end{array} \right ]$, $\vec{\mathbf{u}} =
\vec{\mathbf{u}}_{{\mathbf 1}}$ (the input to the entire pipeline), and
$\vec{\mathbf{y}} = \vec{\mathbf{y}}_{{\mathbf 2}}$ (the output of the entire
pipeline). The equations relating $\vec{\mathbf{x}}$,
$\vec{\mathbf{u}}$, and $\vec{\mathbf{y}}$ are:
\starteqnstar
\vec{\dot{\mathbf{x}}} & = & \mathbf{A}\vec{\mathbf{x}} + \mathbf{B}\vec{\mathbf{u}} \\
\vec{\mathbf{y}} & = & \mathbf{C}\vec{\mathbf{x}} + \mathbf{D}\vec{\mathbf{u}}
\doneeqnstar ~ \\ \vspace{-36pt} ~ \\
\starteqnstar
\mathbf{A} = \left [ \begin{array} {cc} \mathbf{A_1} &
\mathbf{0} \\ \mathbf{B_2}\mathbf{C_1} & \mathbf{A_2} \end{array} \right ] &~&
\mathbf{B} = \left [ \begin{array} {c} \mathbf{B_1} \\ \mathbf{B_2}\mathbf{D_1} \end{array} \right ] \\ ~&~&~ \\
\mathbf{C} = \left [ \begin{array} {cc} \mathbf{D_2}\mathbf{C_1} & \mathbf{C_2} \end{array} \right ] &~&
\mathbf{D} = \mathbf{D_2}\mathbf{D_1}
\doneeqnstar

The input to the pipeline is identical to the input to ${\cal R}_1$,
and the output of the pipeline is identical to the output of
${\cal R}_2$. Furthermore, the states of the pipeline are the states
of the first block appended to the states of the second
block. Thus, $u = u_2$, $o = o_1$, $s = s_1 + s_2$, and 
$\overrightarrow{\mathbf{initVec}} = \left [ \begin{array} {c}
\overrightarrow{\mathbf{initVec}_\mathbf{1}} \\ \overrightarrow{\mathbf{initVec}_\mathbf{2}}
\end{array} \right ]$.

If neither block has an initialization matrix, then the entire
pipeline does not need an initialization matrix, so $\mathbf{B_{pre}}
= \mt{null}$, $o_{pre} = 0$, and $\mt{stored} = 0$. If only the first block
has an initialization matrix, then we initialize the states in the
pipeline corresponding to the first block while keeping the states
corresponding to the second block unchanged:
\starteqnstar
~~~~ \mathbf{B_{pre}} = \left [ \begin{array} {c} \mathbf{B_{pre1}} \\ \mathbf{0} \end{array} \right ]
~~~~ o_{pre} = o_{pre1} 
~~~~ stored = stored_1
\doneeqnstar

If the second block has an initialization matrix, the first block must
run enough times to provide the necessary inputs to initialize the
second block. However, this might result in the first block providing
extra initial inputs to the second block. In that case, we must change
the representation of the second block to increase its number of
stored inputs.  A full description of this case appears
in~\cite[pp. 46-49]{Agrawal04}.

If there are more than two blocks in a pipeline, they can be collapsed
in the following manner: first combine the first two blocks to get one
block representation, then combine this representation with the third
block, and so on.

\mysubsubsection{Splitjoin and Feedback Loop}

The combination rules for splitjoins and feedback loops are somewhat
involved, and we omit them due to space considerations.  An important
benefit of a linear state space representation over a linear
representation is that feedback loops can be collapsed; the items on
the feedback path become states in the combined block.  A thorough
treatment of these cases appears in~\cite[pp. 36-43]{Agrawal04}.

\mysubsection{Expansion}
\label{sec:expansion}

Sometimes it is necessary to simulate multiple executions of a block
in order to combine it properly with other blocks.  For example,
suppose block $B_1$ outputs two items and block $B_2$ inputs five
items. In order to combine these blocks in a pipeline, $B_1$ must run
five times (in order to output ten items) and $B_2$ must run two times
(in order to input ten items). Therefore, a method is needed to expand
a representation so that it models a block running multiple times,
rather than once.

Consider the state space equation pair, where $\vec{\mathbf{u}}_{{\mathbf 1}}$ and
$\vec{\mathbf{y}}_{{{\mathbf 1}}}$ are the first set of inputs and outputs, and
$\vec{\mathbf{x}}$ is the original state vector:
\starteqnstar
\vec{\dot{\mathbf{x}}} & = & \mathbf{A}\vec{\mathbf{x}} + \mathbf{B}\vec{\mathbf{u}}_{{\mathbf 1}} \\
\vec{\mathbf{y}}}_{{{\mathbf 1}} & = & \mathbf{C}\vec{\mathbf{x}} +
\mathbf{D}\vec{\mathbf{u}}_{{\mathbf 1}}
\doneeqnstar

If we run the block again, the equation pair in terms of the
original state vector $\vec{\mathbf{x}}$ and the next set of
inputs and outputs ($\vec{\mathbf{u}}_{{\mathbf 2}}$ and $\vec{\mathbf{y}}_{{\mathbf 2}}$)
is:
\starteqnstar
\vec{\dot{\mathbf{x}}} & = & \mathbf{A}(\mathbf{A}\vec{\mathbf{x}}
+
\mathbf{B}\vec{\mathbf{u}}_{{\mathbf 1}}) + \mathbf{B}\vec{\mathbf{u}}_{{\mathbf 2}} \\
\vec{\mathbf{y}}}_{{{\mathbf 2}} & = & \mathbf{C}(\mathbf{A}\vec{\mathbf{x}} +
\mathbf{B}\vec{\mathbf{u}}_{{\mathbf 1}}) + \mathbf{D}\vec{\mathbf{u}}_{{\mathbf 2}}
\doneeqnstar
\noindent Simplifying yields:
\starteqnstar
\vec{\dot{\mathbf{x}}} & = & \mathbf{A}^2\vec{\mathbf{x}} +
\mathbf{AB}\vec{\mathbf{u}}_{{\mathbf 1}} + \mathbf{B}\vec{\mathbf{u}}_{{\mathbf 2}} \\
\vec{\mathbf{y}}}_{{{\mathbf 2}} & = & \mathbf{CA}\vec{\mathbf{x}} +
\mathbf{CB}\vec{\mathbf{u}}_{{\mathbf 1}} + \mathbf{D}\vec{\mathbf{u}}_{{\mathbf 2}}
\doneeqnstar

Let $\vec{\mathbf{u}}$ denote the combined input vector ($\vec{\mathbf{u}}
= \left [ \begin{array} {c} \vec{\mathbf{u}}_{{\mathbf 1}}
\\ \vec{\mathbf{u}}_{{\mathbf 2}} \end{array} \right ]$) and $\vec{\mathbf{y}}$ denote the 
combined output vector ($\vec{\mathbf{y}} = \left [ \begin{array} {c}
\vec{\mathbf{y}}_{{\mathbf 1}}
\\ \vec{\mathbf{y}}_{{\mathbf 2}} \end{array} \right ]$). The representation in terms of these two
vectors is:
\starteqnstar
\vec{\dot{\mathbf{x}}} & = & \mathbf{A_2}\vec{\mathbf{x}} + \mathbf{B_2}\vec{\mathbf{u}} \\
\vec{\mathbf{y}} & = & \mathbf{C_2}\vec{\mathbf{x}} + \mathbf{D_2}\vec{\mathbf{u}}
\doneeqnstar ~ \\ \vspace{-36pt} ~ \\
\starteqnstar
\mathbf{A_2} = \mathbf{A}^2 ~~~~~
\mathbf{B_2} = \left [ \begin{array} {cc} \mathbf{AB} & \mathbf{B} \end{array} \right ]  \\
\mathbf{C_2} = \left [ \begin{array} {c} \mathbf{C} \\
\mathbf{CA} \end{array} \right ] ~~~~~
\mathbf{D_2} = \left [ \begin{array} {cc} \mathbf{D} & \mathbf{0} \\
\mathbf{CB} & \mathbf{D} \end{array} \right ]
\doneeqnstar
This new representation corresponds to a block that, upon every
execution, runs the old block twice.  By induction, a general formula
for running a block $n$ times is:
\starteqnstar
\mathbf{A_n} = \mathbf{A}^n ~~~~~
\mathbf{B_n} = \left [ \begin{array} {ccccc} \mathbf{A}^{n-1}
\mathbf{B} & \mathbf{A}^{n-2} \mathbf{B} & ...  & \mathbf{AB} &
\mathbf{B} \end{array} \right ] \\
\mathbf{C_n} = \left [ \begin{array} {c} \mathbf{C} \\
\mathbf{CA} \\
... \\
\mathbf{CA^{n-2}} \\
\mathbf{CA^{n-1}} \end{array} \right ]~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
\mathbf{D_n} = \left [ \begin{array} {ccccccc}
\mathbf{D} & \mathbf{0} & \mathbf{0} & ... & \mathbf{0} & \mathbf{0} \\
\mathbf{CB} & \mathbf{D} & \mathbf{0} & ... & \mathbf{0} & \mathbf{0} \\
\mathbf{CAB} & \mathbf{CB} & \mathbf{D} & ... & \mathbf{0} & \mathbf{0} \\
... & ... & ... & ... & ... & ... \\
\mathbf{CA}^{n-3} \mathbf{B} & \mathbf{CA}^{n-4} \mathbf{B} &
\mathbf{CA}^{n-5} \mathbf{B} & ... & \mathbf{D} & \mathbf{0} \\
\mathbf{CA}^{n-2} \mathbf{B} & \mathbf{CA}^{n-3} \mathbf{B} &
\mathbf{CA}^{n-4} \mathbf{B} & ... & \mathbf{CB} &
\mathbf{D} \end{array} \right ]
%% following version is same but with one extra column and row
%% \mathbf{D_n} = \left [ \begin{array} {ccccccc}
%% \mathbf{D} & \mathbf{0} & \mathbf{0} & ... & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
%% \mathbf{CB} & \mathbf{D} & \mathbf{0} & ... & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
%% \mathbf{CAB} & \mathbf{CB} & \mathbf{D} & ... & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
%% ... & ... & ... & ... & ... & ... & ... \\
%% \mathbf{CA}^{n-4} \mathbf{B} & \mathbf{CA}^{n-5} \mathbf{B} &
%% \mathbf{CA}^{n-6} \mathbf{B} & ... & \mathbf{D} & \mathbf{0} & \mathbf{0} \\
%% \mathbf{CA}^{n-3} \mathbf{B} & \mathbf{CA}^{n-4} \mathbf{B} &
%% \mathbf{CA}^{n-5} \mathbf{B} & ... & \mathbf{CB} & \mathbf{D} & \mathbf{0} \\
%% \mathbf{CA}^{n-2} \mathbf{B} & \mathbf{CA}^{n-3} \mathbf{B} &
%% \mathbf{CA}^{n-4} \mathbf{B} & ... & \mathbf{CAB} & \mathbf{CB} &
%% \mathbf{D} \end{array} \right ]
\doneeqnstar

Since initializations are not affected,
$\overrightarrow{\mathbf{initVec}}$, $\mathbf{B}_\mathbf{pre}$,
$\mt{stored}$, and $o_{pre}$ remain unchanged from the initial
representation. Since the number of states is not changed, $s$ remains
the same. The new representation runs the old representation $n$
times, so $u_{new} = n * u_{old}$ and $o_{new} = n * o_{old}$.

As mentioned in Section~\ref{sec:pipeline}, it is sometimes necessary
to simulate the initialization stage of a block (in addition to
simulating $n$ executions) for the purpose of initializing a full
pipeline.  In this case, the equations are very similar to above, but
also include terms for $\mathbf{B_{pre}}$.  Full details appear
in~\cite[pp. 45-46]{Agrawal04}.
