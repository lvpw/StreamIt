\section{Trace Extraction and Scheduling}
\label{sec:traces}
After synchronization removal, we are left with a flat stream graph
composed of filters.  Conceptually, we break away from the notion that
filters are single input and single output as all splitters and
joiners in the graph are removed.  At this point we are ready to
extract the traces from the graph and schedule them for execution.  To
better understand the design decisions for trace extraction and
scheduling it is necessary to present the idiosyncrasies of the
underlying, low-level implementation.

\subsection{Low-level Space-Time Implementation}
Currently, we have implemented a robust, though restricted, hybrid
space-time execution model.  Firstly and obviously, the number of
filters in a trace must be less than or equal to the number of tiles
in the Raw chip to which we are targeting.  Traces are homogeneous,
meaning that traces are composed of either completely linear or
completely non-linear filters.  Currently, traces are restricted to
straight pipelines of filters.  More specifically, only the end-points
of traces can communicate with more than one filter, inner filters are
restricted to single input and single output.  All intra-trace
communication is handled on-chip through Raw's static network.  An
upstream filter simply sends its output to its downstream filter that
is guaranteed to be mapped to a neighboring tile and so on.

Inter-trace communication is handled differently from intra-trace
communication.  As mentioned in Section \ref{section:raw}, in this
project we have configured the Raw chip to have a streaming DRAM
module attached to each I/O port of the chip.  It is the task of the
tile to send memory requests to these modules and communicate data to
and from the memory modules via the static network.  Inter-trace
communication utilizes these off-chip memory modules.  This means that
all trace end-point filters must be mapped to the border tiles of the
Raw chip.  These tiles directly communicate to the memory modules via
their I/O ports.  Trace end-points read their inputs from the memory
attached to the I/O of tile, expecting that the data has already been
correctly reordered if the filter has multiple inputs.  Trace
end-point filters write their outputs to the memory attached to the
I/O of the tile as the output is generated. For traces that
communicate to or from multiple traces, the static network is used to
perform the necessary data-reordering.

For traces that output to multiple traces, the data is read from the
memory in the order it was produced by the last filter of the trace
and communicated over the static network to the memory modules that
are connected to the tiles mapped to the first filter of the
downstream traces. The switch is programmed to correctly communicate
the data, adhering to the duplication and routing calculated by the
synchronization removal pass.

For traces whose input originates from more than one trace, a similar
mechanism is used.  Before the first filter of the trace is executed,
the input to the trace is gathered from the various memory modules
and written to the memory module attached to the tile to which the
first filter of the trace was mapped.  Again, the static network
switches are programmed to correctly order the data as calculated by
the synchronization removal stage.   

Restrictions are placed on the assignment of filters to tiles. From
above we can see that filter endpoints of traces must be mapped to the
border tiles of the Raw chip. We must also take special care when
dealing with application input and output streaming to and from I/O
ports on the Raw chip.  As mentioned in Section \ref{sec:raw} an I/O
port is multiplexed between its streaming memory module, an input
stream originating off-chip, and an output stream whose final
destination is off-chip.  Only one trace with an off-chip application
input stream can be mapped to a tile.  The same is true for a trace
that writes to an off-chip application output stream.  But a single
tile can accommodate both an off-chip application input stream and
output stream.

\subsubsection{Software Pipelining of Traces}

\section{Trace Extraction}
Trace extraction refers to the process of assigning each filter of the
stream graph to a trace.  As stated above, a trace is a contiguous
section of the stream graph that is scheduled for execution as a
group. The trace will occupy a portion of the chip as it executes and
then be swapped out when it completes execution.  Each filter in the
stream graph is a member of exactly one trace.

Our initial algorithm for trace extraction is rather straightforward
and a brief description of the algorithm should suffice.  Due to the
restrictions of the current low-level implementation, as we traverse
the stream graph, trace boundaries are produced {\it before} a filter
with multiple inputs and {\it after} a filter with multiple outputs,
thus restricting the traces to pipelines of filters. We introduce a
new trace when within a pipeline a non-linear filter directly
communicates to a linear filter and vice versa. Finally, as we are
adding filters to the trace, we must introduce a trace boundary if the
size of the trace is equal to the number of tiles in the Raw
configuration.

%%what about linear filters, are they restricted to being pipelines???

Additionally, we try to coax the generation of load-balanced traces.
For each filter, we calculate a static work estimation of the filter
based on an analysis of the {\tt work} function.  Because of the
static I/O (push, pop, and peek) rates in this version of StreamIt,
most loops within {\tt work} can be unrolled, allowing a close
approximation of the actual cycle count.  This estimate is multiplied
by the number of times the filter executes in the steady-state.

As we are adding filters to the trace, we compare the work estimation
of the current filter we are examining to the work estimation for the
first filter of the trace.  If the ratio of the work in the current
filter to the work in the first filter is within a predefined
threshold, termed the {\it work threshold}, then we proceed to add the
filter to the trace.  Otherwise, begin a new trace with the current
filter as the first filter in this new trace.  After experimentation,
we found that smaller, well-balanced traces are preferable.  The
results given in Section \ref{sec:results} were generating using a
work threshold of 0.80, meaning that each filter of the trace has a
workload that is within 80-125\% of the workload of the first filter
of the trace.

\section{Trace Scheduling}