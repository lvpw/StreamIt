\section{Implementation}
\label{sec:implementation}
After synchronization removal, we are left with a flat stream graph
composed of filters.  Conceptually, we break away from the notion that
filters are single input and single output as all splitters and
joiners in the graph are removed.  At this point we are ready to
extract the traces from the graph and schedule them for execution.  

%To better understand the design decisions for trace extraction and
%scheduling it is necessary to present the idiosyncrasies of the
%underlying, low-level implementation.

\subsection{Low-level Space-Time Implementation}
Currently, we have implemented a robust, though restricted, hybrid
space-time execution model.  Firstly and obviously, the number of
filters in a trace must be less than or equal to the number of tiles
in the Raw chip to which we are targeting.  Traces are homogeneous,
meaning that traces are composed of either completely linear or
completely non-linear filters.  Currently, traces are restricted to
straight pipelines of filters.  More specifically, only the end-points
of traces can communicate with more than one filter; inner filters are
restricted to single input and single output.  All intra-trace
communication is handled on-chip through Raw's static network.  An
upstream filter simply sends its output to its downstream filter that
is guaranteed to be mapped to a neighboring tile and so on.

Inter-trace communication is handled differently from intra-trace
communication.  As mentioned in Section \ref{section:raw}, in this
project we have configured the Raw chip to have a streaming DRAM
module attached to each I/O port of the chip.  It is the task of the
tile to send memory requests to these modules and communicate data to
and from the memory modules via the static network.  Inter-trace
communication utilizes these off-chip memory modules.  This means that
all trace end-point filters must be mapped to the border tiles of the
Raw chip.  These tiles directly communicate to the memory modules via
their I/O ports.  

The first filter of a trace reads its input from the memory attached
to the I/O port of its tile.  If a trace has multiple inputs, the data
is reordered prior to execution of the trace.  Before the first filter
of the trace is executed, the input to the trace is gathered from the
various memory modules and written to the memory module attached to
the tile to which the first filter of the trace was mapped.  The
static network switches are programmed to correctly order the data as
calculated by the synchronization removal stage.

The last filter of a trace writes its output to the memory attached to
the I/O of the tile as the output is generated. For traces that output
to multiple traces, the data is read from the memory in the order it
was produced by the last filter of the trace and communicated over the
static network to the memory modules that are connected to the tiles
mapped to the first filter of the downstream traces. Again, the switch
is programmed to correctly communicate the data, adhering to the
duplication and routing calculated by the synchronization removal
pass.

Restrictions are placed on the assignment of filters to tiles. From
above we can see that filter endpoints of traces are mapped to the
border tiles of the Raw chip. We must also take special care when
dealing with application input and output streaming to and from I/O
ports on the Raw chip.  As mentioned in Section \ref{sec:raw} an I/O
port is multiplexed between its streaming memory module, an input
stream originating off-chip, and an output stream whose final
destination is off-chip.  Only one trace with an off-chip, application
input stream can be mapped to a tile.  The same is true for a trace
that writes to an off-chip, application output stream.  But a single
tile can accommodate both an off-chip, application input stream and
output stream.

\subsubsection{Support for Software Pipelining of Traces}
\label{sec:softpipe}
Currently, the hybrid space-time multiplexing backend does not support
full software-pipelining of traces, in the tradition sense.  The trace
scheduling algorithm (see Section \ref{sec:scheduling}) is limited
when scheduling traces because of the underlying implementation.

The scheduler is free to schedule a trace for execution as long as in
the final schedule, for each trace $t$:
\begin{itemize}
\item all directly connected downstream traces of $t$ are either {\it all}
scheduled before or {\it all} scheduled after $t$.
\item all directly connected upstream traces of $t$ are either {\it all}
scheduled before or {\it all} scheduler after $t$.
\end{itemize}
These restrictions limited the freedom of the scheduler and we have
plans increase the flexibility of the implementation.

We must properly fill the input buffers of any trace that is scheduled
before its upstream neighbor.  This is analogous to the prologue of a
software-pipelined loop.  In the compiler we calculate a schedule of
traces that will perform this function.  This schedule is executed
once, prior to the steady-state execution.  We call this schedule the
{\it prologue schedule}; calculating the prologue schedule is
described in Section \ref{sec:prologue}.
