\section{Code Generation}
\label{sec:codegen}
In this section we consider low-level code generation for the Raw
microprocessor. Each Raw tile is an amalgam of the filters that
execute on it. To generate the execution code for the entire
application, we traverse the schedule for each phase of computation:
first the peek initialization stage, then prologue stage, and finally
the steady-state.  As we traverse each schedule, we generate the
computation and communication code as we visit each trace.  For the
initialization and stage stages, we traverse a breadth-first traversal
of the trace graph to maintain the data-dependencies stipulated by the
stream graph. For the steady-state we traverse the schedule as
calculated by the trace scheduling algorithm of Section
\ref{sec:scheduling}.

When we visit a trace, we first generate DRAM requests and
communication code to gather its input.  Next, we generate
computation and communication code for each filter of the trace.
Finally, we generate DRAM requests and communication code to scatter
its outputs.  The following sections investigates the steps
in more detail.

\subsection{Trace Data Reordering}
Before the first filter of a trace can execute, its single input
stream must be assembled in the order stipulated by the round-robin
weights on its incoming channels. To achieve this, the various input
streams of the traces are collected into the memory module attached to
the tile to which the first filter of the trace is mapped. Streaming
DRAM requests are generated to read each input channel and to write
the final, reassemble input.  Code is generated for the static
routers, as they enforce the ordering of the final input stream.

After the last filter of a trace executes, its single output stream is
split into memory ports for recollection into the memory ports of the
downstream filters (described in the paragraph above). The choice of
memory port for each {\it intermediate} stream is beyond the scope
of this paper. Streaming DRAM requests are generated to read
the output of the last filter and requests generated to write the
split (and possible duplicated) output to the memory ports assigned to
the intermediate streams.

\subsection{Filter Code Generation}
When we visit a trace in the schedule for each execution phase, we
iterate over the filters that compose the trace generating 
communication and computation code necessary to execute the trace.

\subsubsection{Compute Processor}
For linear traces, we generate parameterized, template assembly code
for both computation and communication as described in Section
\ref{sec:linear}.  As for non-linear traces we generate C code that is
compiled using Raw's GCC 3.3 port.  

ADD description!!

Consequently, each compute processor begins by calling the {\tt init}
function of all the filters mapped to it.  Next, it executes the {\tt
work} of each filter in the sequence determined by the traversal of
the schedule of each execution phase.  The translation of the {\tt
init} function, {\tt work} functions, and any helper function is, for
the large part, straightforward.  Most StreamIt expression have direct
analogs in C except for the channel expressions {\tt push()}, {\tt
peek(index)} and {\tt pop()}.

One major benefit of StreamIt over other streaming languages is that
the StreamIt compiler is responsible for input buffer management.  The
programmer is free to call {\tt pop()} or {\tt peek(index)} without
having to worry about the structure of the buffer or updating the
buffer's index variable after each execution of the work function.
Currently, we classify filters as one of three types with respect to
buffer management: 
\begin{enumerate}
\item If a filter $i$ does not contain any {\tt peek} statments, $ii$
does not contain any control-flow dependent channel expressions, and
$iii$ has peek rate equals to pop rate, then a buffer is not needed
and {\tt pop()} statements are translated directly into static network
receives.  There exist other initialization considerations which are
beyond the scope of this paper.
\item If a filter contains peek statements, but $peek = pop$ then we can
use a simple linear buffer that is reset after each work function execution.
\item Otherwise, when $peek > pop$, we use a circular buffer to
account for the data remaining on the channel after the work function
invocation.
\end{enumerate}
Each {\tt push()} statement is translated directly into a static
network send.  

\subsubsection{Switch Processor}
As mentioned in Section \ref{sec:traces}, individual traces are
limited to pipelines, thus we do not have to worry about

ADD something here...

The end-points of a trace send or receive their data to or
from off-chip.  We route the items off the side of the chip and
generate the necessary port multiplexing and/or DRAM instructions as
described above.

