\section{Results}
\label{sec:results}

Our compiler currently has two linear optimizations. The first, {\it
linear replacement}, transforms linear streams ({\it i.e.,} filters,
splitjoins and pipelines) by replacement with a filter that directly
implements the corresponding linear node.  The second optimization,
{\it frequency replacement}, converts a linear node into a filter that
performs the computation in the frequency domain.  This transformation
uses FFTW~\cite{frigo99fast}, an adaptive and high-performance FFT
library, to perform the necessary basis conversions.  Our optimization
selection algorithm automatically determines which transformations
should be applied.  Below we describe experiments and results that
demonstrate substantial performance improvements due to our methods.

\subsection{Measurement Methodology}
%We chose to measure the strength of our optimizations in terms of 
%floating point instruction reduction. The StreamIt compiler currently
%has two code generation backends. The uniprocessor backend generates sequential C code
%that can be compiled and linked against a supporting library. 
%There is also a backend that generates code for the RAW microprocessor
%\cite{waingold97baring, raw-micro}, which features
%a grid of processors interconnected via various communication facilities. 
%Mapping a StreamIt program on to the RAW architecture is complicated
%by issues of communication, load balancing and partitioning\cite{streamit-asplos}. 
%Therefore, the effects of our linear analysis optimizations can not
%be easily differentiated from differences in placement, routing and fusion that result
%from modifying the program's stream graph (as is the case for linear replacement). 
%Therefore we chose to use the uniprocessor backend for our measurements.

%As always, the appropriate metric to measure performance is not totally clear. 
%Running time is complicated by the multitasking environment offered by 
%modern operating systems. Also, given that the uniprocessor backend for
%the StreamIt compiler is meant for prototyping, the supporting 
%library is anything but optimized. Therefore, measuring running time is
%probably not an appropriate metric.

\begin{figure}[t]
%\vspace{-6pt}
\center
\epsfxsize=3.2in
\epsfbox{images/multiplications-remaining.eps}
\vspace{-6pt}
\caption{Percent of multiplication operations remaining after performing linear replacement, frequency replacement, and both.}
\label{fig:linear-freq-both}
\vspace{-12pt}
\end{figure}

%Floating point multiplication instructions 
%in the IA-32 instruction set are defined to be any of ({\tt fmul fmulp fimulp fdiv fdivp fidivp fdivr fdivrp fidivr}).
%We measure execution time normalized to the number of program outputs generated. 

Our measurement platform is a Dual Intel 2.2 GHz P4 Xeon processor
system with 2GB of memory running GNU/Linux. We compile our benchmarks
using StreamIt's uniprocessor backend and generate executables from
the resulting C files using {\tt gcc -O2}.  To measure the number of
runtime multiplications, we use an instruction counting
DynamoRIO\cite{dynamo99} client.  Since there are no external sources
of benchmarks written in StreamIt, we assembled a set of
representative programs\footnote{Stream graphs appear in Appendix A
XXXXXXXXXXXXXXXXXXXXXX.} which perform computations that are commonly
found in streaming applications: 1) {\bf FIR}, a single 256 point low
pass FIR filter; 2) {\bf RateConvert}, an audio down sampler that
converts the sampling rate by a non-integral factor ($\frac{2}{3}$);
3) {\bf TargetDetect}, four matched filters in parallel with threshold
target detection; 4) {\bf FMRadio}, an FM software radio with
equalizer; 5) {\bf Radar}, the core functionality in modern radar
signal processors, based on a system from the Polymorphic Computing
Architecture~\cite{pca}; 6) {\bf FilterBank}, a multi-rate signal
decomposition processing block common in communications and image
processing; 7) {\bf Vocoder}, a channel voice coder, commonly used for
speech analysis and compression; 8) {\bf Oversample}, a $16x$
oversampler, a function found in many CD players, 9) {\bf DToA}, an
audio post-processing stage prior to a 1-bit D/A converter with
oversampler and first order noise shaper.

\subsection{Performance}
To determine the effects of our linear replacement and frequency
replacement optimizations, we compiled each benchmark program with
linear replacement, with frequency replacement, with a back to back
application of frequency replacement and linear replacement, and with
automatic optimization selection.  Figure~\ref{fig:linear-freq-both}
shows the reduction in the number of floating point multiplications
that our optimizations achieve.  For each benchmark, our optimizations
reduce the number of multiplications considerably. For FIR and
TargetDetect, the reduction comes from frequency replacement, but in
other benchmarks linear replacement reduces the multiplications as
well. For TargetDetect, Radar and Vocoder, combining both
optimizations using the selection algorithm decreases multiplications
more than either alone. Naive linear replacement increases the number
of multiplications required because it ``unfactors'' some of the
computation that the original programmer had expressed in the
program's structure. With automatic selection, only beneficial
combinations are performed, thereby reducing the number of
multiplications required.  This is especially important in the case of
Radar, in which execution actually slows down without the selection
algorithm.
% AAL-is there anything else interesting that we would like to point out?

\begin{figure}[t]
\center
\epsfxsize=3.2in
\epsfbox{images/execution-speedup.eps}
\vspace{-6pt}
\caption{Execution speedup for each of the benchmarks with 
  frequency replacement, linear replacement, both and partitioning.}
\label{fig:execution-speedup}
\vspace{-6pt}
\end{figure}

%% In general, reducing the number of floating point multiplies does not
%% necessarily translate into a faster program, but the two statistics
%% are usually correlated.  

As Figure~\ref{fig:execution-speedup} demonstrates, our benchmarks
speedup on average by a factor of {\bf XXXXX} and by a factor of
$XXXXXXX$ in the best case.  Our performance improvements are due only
in part to to the efficiency of FFTW.  Multiplications are decreased
and execution time is improved in RateConvert, FM, FilterBank and
Vocoder with only linear replacement. Linear replacement improvements
are due to removing redundant computations -- in essence, it performs
algebraic simplification across filter boundaries.

There are two cases in which the multiplication reduction is not a
good predictor of execution speedup.  For Vocoder, the optimization
selection algorithm achieves the same number of multiplications as
applying both linear and frequency replacement; however, the speedup
is higher with automatic selection.  This is because the selection
algorithm refactors and combines a linear splitjoin where the plain
optimizations do not.  While this has no effect on the number of
multiplications required, it lowers the execution time by decreasing
the runtime overhead in the execution of the stream graph.  FilterBank
illustrates similar behavior -- the selection algorithm determines a
clever refactoring that reduces the number of multiplications compared
to applying both linear and frequency replacement, but the additional
overhead of more filters actually causes the execution time to
increase. A more accurate cost model that takes into account costs
other than multiplications could help the selection algorithm improve
its choice of stream graph arrangements.

\begin{table*}[t]
\vspace{-6pt}
\centering
\small
\begin{tabular}{|l|c|c|c||c||c|c|c|} 
\hline
& \multicolumn{3}{|c||}{Originally}  &             & \multicolumn{3}{|c|}{After Optimizations} \\
\hline
Benchmark  & Filters & Pipelines& SplitJoins & Average     & Filters      & Pipelines         & SplitJoins \\
           & (linear)& (linear) & (linear)   & vector size &              &                   &            \\
\hline
FIR        & 3 (1)   & 1(0)     & 0 (0)      & 256         & 3            & 1                 & 0 \\
\hline
RateConvert& 5 (3)   & 2 (1)    & 0 (0)      & 102         & 4            & 1                 & 0 \\
\hline
TargetDetect& 10 (4) & 6 (0)    & 1 (0)      & 300         & 7            & 2                 & 1 \\
\hline
FMRadio    & 26 (22) & 11 (9)   & 2 (2)      & 40          & 5            & 1                 & 0 \\
\hline
Radar      & 76 (60) & 17 (0)   & 2 (0)      & 4412        & 21           & 1                 & 2 \\
\hline
FilterBank & 27 (24) & 17 (9)   & 4 (4)      & 52          & 10           & 1                 & 2 \\
\hline
Vocoder    & 17 (13) & 10 (8)   & 2 (1)      & 60          & 7            & 2                 & 1 \\
\hline
Oversampler& 10 (8) & 1 (1)     & 0 (0)      & 33          & 3            & 1                 & 0 \\
\hline
DToA       & 14 (10) & 3 (1)    & 0 (0)      & 52          & 6            & 2                 & 0 \\
\hline

\end{tabular}
\vspace{-3pt}
\caption{Statistics for benchmarks before and after optimizations.
\protect\label{fig:benchmark-statistics}}
\vspace{-4pt}
\end{table*}
