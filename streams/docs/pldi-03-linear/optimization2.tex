\section{Translation to Frequency}
\label{sec:freq}

In this section, we demonstrate how we can leverage our linear
representation to automatically perform a common domain-specific
optimization: translation to the frequency domain.  First, we show
that a linear node is equivalent to a set of convolution sums, which
can benefit from algorithmic gains if performed in frequency rather
than time.  We then present an optimized code generation strategy for
transforming linear nodes to frequency.

\subsection{Basic Frequency Implementation}

Our first goal is to show that the computation of a linear node can be
represented as a convolution sum.  Consider executing $m$ iterations
of a linear node $\lambda = \{A, {\vec 0}, e, 1, 1\}$---that is, a
node with $\vec{b} = 0$ and $\mbox{push}=\mbox{pop}=1$ (these assumptions
will be relaxed below).  Let $\vec{out}[i]$ represent the $i$th value
that is pushed during execution, let $\vec{in}[i]$ hold the value of
$peek(i)$ as seen before the execution begins, and let ${\vec y}$ be
the convolution of the only column of $A$ with the vector $\vec{in}$
(that is, $y = A[*,0] * {\vec{in}}$).  Note that $\vec{out}$ is an
$m$-element vector, $A[*,0]$ is an $e$-element vector, ${\vec{in}}$ is
an $(m+e-1)$-element vector, and ${\vec y}$ is an $(m+2e-2)$-element
vector.

Then, we make the following claim:
\begin{equation}
\label{eq:claim1}
\forall i \in [0, m-1]:~~{\vec{out}}[i] = {\vec y}\hspace{1pt}[i+e-1]
\end{equation}
To see that this is true, recall the definition of convolution:
\[
  {\vec y}\hspace{1pt}[i] = A[i,0] * \vec{in}[i] = \sum_{k=-\infty}^{\infty} A[k,0] \vec{in}[i-k]
\]
Substituting $\vec{in}$ by its definition, and restricting $k$ to
range over the valid rows of $A$, we have:
\[
  {\vec y}\hspace{1pt}[i] = \sum_{k=0}^{e-1} A[k,0] \mbox{peek}(i-k)
\]
Remapping the index $i$ to $i+e-1$ makes the right hand side
equivalent to ${\vec{out}}[i]$, by Definition 1.
Claim~\ref{eq:claim1} follows.

In other words, values pushed by a linear node can be calculated by a
convolution of the input tape with the coefficients $A$.  The
signficance of this fact is that a convolution operation can be
implemented very efficiently by using the Fast Fourier Transform (FFT)
to translate into the frequency domain.  To compute the convolution,
two $N$-point FFTs of $\vec{in}$ and $A[*,0]$ are calculated to obtain
$\vec{X}$ and $\vec{\cal A}$, each of which is a complex-valued vector
of length $N$.  Element-wise multiplication of $\vec X$ and
$\vec{\cal A}$ yields a vector $\vec{Y}$, to which the inverse
transform (IFFT) is applied to obtain $\vec{y}$.  Convolution in the
frequency domain requires $O(N \lg(N))$ operations, as each FFT and
IFFT has a cost of $O(N \lg (N))$ and the vector multiplication is
$O(N)$.  By contrast, the complexity is $O(N^2)$ in the time domain,
as each of the $N$ output values requires $O(N)$ operations.  For more
details, refer to~\cite{oppenheim-discrete}.

We can use the procedure described above to implement a linear node in
the frequency domain.  We simply calculate ${\vec y} = A[*,0] * {\vec
in}$, and extract values ${\vec y}\hspace{1pt}[e-1] \dots {\vec y}\hspace{1pt}[m+(e-1)-1]$ as
the $m$ values pushed by the node.  Note that ${\vec y}\hspace{1pt}[i]$ is also
defined for $i \in [0, e-2]$ and $i \in [m+e-1,m+2e-2]$; these values
represent partial sums in which some coefficients were excluded.  Our
{\naive} implementation simply disregards these values.  However, in the
next section, we give an optimized implementation that takes advantage
of them.

The only task remaining for the implementation is to choose $N$, the
FFT size, and $m$, the number of iterations to execute at once in the
frequency domain.  According to Fourier's theorem, an $N$-point FFT
can exactly represent any discrete sequence of $N$ numbers, so the
only constraint on $N$ and $m$ is that $N \ge m+2e-1$.  For
performance reasons, $N$ should be a power of two and as large as
possible.  In our implementation, we set $N$ to the first power of two
that is larger than $2e$, which strikes a reasonable compromise
between storage space and performance for our uniprocessor
benchmarking platform.  The choice of $N$ should be adjusted for the
particular resource constraints of the target architecture.

The transformation below gives a {\naive} translation of a linear node to
the frequency domain.  In addition, it relaxes all of the assumptions
that we made above.  The algorithm allows for a non-zero value of
${\vec b}$ by simply adding $\vec{b}$ after returning from the
frequency domain.  To accommodate a push rate greater than one, the
algorithm generates $\vec{Y}$ and $\vec{y}$ {\it matrices} and
alternates pushing values from each column of $\vec{y}$ in turn.
Finally, to accommodate a pop rate greater than one, the algorithm
proceeds as if the pop rate was one and adds a special decimator node
that discards the extra outputs.  Though this introduces inefficiency
by calculating values that are never used, it still leaves room for
large performance improvements, as the frequency transformation
improves performance by an average factor of 50 (in the case where
$\mbox{pop}=1$.)

\begin{transformation} ({\Naive} frequency implementation)
Given a linear node $\lambda = \{A, {\vec b}, e, o, u\}$, the
following stream is a {\naive} implementation of $\lambda$ in the
frequency domain:
\begin{equation} \nonumber
{\small
  \begin{array}{ll}
    {\tt float \rightarrow float}~{\tt pipeline~naiveFreq~}(A, {\vec b}, e, o, u){\tt ~\{} & \hspace{40pt}\\
    ~~{\tt add~float \rightarrow float~filter~\{}\\
    ~~~~N \leftarrow 2^{\lceil \lg(2e) \rceil} \\
    ~~~~m \leftarrow N-2e+1 \\
    \\
    ~~~~{\tt init}~{\tt \{} \\
    ~~~~~~{\tt for}~j=0~{\tt to}~u-1\\
    ~~~~~~~~\vec{H}[*,j] \leftarrow \mathbf{FFT}(N,A[*,u-1-j]) \\
    ~~~~{\tt \}} \\
    \\
    ~~~~{\tt work}~{\tt peek}~m+e-1~{\tt pop}~m~{\tt push}~u*m~{\tt \{} \\
    ~~~~~~\vec{x} \leftarrow {\tt peek}(0 \dots m+e-1 ) \\
    ~~~~~~\vec{X} \leftarrow \mathbf{FFT} (N, \vec{x}) \\
    ~~~~~~{\tt for}~j=0~{\tt to}~u-1\\
    ~~~~~~~~\vec{Y}[*,j] \leftarrow \vec{X} .* \vec{H}[*,j] \\
    ~~~~~~~~\vec{y}\hspace{1pt}[*,j] \leftarrow \mathbf{IFFT}(N, \vec{Y}[*,j]) \\
    ~~~~~~{\tt \}} \\
    ~~~~~~{\tt for}~i=0~{\tt to}~m-1\\
    ~~~~~~~~{\tt pop()} \\
    ~~~~~~~~{\tt for}~j=0~{\tt to}~u-1\\
    ~~~~~~~~~~{\tt push}(\vec{y}\hspace{1pt}[i+e-1,j] + {\vec b}[j])\\
    ~~~~~~{\tt \}} \\
    ~~~~{\tt \}} \\
    ~~{\tt \}} \\
    ~~{\tt add~FreqDecimator(o, u)} \\
    {\tt \}} \\ ~ \\
    {\tt float \rightarrow float}~{\tt filter~freqDecimator~}(o, u){\tt ~\{} & \hspace{40pt}\\
    ~~{\tt work~}{\tt peek}~u*o~{\tt pop}~u*o~{\tt push}~u~{\tt \{} \\
    ~~~~{\tt for}~i=0~{\tt to}~u-1 \\
    ~~~~~~{\tt push(pop())} \\
    ~~~~{\tt for}~i=0~{\tt to}~u-1 \\
    ~~~~~~{\tt for}~j=0{\tt to}~o-2 \\
    ~~~~~~~~{\tt pop()} \\
    ~~{\tt \}} \\
    {\tt \}}
  \end{array}}
\end{equation}
\vspace{-6pt}
\label{trans:freq1}
\end{transformation}

\subsection{Optimized Frequency Implementation}

The {\naive} frequency implementation discards $e-1$ elements from the
beginning and end of each column of ${\vec y}$ that it computes.
These values represent partial sums in which some of the coefficients
of $A$ are excluded. However, for $i \in [0, e-1-1]$, ${\vec
y}\hspace{1pt}[i,j]$ in one iteration contains the missing terms from
${\vec y}\hspace{1pt}[m+e-1+i,j]$ in the previous iteration.  The sum
gives the correct output value.  This symmetry arises from the
convolution of $A$ ``off the edges'' of the input block that we
consider in a given iteration. Reusing the partial sums---which is
exploited in the transformation below---is one of several methods that
use blocking to efficiently convolve a short filter with a large
amount of input~\cite{oppenheim-discrete}.
%% Recall that $A$ is of length $e$ and the input tape is given in blocks
%% of length $m$.  Then, consider element $m+i$ of the first execution.
%% Following equation~\ref{eq:linconv}, we have:
%% \[
%% {\vec y}\hspace{1pt}[m+i] = \sum_{k=0}^{e-1} A[k] \mbox{\tt peek}(m+i+e-1-k)
%% \]
%% Since we are in the first execution, we can only peek at values $0
%% \dots m-1$, which implies that $m+i+e-1-k \in [0, m-1]$ and thus $k
%% \in [i+e, m+i+e-1]$.  That is, only coefficients $A[k]$ for these
%% values of $k$ will be included in the sum.  Now consider the
%% computation of element $i$ on the second iteration.  Since the second
%% iteration can peek at values in the range of $[m,2m-1]$, we have (by
%% the same reasoning as above) that $k \in [$.

\begin{transformation} \label{trans:freq1}
(Optimized frequency implementation)
Given a linear node $\lambda = \{A, {\vec b}, e, o, u\}$, the
following stream is an optimized implementation of $\lambda$ in the
frequency domain:
\begin{equation} \nonumber
{\small
  \begin{array}{ll}
    {\tt float \rightarrow float}~{\tt pipeline~optimizedFreq~}(A, {\vec b}, e, o, u){\tt ~\{} & \hspace{40pt}\\
    ~~{\tt add~float \rightarrow float~filter~\{}\\
    ~~~~N \leftarrow 2^{\lceil \lg(2e) \rceil} \\
    ~~~~m \leftarrow N-2e+1 \\
    ~~~~\vec{partials} \leftarrow {\tt new~array}[0 \dots (e-1)-1, 0\dots u-1] \\
    ~~~~r \leftarrow m+e-1 \\
\end{array}
}
\end{equation}
\end{transformation}

\begin{equation} \nonumber 
\small{
\begin{array}{ll}
\vspace{-24pt}
    \\
    ~~~~{\tt init}~{\tt \{} \\
    ~~~~~~{\tt for}~j=0~to~u-1\\
    ~~~~~~~~\vec{H}[*,j] \leftarrow \mathbf{FFT}(N,A[*,u-1-j]) \\
    ~~~~{\tt \}} \\
    \\
    ~~~~{\tt prework}~{\tt peek}~r~{\tt pop}~r~{\tt push}~u*m~{\tt \{}\\
    ~~~~~~\vec{x} \leftarrow {\tt pop}(0 \dots m+e-2) \\
    ~~~~~~\vec{X} \leftarrow \mathbf{FFT} (N, \vec{x}) \\
    ~~~~~~{\tt for}~j=0~to~u-1\\
    ~~~~~~~~\vec{Y}[*,j] \leftarrow \vec{X} .* \vec{H}[*,j] \\
    ~~~~~~~~\vec{y}[*,j] \leftarrow \mathbf{IFFT}(N, \vec{Y}[*,j]) \\
    ~~~~~~~~\vec{partials}[*,j] \leftarrow \vec{y}\hspace{1pt}[m+e-1 \dots m+2e-3,j] \\
    ~~~~~~{\tt \}} \\
    ~~~~~~{\tt for}~i=0~to~m-1\\
    ~~~~~~~~{\tt for}~j=0~to~u-1\\
    ~~~~~~~~~~{\tt push}(\vec{y}\hspace{1pt}[i+e-1,j] + \vec{b}[j]) \\
    ~~~~{\tt \}} \\
    \\
    ~~~~{\tt work}~{\tt peek}~r~{\tt pop}~r~{\tt push}~u*r~{\tt \{} \\
    ~~~~~~\vec{x} \leftarrow {\tt pop}(0 \dots m+e-2) \\
    ~~~~~~\vec{X} \leftarrow \mathbf{FFT} (N, \vec{x}) \\
    ~~~~~~{\tt for}~j=0~to~u-1\\
    ~~~~~~~~\vec{Y}[*,j] \leftarrow \vec{X} .* \vec{H}[*,j] \\
    ~~~~~~~~\vec{y}[*,j] \leftarrow \mathbf{IFFT}(N, \vec{Y}[*,j]) \\
    ~~~~~~{\tt \}} \\
    ~~~~~~{\tt for}~i=0~to~e-1\\
    ~~~~~~~~{\tt for}~j=0~to~u-1\\
    ~~~~~~~~~~{\tt push}(\vec{y}\hspace{1pt}[i,j] + \vec{partials}[i,j])\\
    ~~~~~~~~~~\vec{partials}[i,j] \leftarrow \vec{y}\hspace{1pt}[m+e-1+i,j]\\
    ~~~~~~{\tt for~i~=~0~to~}m-1\\
    ~~~~~~~~{\tt for~j~=~0~to~}u-1\\
    ~~~~~~~~~~{\tt push}(\vec{y}\hspace{1pt}[i+e-1,j] + {\vec b}[j])\\
    ~~~~{\tt \}} \\
    ~~{\tt \}} \\
    ~~{\tt add~Decimator}(o, u)~~\mt{// see Transformation 5}\\
    {\tt \}} \\
  \end{array}}
\end{equation}

\subsection{Applications of Frequency Transformation}

The transformation to the frequency domain is straightforward in
theory and very common in practice. However, the detailed record
keeping, transform size selection, and state management make an actual
implementation quite involved.  Further, as the complexity of DSP
programs continue to grow, manually determining the disparate regions
across which to apply this optimization is an ever more daunting task.
For example, several filters individually may not perform sufficiently
large convolutions to merit a frequency transformation, but after a
linear combination of multiple filters the transformation could be
beneficial.  Differing levels of architectural support for various
convolution and frequency operations further complicates the task of
choosing the best transform.  Our compiler automatically determines
all the necessary information and transforms the computation into the
frequency domain.

%% Then consider that we push u values.  Then n indicates the firing
%% number of the work function.
%%
%%   For all j in [0,u-1]:
%%
%%     y[n*u+j] = sum_i (x[n-i]*A[e-1+i,j])
%%
%% Consider instead that we pop o items.  Then have:
%%
%%   y[n] = sum_i (x[o*n-i]*A[e-1+i])

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% In order to derive a frequency implementation of a linear node, let us
%% start by formulating the computation that the node performs.  To
%% simplify the derivation, assume that we are given a node $\lambda$
%% with $\vec{b} = 0$ and $\mt{push}=\mt{pop}=1$ (we will relax these
%% restrictions below).  As usual, let $e$ denote the peek rate for
%% $\lambda$; also let $p_0$ denote the item that is pushed.  Then, by
%% Definition 1, we have the following:
%% \begin{equation}
%%   p_0 = \vec{x} A + {\vec b} = \sum_{k=0}^{e-1} A[k] \mbox{\tt peek}(e-1-k)
%% \end{equation}
%% Note that in this case, $p_0$ is a scalar since $\mt{push}=1$ (and
%% thus $A$ has a single column).  Now let us consider $m$ executions of
%% the node, with the $i$th output being pushed to location ${\vec
%% p}[i]$.  This computation is exactly as above, except that the input
%% and output tapes are offset by $i$:
%% \begin{equation}
%%   \protect\label{eq:linconv}
%%   \forall i \in [0, m-1]:~~\vec{p}[i] = \sum_{k=0}^{e-1} A[k] \mbox{\tt peek}(i+e-1-k)
%% \end{equation}
%% At this point, our equation is very similar in form as a convolution
%% sum from digital signal processing.  Given two vectors $\vec{h}$ and
%% $\vec{x}$, their convolution is defined as:
%% \begin{equation}
%%   {\vec y}\hspace{1pt}[n] = \vec{x} * \vec{h} = \sum_{k=-\infty}^{\infty} \vec{x}[k] \vec{h}[n-k]
%% \end{equation}
