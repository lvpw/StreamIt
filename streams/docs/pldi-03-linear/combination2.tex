%  could replace ``filter'' by ``stream'' in the next few sentences, but 
% I think it reads better as it is, actually
\section{Combining Linear Filters}
\label{sec:combine}

A primary benefit of linear filter analysis is that neighboring
filters can be collapsed into a single matrix representation if both
of the filters are linear.  This transformation can automatically
eliminate redundant computations in linear sections of the stream
graph, thereby allowing the programmer to write simple, modular
filters and leaving the combination to the compiler.  In this section,
we first describe a {\it linear expansion} operation that is needed to
match the sizes of $A$ and $\vec{b}$ for different linear nodes and is
therefore an essential building block for the other combination
techniques.  We then give rules for collapsing pipelines and
splitjoins into linear nodes; we do not deal with feedbackloops as
they require a notion of ``linear state'', which we do not describe
here.

\subsection{Linear Expansion}

In StreamIt programs, the input and output rate of each filter in the
stream graph is known at compile time.  The StreamIt compiler
leverages this information to compute a static schedule---that is, an
ordering of the node executions such that each filter will have enough
data available to atomically execute its work function, and no
buffer in the stream graph will grow without bound in the steady
state.  A general method for scheduling StreamIt programs is given by
Karczmarek~\cite{karczma-thesis}.

A fundamental aspect of the steady-state schedule is that neighboring
nodes might need to be fired at different frequencies.  For example,
if there are two filters $F_1$ and $F_2$ in a pipeline and
$F_1$ produces $2$ elements during its work function but $F_2$
consumes $4$ elements, then it is necessary to execute $F_1$ twice for
every execution of $F_2$.

%% \begin{figure}
%% \center
%% \epsfxsize=3.0in
%% \epsfbox{images/expanding-a-filter.eps}
%% \caption{Expanding {\tt stream} $S$ by a factor $f$}
%% \label{fig:expanding-a-filter}
%% \vspace{-12pt}
%% \end{figure}

Consequently, when we combine hierarchical structures into a linear
node, we often need to {\it expand} a matrix representation to
represent multiple executions of the corresponding stream.  Expansion
allows us to multiply matrices and interleave columns from matrices
that originally had mismatching dimensions.  The transformation can be
done as follows.

% NOT TRUE ANYMORE
%% If we expand a linear node by a factor of $k$, then one execution of
%% the new node will be exactly equivalent to $k$ executions of the
%% original.

\begin{transformation} (Linear expansion)
% DOUBLE-CHECK whether we're assuming 0-indexed or 1-indexed matrices
Given a linear node $\lambda = \{A, {\vec b}, e, o, u\}$, the expansion of
$\lambda$ to a rate of $(e', o', u')$ is given by $\mbox{\bf
expand}(\lambda, e', o', u') = \{A', {\vec b}', e', o', u'\}$, where
$\mathbf{A'}$ is a $e' \times u'$ matrix and $\vec{\mathbf{b}}'$ is a
$u'$-element row vector:
\vspace{-6pt} \\
\begin{equation} \nonumber
\begin{array}{rcl}
\multicolumn{3}{l}{\mt{shift}(r,c) ~\mt{is a}~ u' \times e' ~\mt{matrix}:} \\
\multicolumn{3}{l}{\parbox{3in}{
    \begin{equation} \nonumber
    \mt{shift}(r,c)[i,j] = \left\{
      \begin{array}{l}
	A[i-r,j-c] \\
	~~~\mt{if}~ i-r \in [0,e-1] \wedge j-c \in [0, u-1] \\
	~ 0 \mt{~otherwise}
      \end{array}
      \right.
    \end{equation}
}} \\ ~ \vspace{-6pt} \\
%
\mathbf{A'} & = & \sum_{m=0}^{\lceil u' / u \rceil} \mt{shift}(u'- u -m*u, e'-e-m*o) \\ ~ \vspace{-4pt} \\
%
\vec{\mathbf{b}}'[j] & = & {\vec b}\hspace{1pt}[u-1-(u'-j-1)~\mt{mod}~u]
%%
%% \raisebox{-14pt}{\parbox{3in}{is created by starting with a zero
%% matrix with $e'$ rows and $u'$ columns.  $A$ is then copied $k$ times
%% along the diagonal. Starting at the top left, each copy of $A$ is
%% offset from the previous copy by $u$ columns and $o$ rows.}} \\ ~ \vspace{-8pt} \\
%% $b'$ & is an row vector containing $k$ adjacent copies of $b$.
\end{array}
\end{equation}
\end{transformation}

\begin{figure}[t]
\center
\vspace{-12pt}
\epsfxsize=3.2in
\epsfbox{images/filter-expand.eps}
\vspace{-12pt}
\caption{Expanding a linear node to rates $(e', o', u')$.  }
\label{fig:expanding-a-matrix}
\makeline
\vspace{-12pt}
\end{figure}

The intuition behind linear expansion is straightforward (see
Figure~\ref{fig:expanding-a-matrix}).  Linear expansion aims to scale
the peek, pop and push rates of a linear node while preserving the
functional relationship between the values pushed and the values
peeked on a given execution. To do this, we construct a new matrix
$A'$ that contains copies of $A$ along the diagonal starting from the 
bottom right.  To account for items that are popped between invocations, 
each copy of $A$ is offset by $o$ from the previous copy. 
The complexity of the definition is
due to the end cases.  If the new push rate $u'$ is not a multiple of
the old push rate $u$, then the last copy of $A$ includes only some of
its columns.  Similarly, if the new peek rate $e'$ exceeds that which
is needed by the diagonal of $A$s, then $A'$ needs to be padded with
zeros at the top (since it peeks at some values without using them in
the computation).

Note that a sequence of executions of an expanded node $\lambda'$
might not be equivalent to any sequence of executions of the original
node $\lambda$, because expansion resets the push and pop rates and
can thereby modify the ratio between them.  However, if $u' = k * u$
and $o' = k * o$ for some integer $k$, then $\lambda'$ is completely
interchangeable with $\lambda$.  In the combination rules that follow,
we utilize linear expansion both in contexts that do and do not
satisfy this condition.

\subsection{Collapsing Linear Pipelines}

%\begin{figure}
%\center
%\epsfxsize=3.0in
%\epsfbox{images/pipeline-combination.eps}
%\caption{A pipeline of two linear forms $(A,b)$ and $(C,d)$ (above) and the same pipeline with rate matched forms (below).}
%\label{fig:combining-pipeline}
%\vspace{-12pt}
%\end{figure}

The {pipeline construct is used to compose streams in sequence,
with the output of stream $i$ connected to the input of stream $i+1$.
The following transformation describes how to collapse two linear
nodes in a pipeline; it can be applied repeatedly to collapse
any number of neighboring linear nodes.

\begin{transformation} (Pipeline combination)
Given two linear nodes $\lambda_1$ and $\lambda_2$ where the output of
$\lambda_1$ is connected to the input of $\lambda_2$ in a pipeline
construct, the combination $\mbox{\bf pipeline}(\lambda_1, \lambda_2)$ $=$
$\{\mathbf{A}', \vec{\mathbf{b}}', \mathbf{e}', \mathbf{o}', \mathbf{u}'\}$ 
represents an equivalent node that
can replace the original two.  Its components are as follows:
\begin{equation} \nonumber
\begin{array}{rcl}
\mt{chanPop} & = & \mt{lcm}(u_1, o_2) \\ ~ \vspace{-8pt} \\
\mt{chanPeek} & = & \mt{chanPop} + e_2 - o_2 \\ ~ \vspace{-8pt} \\
\lambda_1^e & = & \mt{expand}(\lambda_1, (\left \lceil \frac{\mt{chanPeek}}{u_1} \right \rceil - 1) * o_1 + e_1, \\
~ & ~ & ~\hspace{0.32in}~ \mt{chanPop} * \frac{o_1}{u_1}, \mt{chanPeek}) \\ ~ \vspace{-8pt} \\
\lambda_2^e & = & \mt{expand}(\lambda_2, \mt{chanPeek}, \\
~ & ~ & ~\hspace{0.32in}~ \mt{chanPop}, \mt{chanPop} * \frac{u_2}{o_2}) \\
\mathbf{A'} & = & A_1^e A_2^e \\ ~ \vspace{-8pt} \\
\vec{\mathbf{b}}' & = & {\vec b}_1^e A_2^e + {\vec b}_2^e \\ ~ \vspace{-8pt} \\
\mathbf{e'} & = & e_1^e \\ ~ \vspace{-8pt} \\
\mathbf{o'} & = & o_1^e \\ ~ \vspace{-8pt} \\
\mathbf{u'} & = & u_2^e
\end{array}
\end{equation}
\end{transformation}

The basic forms of the above equations are simple to derive.  Let
${\vec x}_i$ and ${\vec y}_i$ be the input and output channels, respectively, for
$\lambda_i$.  Then we have by definition that ${\vec y}_1 = {\vec x}_1 A_1 + {\vec b}_1$
and ${\vec y}_2 = {\vec x}_2 A_2 + {\vec b}_2$.  But since $\lambda_1$ is connected to
$\lambda_2$, we have that ${\vec x}_2 = {\vec y}_1$ and thus 
${\vec y}_2 = {\vec y}_1 A_2 + {\vec b}_2$.
Substituting the value of ${\vec y}_1$ from our first equation gives ${\vec y}_2 =
{\vec x}_1 A_1 A_2 + {\vec b}_1 A_2 + {\vec b}_2$.  Thus, the intuition is that the
two-filter sequence can be represented by matrices $A' = A_1 A_2$
and ${\vec b}' = {\vec b}_1 A_2 + {\vec b}_2$, with peek and pop rates borrowed from
$\lambda_1$ and the push rate borrowed from $\lambda_2$.

However, there are two implicit assumptions in the above analysis
which complicate the equations for the general case.  First, the
dimensions of $A_1$ and $A_2$ must match for the matrix multiplication
to be well-defined.  If $u_1 \ne e_2$, this will require constructing
expanded nodes $\lambda_1^e$ and $\lambda_2^e$ in which the push and
peek rates match (and thus $A_1^e$ and $A_2^e$ can be multiplied).

The second complication is with regards to peeking.  If the downstream
node $\lambda_2$ peeks at items which it does not consume ({\it i.e.},
if $e_2 > o_2$), then there needs to be a buffer to hold items that
are read during multiple invocations of $\lambda_2$.  However, in our
current formulation, a linear node has no concept of internal state,
such that this buffer cannot be incorporated into the collapsed
representation.  To deal with this issue, we adjust the expanded form
of $\lambda_1$ to recalculate items that $\lambda_2$ uses more than
once, thereby trading computation for storage space.  This adjustment
is evident in the push and pop rates chosen for $\lambda_1^e$: though
$\lambda_1$ pushes $u_1$ items for every $o_1$ items that it pops,
$\lambda_1^e$ pushes $\mt{chanPeek}*u_1$ for every $\mt{chanPop}*o_1$
that it pops.  When $\mt{chanPeek}>\mt{chanPop}$, this means that the
outputs of $\lambda_1^e$ are overlapping, and
$\mt{chanPeek}-\mt{chanPop}$ items are being regenerated on every
firing.

Note that although $\lambda_1^e$ performs duplicate computations in
the case where $\lambda_2$ is peeking, this computation cost can be
amortized by increasing the value of $\mt{chanPop}$.  That is, though
the equations set $\mt{chanPop}$ as the {\it least} common multiple of
$u_1$ and $o_2$, any common multiple is legal.  As $\mt{chanPop}$
grows, the regenerated portion $\mt{chanPeek}-\mt{chanPop}$ becomes
smaller on a percentage basis.

However, it is the case that some collapsed linear nodes are always
less efficient than the original pipeline sequence.  The worst case is
when $A_1^e$ is a column vector and $A_2^e$ is a row vector, which
requires $O(N)$ operations originally but $O(N^2)$ operations if
combined (assuming vectors of length $N$).  To avoid such
performance-degrading combinations, we employ an automated selection
algorithm that only performs beneficial transformations (see
Section~\ref{sec:partitioning}).

\begin{figure}[t]
\begin{center}
\psfig{figure=images/example-pipeline-combination.eps,width=3.1in}
\vspace{-6pt}
\caption{Pipeline combination example.}
\label{fig:example-pipeline-combination}
\end{center}
\vspace{-22pt}
\makeline
\vspace{-14pt}
\end{figure}

Figure~\ref{fig:example-pipeline-combination} illustrates the
combination of back to back FIR filters. Since the push rate of the
first filter ($u_1=1$) differs from the peek rate of the second
($e_2=3$), the first filter must be expanded to $\lambda_1^e =
\mt{expand}(\lambda_1,4,1,3)$.  There is no need to expand the second
filter, so $\lambda_2^e = \lambda_2$. By construction, we can now form
the matrix product of $A_1^e$ and $A_2^e$, which corresponds to the
matrix for the overall linear node.

\subsection{Collapsing Linear SplitJoins}

The splitjoin construct allows the StreamIt programmer to
express explicitly parallel computations.  Data elements that arrive
at the splitjoin are directed to the parallel child streams
using one of two pre-defined splitter constructs: 1) duplicate,
which sends a copy of each data item to all of the child streams, and
2) roundrobin, which distributes items cyclically according to an
array of weights.  The data from the parallel streams are
combined back into a single stream by means of a roundrobin 
joiner with an array of weights $w$.  First, $w_0$ items from the
output tape of the leftmost child are placed onto the overall output
tape, then $w_1$ elements are taken from the second leftmost child,
and so on.  The process repeats itself after one complete set of
$\sum_{i=0}^{N-1} w_i$ elements has been pushed.

In this section, we demonstrate how to collapse a splitjoin into
a single linear node when all of its children are linear nodes.  Since
the children of splitjoins in StreamIt can be parameterized, it
is often the case that all sibling streams are linear if any one of
them is linear.  However, if a splitjoin contains only a few
adjacent streams that are linear, then these streams can be combined
by wrapping them in a hierarchical splitjoin and then collapsing the
wrapper completely.  Our technique also assumes that each 
splitjoin admits a valid steady-state schedule; this property is
verified by the StreamIt semantic checker.

Our analysis distinguishes between two cases. For duplicate splitters,
we directly construct a linear node from the child streams.  For
roundrobin splitters, we translate the splitjoin to use a duplicate
splitter and then rely on the transformation for duplicate splitters.
We describe these translations below.

\subsubsection{Duplicate Splitter}

Intuitively, there are three main steps to combining a duplicate
splitjoin into a linear node.  Since the combined node will represent
a steady-state execution of the splitjoin construct, we first need to
expand each child node according to its multiplicity in the schedule.
Secondly, we need to ensure that each child's matrix representation
has the same number of rows---that is, that each child peeks at the
same number of items.  Once these conditions are satisfied, we can
construct a matrix representation for the splitjoin by simply
arranging the columns from child streams in the order specified by the
roundrobin joiner (see Figure~\ref{fig:splitjoin-duplicate-matrix}).
This third step is simplified by the fact that, with a duplicate
splitter, each row of a child's linear representation refers to the
same input element of the splitjoin.

The following transformation describes splitjoin combination in
mathematical terms.

%\begin{figure}
%\center
%\epsfxsize=3.0in
%\epsfbox{images/splitjoin-duplicate-ratematch.eps}
%\caption{Expanding sub {\tt streams} to match their output rates in a linear {\tt SplitJoin}.}
%\label{fig:splitjoin-duplicate-ratematch}
%\end{figure}

\begin{transformation} (Duplicate splitjoin combination)
Given a splitjoin $s$ containing a duplicate splitter, children that
are linear nodes $\lambda_0 \dots \lambda_{n-1}$, and a roundrobin
joiner with weights $w_0 \dots w_{n-1}$, the combination 
$\mbox{\bf splitjoin}(s)$ $=$ $ \{ \mathbf{A}',$ $\vec{\mathbf{b}}',$ $\mathbf{e}',$
$\mathbf{o}',$ $\mathbf{u}'\}$ represents an equivalent node that can
replace the entire stream $s$.  Its components are as follows:
\begin{equation} \nonumber
  \begin{array}{rcl}
    \mt{joinRep} & = & \mt{lcm}(\frac{\mt{lcm}(u_0,w_0)}{w_0}, \dots, \frac{\mt{lcm}(u_{n-1},w_{n-1})}{w_{n-1}}) \\
    \mt{maxPeek} & = & \mt{max}_i (o_i * \mt{rep}_i + e_i - o_i) \\ ~ \vspace{-2pt} \\
    \multicolumn{3}{l}{\forall k \in [0, n-1]:} \\ ~ \vspace{-6pt} \\
    \mt{wSum}_k & = & \sum_{i=0}^{k-1} w_i \\ ~ \vspace{-6pt} \\
    \mt{rep}_k & = & \frac{w_k * \mt{joinRep}}{u_k} \\ ~ \vspace{-6pt} \\
    \lambda_k^e & = & \mt{expand}(\lambda_k, \mt{maxPeek}, 
    o_k * \mt{rep}_k, u_k * \mt{rep}_k) \\ ~ \vspace{-2pt} \\
    \multicolumn{3}{l}{
      \forall k \in [0, n-1], 
      \forall m \in [0, joinRep-1], 
      \forall p \in [0, u_k-1]:
    } \\ ~ \vspace{-6pt} \\
    \multicolumn{3}{l}{\mathbf{A}'[*, u'-1-p - m * \mt{wSum}_{n}-\mt{wSum}_{k}] = % should this be wSum_{k-1}? 
      A_{k}^e [*,u_k^e-1-p]} \\ ~ \vspace{-6pt} \\
    \multicolumn{3}{l}{\vec{\mathbf{b}}'[u'-1-p - m * \mt{wSum}_{n}-\mt{wSum}_{k}] = 
      b_{k}^e [u_k^e-1-p]} \\ ~ \vspace{-2pt} \\
    \mathbf{e}' & = & e_0^e = \dots = e_{n-1}^e \\
    \mathbf{o}' & = & o_0^e = \dots = o_{n-1}^e \\
    \mathbf{u}' & = & \mt{joinRep} * \mt{wSum}_n \\
  \end{array}
\end{equation}
\end{transformation}

The above formulation is derived as follows.  The $\mt{joinRep}$
variable represents how many cycles the joiner completes in an
execution of the splitjoin's steady-state schedule; it is
the minimal number of cycles required for each child node to execute
an integral number of times and for all of their output to be consumed
by the joiner. Similarly, $\mt{rep}_k$ gives the execution count for
child $k$ in the steady state.  Then, in keeping with the procedure
described above, $\lambda_k^e$ is the expansion of the $k$th node by
a factor of $\mt{rep}_k$, with the peek value set to the maximum peek
across all of the expanded children.  Following the expansion, each
$\lambda_i^e$ has the same number of rows, as the peek uniformization
causes shorter matrices to be padded with rows of zeros at the top.

\begin{figure}[t]
\center
\epsfxsize=3.2in
\epsfbox{images/splitjoin-combination.eps}
\vspace{-6pt}
\caption{Matrix resulting from combining a splitjoin of rate-matched children.
\protect\label{fig:splitjoin-duplicate-matrix}}
\vspace{-2pt}
\makeline
\vspace{-14pt}
\end{figure}

The final phase of the transformation is to re-arrange the columns of
the child matrices into the columns of $A'$ and $\vec{b}'$ such that
they generate the correct order of outputs.  Though the equations are
somewhat cumbersome, the concept is simple (see
Figure~\ref{fig:splitjoin-duplicate-matrix}): for the $k$th child and
the $m$th cycle of the joiner, the $p$th item that is pushed by child
$k$ will appear at a certain location on the joiner's output tape.
This location (relative to the start of the node's execution) is $p +
m * \mt{wSum}_n + \mt{wSum}_k$, as the reader can verify.  But since
the right-most column of each array $A$ holds the formula to compute
the first item pushed, we need to subtract this location from the
width of $A$ when we are re-arranging the columns. The width of $A$ is
the total number of items pushed---$u'$ in the case of $A'$ and
$u_k^e$ in the case of $A_k^e$.  Hence the equation as written above:
we copy all items in a given column from $A_k^e$ to $A'$, defining
each location in $A'$ exactly once.  The procedure for $\vec{b}$ is
analogous.

It remains to calculate the peek, pop and push rates of the combined
node.  The peek rate $e'$ is simply $maxPeek$, which we defined to be
equivalent for all the expanded child nodes.  The push rate
$\mt{joinRep}*\mt{wSum}_n$ is equivalent to the number of items
processed through the joiner in one steady-state execution.  Finally,
for the pop rate we rely on the fact that the splitjoin is well-formed
and admits a schedule in which no buffer grows without bound.  If this
is the case, then the pop rates must be equivalent for all the
expanded streams; otherwise, some outputs of the splitter would
accumulate infinitely on the input channel of some stream.

These input and output rates, in combination with the values of $A'$
and $\vec{b}'$, define a linear node that exactly represents the
parallel combination of child nodes that are fed by a duplicate
splitter. Figure~\ref{fig:example-splitjoin-combination} provides an
example of splitjoin combination.

%The node on the left pushes four items per work function 
%whereas the node on the right pushes one item per work function. In order
%to match the output rates to the rate of the roundrobin joiner
%the right filter needs to be expanded to 
%$\lambda_2^e=\mt{expand}(\lambda_2,2,2,2)$. The columns of the
%two linear nodes are then interleaved into the overall
%linear node $\lambda'$.
% I don't know if this is useful or if it is enough information
% concievably, we could walk though the entire algorithm and the 
% calculation of the intermediate variables. Somehoe, however, I think 
% that this is best done in my thesis -- AAL


\subsubsection{Roundrobin Splitter}

In the case of a roundrobin splitter, items are directed to each child
stream $s_i$ according to weight $v_i$: the first $v_0$ items are sent
to $s_0$, the next $v_1$ items are sent to $s_1$, and so on.  Since a
child never sees the items that are sent to sibling streams, the items
that are seen by a given child form a periodic but non-contiguous
segment of the splitjoin's input tape.  Thus, in collapsing the
splitjoin, we are unable to directly use the columns of child matrices
as we did with a duplicate splitter, since with a roundrobin splitter
these matrices are operating on disjoint sections of the input.

Instead, we collapse linear splitjoins with a roundrobin splitter by
converting the splitjoin to use a duplicate splitter.  In order to
maintain correctness, we add a decimator on each branch
of the splitjoin that eliminates items which were intended for other
streams.

\begin{transformation} (Roundrobin to duplicate)
Given a splitjoin $s$ containing a roundrobin splitter with weights
$v_0 \dots$ $v_{n-1}$, children that are linear nodes $\lambda_0 \dots
\lambda_{n-1}$, and a round-robin joiner $j$, the transformed
$\mbox{\bf rr-to-dup}(s)$ is a splitjoin with a duplicate splitter,
linear child nodes $\mathbf{\lambda_0'} \dots
\mathbf{\lambda_{n-1}'}$, and roundrobin joiner $j$.  The child nodes
are computed as follows:
\begin{equation} \nonumber
\begin{array}{rcl}
\mt{vSum}_k & = & \sum_{i=0}^{k-1} v_i \\ ~ \vspace{-6pt} \\
\mt{vTot} & = & \mt{vSum}_n \\ ~ \vspace{-4pt} \\
\multicolumn{3}{l}{\forall k \in [0, n-1]:} \\ ~ \vspace{-12pt} \\ 
\multicolumn{3}{l}{\parbox{3in}{
    \begin{equation} \nonumber
      \begin{array}{l}
      ~~~~\mt{decimate}[k] ~\mt{is a linear node}~ \{A, \vec 0, \mt{vTot}, \mt{vTot}, v_{k}\} \\ \vspace{-8pt} \\
	~~~~~~~\mt{where}~A[i,j] = \left\{
	\begin{array}{l}
	  1 ~\mt{if}~ i=\mt{vTot}-\mt{vSum}_{k+1}+j \\
	  0 ~\mt{otherwise} \\
	\end{array}
	\right.
      \end{array}
    \end{equation}}} \\
\mathbf{\lambda_k'} & = & \mt{pipeline}(\mt{decimate}[k], \lambda_k)
\end{array}
\end{equation}
\end{transformation}

In the above translation, we utilize the linear pipeline combinator
$\mt{pipeline}$ to construct each new child node $\lambda_i^e$ as a
composition of a decimator and the original node $\lambda_i$.  The
$k$th decimator consists of a $\mt{vTot} \times v_k$ matrix that
consumes $\mt{vTot}$ items, which is the number of items processed in
one cycle of the roundrobin splitter.  The $v_k$ items that are
intended for stream $k$ are copied with a coefficient of $1$, while
all others are eliminated with a coefficient of $0$.

\begin{figure}[t]
\center
\epsfxsize=2.089in
\epsfbox{images/example-splitjoin-combination.eps}
\vspace{-6pt}
\caption{Splitjoin combination example.}
\protect\label{fig:example-splitjoin-combination}
\vspace{-2pt}
\makeline
\vspace{-14pt}
\end{figure}

%% \begin{figure}
%% \center
%% \epsfxsize=3.0in
%% \epsfbox{images/splitjoin-roundrobin-matrix.eps}
%% \caption{Corresponding matrix for splitter conversion from roundrobin to duplicate.}
%% \label{fig:splitjoin-roundrobin-matrix}
%% \end{figure}

\subsection{Applications of Linear Combination}

%% Linear analysis is valuable because it provides a precise relationship
%% between input and output that is nearly impossible to extract from
%% general-purpose imperative programs.  While the combination of linear
%% structures is analogous to algebraic simplification between loop
%% bodies in an imperative program, it would be extremely difficult for a
%% general-purpose compiler to extract the same information.

%% Combining the action of {\tt streams} in a {\tt pipeline} is analogous
%% to algebraic simplification between the bodies of sequential loops in
%% an imperative program.  Once an overall linear node for the {\tt
%% pipeline} has been determined, our compiler can perform this
%% transformation automatically, leaving the the programmer to express
%% the computation in the most convenient manner.

There are numerous instances where the linear combination
transformation could benefit a programmer.  For example, although a
bandpass filter can be implemented with a low pass filter followed by
a high pass filter, actual implementations determine the
coefficients of a single combined filter that performs the same
computation. While a simple bandpass filter is easy to combine
manually, in an actual system several different filters might be
designed and implemented by several different engineers, making 
overall filter combination infeasible.

Another common operation in discrete time signal processing is
downsampling to reduce the computational requirements of a system.
Downsampling is most often implemented as a low pass filter followed
by an $M$ compressor which passes every $M$th input item to the
output.  In practice, the filters are combined to avoid computing dead
items in the low pass filter.  However, the system specification
contains both elements for the sake of understanding.  Our analysis can
start with the specification and derive the efficient version automatically.

A final example is a multi-band equalizer, in which $N$ different
frequency bands are filtered in parallel (see our FMRadio benchmark).
If these filters are time invariant, then they can be collapsed into a
single node.  However, designing this single overall filter is
difficult, and any subsequent changes to any one of the sub filters
will necessitate a total redesign of the filter.  With our automated
combination process, any subsequent design changes will necessitate
only a recompile rather than a manual redesign.
