%% \begin{figure}[t]
%%   \psfig{figure=images/part-algorithm3.eps,width=3.5in}
%%   \caption{Cost functions for optimization selection.}
%% \end{figure}

\section{Optimization Selection}
\label{sec:partitioning}

To reap the maximum benefit from the optimizations described in the
previous two sections, it is important to apply them selectively.
There are two components of the optimization selection problem: first,
to determine the sequence of optimizations that will give the highest
performance for a given arrangement of the stream graph, and second,
to determine the arrangement of the stream graph that will give the
highest performance overall.  In this section, we explain the
relevance of each of these problems, and we present an effective
selection algorithm that relies on dynamic programming to quickly
explore a large space of possible configurations.

\subsection{The Selection Problem}

First, the selection of optimizations for a given stream graph can
have a large impact on performance.  As alluded to in
Section~\ref{sec:combine}, linear combination can increase the number
of arithmetic operations required, {\it e.g.} if combining a
two-element pipeline where the second filter pushes more items than it
peeks.  However, such a combination might be justified if it enables
further combination with other components and leads to a benefit
overall.  Another consideration is that as the pop rate grows, the
benefit of converting to frequency diminishes; thus, it might be
preferable to transform smaller sections of the graph to frequency, or
to perform linear combination only.  This occurs in our Vocoder and
FMRadio benchmarks, in which selection algorithm improves performance
by choosing plain linear combination over frequency translation.

Second, the arrangement of the stream graph can dictate which
transformations are possible to apply.  Since our transformations
operate on an entire pipeline or splitjoin construct, the graph often
needs to be refactored to put linear nodes in their own hierarchical
unit.  For example, in our TargetDetect benchmark, we cut a splitjoin
horizontally and collapse the top piece before converting to the
frequency domain, thereby amortizing the cost of the FFT on the input
items.
% (see Figure~\ref{fig:horizcut}).  
In our Vocoder benchmark, we cut a splitjoin vertically and combine
all of the filters on the right into a single linear node.
% (seeFigure~\ref{fig:vertcut}).

\subsection{Dynamic Programming Solution}

Our optimization selection algorithm, shown in
Figures~\ref{fig:part-alg} and \ref{fig:part-decl}, automatically
derives the example transformations described above.  Intuitively, the
algorithm works by estimating the minimum cost for each structure in
the stream graph. The minimum cost represents the best of three
configurations: 1) collapsed and implemented in the time domain, 2)
collapsed and implemented in the frequency domain, and 3) uncollapsed
and implemented as a hierarchical unit.  The cost functions for the
collapsed cases are guided by profiler feedback, as described below.
For the uncollapsed case, the cost is the sum of each child's minimum
cost.  However, instead of considering the children directly, the
children are refactored into many different configurations, and the
cost is taken as the minimum over all configurations.  This allows the
algorithm to simultaneously solve for the best set of transformations
and the best arrangement of the stream graph.

The key to the algorithm's efficiency is the manner in which it
considers refactoring the children of hierarchical nodes.  Instead of
considering arbitrary arrangements of the stream graph, it considers
only {\it rectangular} partitions, in which a given splitjoin is
divided into two child splitjoins by either a horizontal 
%cut (Figure~\ref{fig:horizcut}) 
or vertical cut.
%(Figure~\ref{fig:vertcut}).  
This approach works well in practice, because splitjoins often have
symmetrical child streams in which linear components can be separated
by a horizontal line.  Moreover, as the child splitjoins are
decomposed and evaluated, there are overlapping sub-problems that
enable us to search the space of child configurations in polynomial
time, using dynamic programming.

\begin{figure}[t]
\psfig{figure=images/part-algorithm2.eps,width=3.5in}
  \vspace{-16pt}
  \caption{Type declarations for code in Figure~\ref{fig:part-alg}.
  \protect\label{fig:part-decl}}
  \vspace{-19pt}
\end{figure}

\begin{figure}[t]
  \psfig{figure=images/part-algorithm.eps,width=3.5in}
  \vspace{-12pt}
  \caption{Algorithm for optimization selection.
  \vspace{-12pt}
  \protect\label{fig:part-alg}}
\end{figure}

%Pseudocode for our optimization selection algorithm appears in
%Figures~\ref{fig:part-alg} and~\ref{fig:part-decl}.  
There is one subtlety in the pseudocode of Figure~\ref{fig:part-alg}.
First is the translation procedure from a StreamIt graph to a set of
hierarchical {\it Stream} objects.  In particular, each Stream
corresponds only to a pipeline; adjacent splitjoin objects are wrapped
in a pipeline and their children are considered as direct children of
the pipeline. This enables different parts of neighboring splitjoins
to be combined.  However, it implies that a {\tt Stream} might have a
different width at different points (since neighboring splitjoins
could have differing widths); this necessitates the addition of the
{\tt width} array to the algorithm.

\subsection{Cost Functions}

The pseudocode in Figure~\ref{fig:part-alg} refers to functions {\it
getDirectCost} and {\it getFrequencyCost} that estimate a node's
execution time if implemented in the time domain or the frequency
domain.  These cost functions can be tailored to a specific
architecture and code generation strategy.  For example, if there is
architecture-level support for convolution operations (such as the the
{\tt FIRS} instruction in the TMS320C54x~\cite{ti-dsp-manual}), then
this would effect the cost for certain dimensions of matrices;
similarly, if a matrix multiplication algorithm was available that
exploited symmetry or sparsity in a matrix, then this benefit could be
detected where it would apply.  In our implementation, we use the
following versions of the cost functions (let
$\lambda=(A,\vec{b},e,o,u)$ be the linear node corresponding to stream
$s$):
%% \[
%% \begin{array}{rcl}
%% \mt{getDirectCost}(s) \hspace{-6pt} & = \hspace{-6pt} & 1 + 3 * |\{ (i,j)~s.t.~A_{i,j} \ne 0\}| \vspace{3pt} \\ 
%%  ~ & ~ & \hspace{6.8pt}+~|\{ i~s.t.~{\vec b}_i \ne 0 \}| \\ ~ & ~ & ~ \\
%% %%\mt{getDirectCost}(s) \hspace{-6pt} & = \hspace{-6pt} & 1 + 3 * (\mt{rows}(A) * \mt{cols}(A) - \mt{numZeros}(A)) \\ ~ & ~ & ~~~+cols({\vec b}) - numZeros({\vec b}) \\
%% \mt{getFrequencyCost}(s)  \hspace{-6pt} & = \hspace{-6pt} & \frac{1}{50}~(4 * \mt{rows}(A) * \mt{cols}(A) + \mt{rows}(A))
%% \end{array}
%% \]
\[
\mt{getDirectCost}(s) = \left\{ \begin{array}{l}
\infty ~~~~~~~~~ \mbox{(if }s\mbox{~is roundrobin splitjoin)} \\ ~ \\ \vspace{-8pt} \\
100 + 2*u + ~~~~~~~~~~~~~~~~\mbox{(otherwise)} \\ \vspace{-8pt} ~ \\
~~~|\{ i~s.t.~{\vec b}_i \ne 0 \}|~+ \\ \vspace{-8pt} ~ \\
~~~3 * |\{ (i,j)~s.t.~A_{i,j} \ne 0\}| \\ 
\end{array} \right.
\]
\[
% trial and error with the ~, can't get it to move over :(
\mt{getFrequencyCost}(s) = ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
\vspace{-10pt}
\]
\[
~~~\left[100 + 2*u + u * \mt{ln}\left(\frac{1 + 4*e}{1 + \frac{2^{\lceil lg(2*e) \rceil}}{50}}\right)\right] * \mt{max}(o, 1) + \mt{dec}(s)
\]
\[
% trial and error with the ~, can't get it to move over :(
\mt{dec}(s) = (o-1) * (100 + 4 * u) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\]

\clearpage
For the direct cost, we consider the cost to be infinite for
splitjoins with roundrobin splitters.  This is because the splitjoin
combination does not eliminate any arithmetic operations, and for the
roundrobin case it introduces extra overhead (the duplicate case is
different because the input items are shared between streams.)  For
other streams, we count the number of multiplies and adds required to
perform the matrix multiplication, giving more weight to the
multiplies since they are more expensive.  We do not count the zero
entries of the arrays, since our matrix multiply routines take
advantage of the sparsity of the matrix.  We add $100+2u$ to all costs
in order to represent the overhead of the node's execution.

For the frequency cost, our cost curve is guided by profiler feedback.
The speedup gained by translating to the frequency domain depends on
the peek rate of the filter.  To obtain an $n$-fold speedup of the
frequency code over the direct code, the filter has to peek $50*n$
items.  Mathematically, this translates to a logarithmic expression
for the cost of the frequency node in terms of the number of items $e$
(we actually use the next power of two above $e$, as that is the size
of the FFT).  We multiply the above cost by the numer of items pushed,
add the constant offset of $100+2u$ for a node, and then multiply by
$\mt{max}(o, 1)$ because only one out of $o$ items represents a valid
output from the frequency domain.  Finally, we add $\mt{dec}(s)$, the
cost of the decimator for the frequency node.  We estimate an extra
cost of $2$ per push operation in the decimator, as it must read from
the input tape.  The other pop operations in the decimator are free,
because they can be performed as a single adjustment of the tape
position.

Of course, both cost functions are undefined if $s$ is non-linear
({\it e.g.} there is no corresponding $\lambda_s$). If this is the
case, then the optimization selection algorithm assigns an infinite
cost.

%% OPTIMIZING EXECUTION

%% can identify rectangles of stream graph that are equivalent, and use this to:
%%  - save on computing linear node representation
%%  - save on computing partitioning of childre
%%  - automatically compute closed form for some linear sections
