\begin{figure}[t]
  \psfig{figure=images/part-algorithm.eps,width=3.5in}
  \caption{Algorithm for optimization selection.
  \protect\label{fig:part-alg}}
\end{figure}

\begin{figure}[t]
  \psfig{figure=images/part-algorithm2.eps,width=3.5in}
  \caption{Type declarations for code in Figure~\ref{fig:part-alg}.
  \protect\label{fig:part-decl}}
\end{figure}

%% \begin{figure}[t]
%%   \psfig{figure=images/part-algorithm3.eps,width=3.5in}
%%   \caption{Cost functions for optimization selection.}
%% \end{figure}

\section{Optimization Selection}
\label{sec:partitioning}

To reap the maximum benefit from the optimizations described in the
previous two sections, it is important to apply them selectively.
There are two components of the optimization selection problem: first,
to determine the sequence of optimizations that will give the highest
performance for a given arrangement of the stream graph, and second,
to determine the arrangement of the stream graph that will give the
highest performance overall.  In this section, we explain the
relevance of each of these problems, and we present an effective
selection algorithm that relies on dynamic programming to quickly
explore a large space of possible configurations.

\subsection{The Selection Problem}

First, the selection of optimizations for a given stream graph can
have a large impact on performance.  As alluded to in
Section~\ref{sec:combine}, linear combination can increase the number
of arithmetic operations required, {\it e.g.} if combining a
two-element pipeline where the second filter pushes more items than it
peeks.  However, such a combination could be justified if it enables
further combination with other components and leads to a benefit
overall.  Another consideration is that as the pop rate grows,
the benefit of converting to frequency diminishes; thus, excessive combination
could preclude the most effective frequency transformations.  
This is the case for our
RateConverter benchmark, in which three filters could be combined, but
it is better to combine only two in order to enable a frequency
transformation (see Figure~\ref{fig:rategraph}).

Second, the arrangement of the stream graph can dictate which
transformations are possible to apply.  Since our transformations
operate on an entire pipeline or splitjoin construct, the
graph often needs to be refactored to put linear nodes in their own
hierarchical unit.  For example, in our TargetDetect benchmark, we can
divide the splitjoin into two pieces and collapse the top piece
before converting to the frequency domain, thereby amortizing the cost
of the FFT on the input items (see Figure~\ref{fig:targetgraph}).

\subsection{Dynamic Programming Solution}

Our optimization selection algorithm, shown in Figure~\ref{fig:part-alg}
automatically derives both of the example transformations described above.  
The algorithm works by estimating the minimum cost for each structure 
in the stream graph. The minimum cost represents the best
of three configurations: 1) collapsed and implemented in the time
domain, 2) collapsed and implemented in the frequency domain, and 3)
uncollapsed and implemented as a hierarchical unit.  The cost
functions for the collapsed cases are guided by profiler feedback, as
described below.  For the uncollapsed case, the cost is the sum of
each child's minimum cost.  However, instead of considering the
children directly, the children are refactored into many different
configurations, and the cost is taken as the minimum over all
configurations.  This allows the algorithm to simultaneously solve for
the best set of transformations and the best arrangement of the stream
graph.

The key to the algorithm's efficiency is the manner in which it
considers refactoring the children of hierarchical nodes.  Instead of
considering arbitrary arrangements of the stream graph, it considers
only {\it rectangular} partitions, in which a given splitjoin is
divided into two child splitjoins by either a vertical or horizontal
cut.  This approach works well in practice, because splitjoins
often have symmetrical child streams in which linear components can be
separated by a horizontal line (such as TargetDetect, in
Figure~\ref{fig:targetgraph}).  Moreover, as the child splitjoins are
decomposed and evaluated, there are overlapping sub-problems that
enable us to search the space of child configurations in polynomial
time, using dynamic programming.

Pseudocode for our optimization selection algorithm appears in
Figures~\ref{fig:part-alg} and~\ref{fig:part-decl}.  The algorithm
works as described above, using the {\tt memoTable} array to keep
track of the lowest-cost configuration for each rectangular section of
a hierarchical stream.  An important aspect of the algorithm that is
not explicit in the pseudocode is the translation from a StreamIt
graph to a set of hierarchical {\tt Stream} objects.  In particular,
each Stream corresponds only to a pipeline; adjacent 
splitjoin objects are wrapped in a pipeline and their children
are considered direct children of the pipeline. This enables
different parts of neighboring splitjoins to be combined.
However, it implies that a Stream might have a different width
at different points (since neighboring splitjoins could have
differing widths); this necessitates the addition of the {\tt width}
array to the algorithm.

Ommitted from the pseudocode are the functions {\it getDirectCost} 
and {\it getFrequencyCost} that estimate a node's
execution time if implemented in the time domain or the frequency
domain.  These cost functions can be tailored to a specific
architecture and code generation strategy.  For example, if there is
architecture-level support for convolution operations (such as 
the the {\tt FIRS} instruction in the TMS320C54x~\cite{ti-dsp-manual}), 
then this would effect the cost for certain dimensions of matrices; similarly, if a
matrix multiplication algorithm was available that exploited symmetry
or sparsity in a matrix, then this benefit could be detected where it
would apply.  In our implementation, we use simple versions of the
cost functions (let $\lambda_s=(A,\vec{b},e,o,u)$ be the linear node
corresponding to stream $s$):
\[
\begin{array}{rcl}
\mt{getDirectCost}(s) \hspace{-6pt} & = \hspace{-6pt} & 1 + 3 * |\{ (i,j)~s.t.~A_{i,j} \ne 0\}| \vspace{3pt} \\ 
 ~ & ~ & \hspace{6.8pt}+~|\{ i~s.t.~{\vec b}_i \ne 0 \}| \\ ~ & ~ & ~ \\
%%\mt{getDirectCost}(s) \hspace{-6pt} & = \hspace{-6pt} & 1 + 3 * (\mt{rows}(A) * \mt{cols}(A) - \mt{numZeros}(A)) \\ ~ & ~ & ~~~+cols({\vec b}) - numZeros({\vec b}) \\
\mt{getFrequencyCost}(s)  \hspace{-6pt} & = \hspace{-6pt} & \frac{1}{50}~(4 * \mt{rows}(A) * \mt{cols}(A) + \mt{rows}(A))
\end{array}
\]
That is, for the direct cost, we count the number of multiplies and
adds required to perform the matrix multiplication, giving more weight
to the multiplies since they are more expensive.  We do not count the
zero entries of the arrays, since our matrix multiply routines take
advantage of the sparsity of the matrix.  We add $1$ to represent
the overhead of the push and pop operations of the node.  For the
frequency cost, we count the total number of operations in a direct
matrix multiply, since zero entries cannot be exploited in the
frequency domain.  We also add $\mt{rows}(A)$ to represent the 
cost of copying the input values into $\vec{X}$. Then, we discount this sum
by a factor of 50, as our measurements indicate that the frequency
implementation performs 50 times higher than a direct matrix multiply
(independent of the size of the matrix.)  Of course, both of these
cost functions are undefined if $s$ is non-linear (eg there is no
corresponding $\lambda_s$). If this is the case, then
the optimization selection algorithm assigns infinite cost.
Because the frequency transformation in the compiler is not implemented for
non unit pop rates, {\it getFrequencyCost} is also infinite 
for nodes where $o \ne 1$.; 

%% OPTIMIZING EXECUTION

%% can identify rectangles of stream graph that are equivalent, and use this to:
%%  - save on computing linear node representation
%%  - save on computing partitioning of childre
%%  - automatically compute closed form for some linear sections
