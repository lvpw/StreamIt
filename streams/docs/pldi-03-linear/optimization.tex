% this thing used to be about optimizations, now it is all about ``Translation to Frequency Domain''

\section{Translation to Frequency}
\label{sec:freq}

Our linear analysis framework provides a compile time formulation of
the computation that a linear node performs and we use this
information to exploit well known domain specific optimizing
transformations. Using linear node information, our compiler
identifies convolution operations that require substantially fewer
computations if computed in the frequency domain and applies
an automatic transformation.

Digital filters are most very often implemented using convolution sums
and consequently calculating a convolution sum is a common and fundamental 
operation in discrete time signal processing. If the convolution is
sufficiently large, transforming the data to the frequency domain,
performing an element-wise vector multiply and converting back to the time
domain requires fewer operations than the direct convolution.

The transformation from convolution sum into frequency multiplication
is typically done explicitly by human algorithm designers because no 
compiler analysis has had the information necessary to determine, from 
general purpose code, when a transformation could be applied. 
As the complexity of DSP programs continue to grow, manually 
determining the disparate regions across which 
to apply these optimizations is an ever more daunting task. 
For example, several filters individually may not perform sufficiently large 
convolutions to merit a frequency transformation, but after a linear 
combination of multiple filters the transformation will be beneficial.
Differing levels of architectural support for various convolution and 
frequency operations further complicates the task of manually choosing
the optimal transform.

\subsection{Notation and Background}
\label{sec:method-opt-notation}

\begin{equation}
\label{eqn:conv-sum-again}
\vec{y}[n]=\vec{x}[n]*\vec{h}[n]=\sum_{k=-\infty}^{\infty}\vec{x}[k]\vec{h}[n-k]
\end{equation}

A convolution sum is a vector operation on two vectors $\vec{x}$ and 
$\vec{h}$ and is defined in Equation~\ref{eqn:conv-sum-again} for
convenience. We will sometimes refer to the vector $\vec{h}$ as 
{\it filter coefficients}. 
%In general, when talking about vectors, $n$ denotes 
%an index in the time domain, and $k$ represents an index in the 
%frequency domain. 
Filters which compute convolution sums are often called 
Finite Impulse Response (FIR) filters. An FIR filter has a corresponding linear 
node with unit pop rate ($o=1$) and $\vec{b} = \vec{\mathbf{0}}$. From 
Equation~\ref{eqn:conv-sum-again} and the definition of a linear node,
if $o=1$ then each column of $A$ is identified with a vector $\vec{h}$
in a convolution sum. If $A$ has $u$ columns,	
the linear node is equivalent to $u$ FIR filters 
executing in parallel with their outputs interleaved.
To complete the identification of a linear node with an FIR filter,
we denote the input to the filter as $\vec{x}$ and the output of
a filter as $\vec{y}$.

Calculating a convolution sum in the frequency domain is much
more efficient because of a class of algorithms known collectively as 
the Fast Fourier Transform (FFT) which quickly calculate 
the Discrete Fourier Transform (DFT) of a signal.
%\footnote{see Section~\ref{sec:background-fft} for the general derivation.}

To compute the convolution of two discrete time signals, $\vec{y}=\vec{x}*\vec{h}$,
first the $N$-point DFT of $\vec{h}$, $\vec{H}$, is calculated.
Second, the $N$-point DFT of $\vec{x}$, $\vec{X}$, is calculated.
Note that both $\vec{X}$ and $\vec{H}$ are complex valued and each has
length $N$. Multiplying $\vec{X}$ and $\vec{H}$ element-wise produces a new $N$ point
vector, $\vec{Y}$. Taking the inverse DFT (IDFT) of $\vec{Y}$ produces 
$\vec{y}$ which is exactly the same as $\vec{x}*\vec{h}$. See Oppenheim
\cite{oppenheim-discrete} for a thorough treatment.

The computational complexity of calculating $N$ outputs in the time domain
is $O(N^2)$ as each of the $N$ output values requires $O(N)$ operations.
Doing the equivalent computation in the frequency domain requires
only $O(N \lg(N))$ because we need to evaluate two FFTs at a cost of
$O(N \lg(N))$ coupled with an $O(N)$ vector multiplication.

\begin{algorithm}
  \caption{Convolution in the Frequency Domain.\label{alg:freq-overview}}
  \begin{algorithmic}
    \STATE $\vec{h} \leftarrow {\mathit column~of}A$ extracted by dataflow analysis
    \STATE $\vec{H} \leftarrow \mathbf{FFT}(\vec{h},N)$ 
    \WHILE {true}
      \STATE $\vec{X} \leftarrow \mathbf{FFT}(\vec{x},N)$
      \STATE $\vec{Y} \leftarrow \vec{H} .* \vec{X}$ 
      \STATE $\vec{y} \leftarrow \mathbf{IFFT}(\vec{Y},N)$
      \STATE push($\vec{y}$)
    \ENDWHILE
  \end{algorithmic}
  {\bf Note:} $.*$ denotes element wise multiplication.
\end{algorithm}

The transformation to the frequency domain is straightforward in theory and 
very common in practice. However, the detailed record keeping, transform size selection,
and state management make actual implementation much more than a mindless
exercise. Our compiler automatically determines all the necessary
information and transforms the computation into the frequency domain. 
%When the compiler identifies a linear node that is acting as an FIR filter, 
%it replaces it with a new node filter that computes the output using the
%coversion described above.
Algorithm~\ref{alg:freq-overview} summarizes the conversion procedure
at a very high level and the rest of this section describes the transformation in 
detail. We assume an undergraduate level knowledge
of signal processing.
%from thesis: (see Section~\ref{sec:crash-course} for a crash course)

\subsection{Automatic Transformation}
\label{sec:method-opt-freq-details}

%To automatically transform a computation expressed in the time domain 
%to calculation in the frequency domain, the compiler first needs to identify 
%which linear filters are computing convolution sums. Identification is 
%easy once armed with linear node information generated from the dataflow 
%analysis presented in Section~\ref{sec:method-dataflow}. The compiler 
%identifies FIR filters by identifying linear nodes with a peek rate of 
%one ($e=1$) and zero constant component (${\vec b} = {\vec 0}$).

%To realize the $\frac{O(N\lg(N))}{O(N^2)}$ complexity savings of the FFT
%we need to ensure that $N$, the problem size, is big enough. In this case,
%the problem size is related to the length FFT we want to calculate.
%From empirical observations we have determined that convolution sums with 
%$8$ or more coefficients gain actual algorithmic savings in our implementation.

It is critically important to choose the length of the FFT to perform. 
Lengths that are powers of two can be calculated most efficiently. 
In general, if $\vec{y}$ is the result of a convolution of 
two sequences with lengths $e$ and $N$, $\vec{y}$ has length $e+N-1$.
We need to compute the element wise vector multiplication of
$\vec{X}$ and $\vec{H}$ in the frequency domain which requires that 
$\vec{X}$ and $\vec{H}$ be the same length. Therefore, to 
recover $\vec{y}$ from its transform, $\vec{Y}$, 
$\vec{Y}$ must both be at least $e+N-1$ elements long because 
of Fourier's theorem which states that any discrete sequence of $M$ numbers 
can be represented exactly (without frequency aliasing) with 
$M$ values of the DFT. 

In the StreamIt language, filters operate on infinite input streams 
and generate infinite output streams.
The FFT can only be evaluated for finite input. To use the FFT we need to
generate discrete sections of the output using discrete sections of the input.
There are many known ways of convolving a short filter ($\vec{h}$) with
a large amount of input ($\vec{x}$) block by block.
For a good treatment, again see Oppenheim~\cite{oppenheim-discrete}. 

In their original form, StreamIt FIR filters consume one input and 
produce one output and peek at $e$ input values. The compiler creates 
a new filter which produces $N+e-1$ output values by consuming $N+e-1$ input 
values on each work function execution. 
Using $N+x-1$ input values with an $e$ length FIR filter implies that we
must use $N+2(e-1)$ point FFT in order to avoid aliasing.

A $N+2(e-1)$ point FFT and IFFT will result in the output, $\vec{y}$, being $N+2(e-1)$
items long. However, both the starting and ending $e-1$ points of $\vec{y}$ correspond
the tails of the convolution sum where not all of the input values are used. 
A naive implementation would simply discard these tail values to produce 
$N$ output values, consume $N$ input values and peek at $N+e-1$ items.
The simple observation that he two $e-1$ length tails are partial results from the 
calculations of both the previous and next work function invocations results in 
a faster implementation. By keeping these partial results as state between
filter executions, the transformed filters are very efficient and they 
produce $N+e-1$ output points every execution without peeking.

The FFT length is a function of both $N$ and $e$, and $e$ is defined in the
original program. In order to reap the $\frac{O(\lg(N))}{N}$ savings, 
$N \gg e$. The compiler is free to choose the value of $N$, 
so we can reap arbitrarily large savings from the computational complexity 
simply by increasing $N$. 
In practice, we can't increasing $N$ increases the required state
of the transformed filter program. Buffer size explosion is
not of particular concern for our Uniprocessor testbed because of the large
general purpose memory available. For embedded processors 
buffer size is much more important due to the constrained resources available.
Algorithm~\ref{alg:freq-size} selects a size $N$ that ensures 
$N+2(e-1)$ is a power of two and that $N \approx 2e$ which is a 
reasonable trade off between state and performance.

%Increasing $N$ corresponds to amortizing the cost of 
%zero padding the input to the FFT length. 
%If we are willing to use a buffer to store the transformed filter coefficients, $H[k]$, 
%we can save the cost of computing $H[k]$ on each work function invocation.
%Although this doesn't change the asymptotic performance gains, it provides
%a considerable speedup in practice.


%The parameter $N$, described above, is approximately the number of outputs to 
%produce on each execution of the new work function. $N$ is determined automatically according
%to the rules shown in Algorithm~\ref{alg:freq-size}.
%An initial target of $N=2e$ seems a reasonable compromise between increased efficiency from the
%FFT and the increase in buffer size. Because the most efficient FFT algorithms implement compute
%a DFT whose size is a power of two, Algorithm~\ref{alg:freq-size} revises $N$ such that
%$N+2(e-1)$ is a power of two.


%\begin{figure}[t]
%  \center
%  \begin{tabular}{c}
%    \begin{minipage}{1.9in}
%      \center
%      \epsfxsize=1.9in \epsfbox{images/fft-replacement-before.eps}
%      {\bf a)} Original FIR Filter.
%    \end{minipage} \\
%    \begin{minipage}{2.8in}
%      \center
%      \epsfxsize=2.8in \epsfbox{images/fft-replacement-after.eps}
%      {\bf b)} Frequency Implementation.
%    \end{minipage} \\
%  \end{tabular}
%  \caption{Automatically replacing an FIR filter with a frequency implementation.}
%\label{fig:fft-replacement}
%\end{figure}

\begin{transformation} (Frequency implementation)
Given a linear node $\lambda = \{A, {\vec b}, e, o, u\}$, 
the frequency implementation is a two stage filter filter $f$ which
computes Algorithm~\ref{alg:freq-overview}. $f$ has
initWork rates of $e'_{i}=N+e-1$, $o'_{i}=N+e-1$ $u'_{i}=N$ and
work rates of $e'=N+e-1$, $o'=N+e-1$ $u'=N+e-1$.
\\
\begin{equation} \nonumber
  \begin{array}{rccl}
    % perhaps we should condense the next few lines.
    targetN & = &&  (2*e) \\
    FFTSize & = &&  2^{\lceil \lg(targetN + 2*(e-1)) \rceil} \\
    N       & = &&  FFTSize - 2*(e-1) \\ 
    \\
    f.init  & = &\{&\vec{partials} \leftarrow \vec{\mathbf{0}} \\
%            &   &&  H[0 \dots N+2(e-1)] \leftarrow \mathbf{FFT}(N+2(e-1),h[n]) \\
            &   &&  \vec{H} \leftarrow \mathbf{FFT}(N+2(e-1),h[n])~~\} \\
    \\
    f.initWork & = &\{&\vec{x} \leftarrow \mathbf{pop}(0 \dots (N+e-1)) \\
           &     &&\vec{X} \leftarrow \mathbf{FFT} (N+2(e-1), \vec{x}) \\
           &     &&\vec{Y} \leftarrow \vec{X} .* \vec{H} \\
           &     &&\vec{y} \leftarrow \mathbf{IFFT}(N+2(e-1), \vec{Y} \\
           &     &&\mathbf{push}(\vec{y}[e-1 \dots N+e-1]) \\
           &     &&\vec{partials} \leftarrow \vec{y}[N+e-1 \dots N+2(e-1)]~~\} \\
    f.work &   & = &\{\vec{x} \leftarrow \mathbf{pop}(0 \dots (N+e-1)) \\
           &     &&\vec{X} \leftarrow \mathbf{FFT} (N+2(e-1), \vec{x}) \\
           &     &&\vec{Y} \leftarrow \vec{X} .* \vec{H} \\
           &     &&\vec{y} \leftarrow \mathbf{IFFT}(N+2(e-1), \vec{Y}) \\
           &     &&\mathbf{push}(\vec{y}[0 \dots e-1] + \vec{partials})\\
           &     &&\mathbf{push}(\vec{y}[e-1 \dots N+e-1]) \\
           &     &&\vec{partials} \leftarrow \vec{y}[N+e-1 \dots N+2(e-1)]~~\} \\
    \end{array}
\end{equation}
\label{trans:freq}
\end{transformation}



Transformation~\ref{trans:freq} shows how to generate a 
filter which implements the computation of $\lambda$ using the
frequency transformation described in this section. 
Transformation~\ref{trans:freq} as written applies only for
linear nodes with unit push rate (so $A$ is a column vector).
The general case for arbitrary columns is a trivial extension:
simply apply the algorithms to each column individually and interleave
the result. $C$ columns is more efficient than $C$ individual columns
because the values of $\vec{X}$ can 
be reused for each column. The complete algorithm is described by 
Lamb\cite{lamb-thesis}. 


%Figure~\ref{fig:fft-replacement} illustrates an FIR filter before
%and after the automatic frequency transformation.
%After the transformation, the filter peeks and pops $e'=N+e-1$  
%items on each execution whereas the original stream popped 1 and peeked $e$.
%The new filter automatically computes the FFT of the filter coefficients
%$H[k] \leftarrow FFT(N+2(e-1),h[n])$, 
%in the {\tt init} function and saves them as state 
%for use in each subsequent {\tt work} function. Algorithm~\ref{alg:freq-init}
%contains the pseudo code generated in the new {\tt init} functions.
%A new {\tt work} function is generated by the compiler that calculates the 
%FFT of $N+e-1$ input values, $X[k] \leftarrow DFT(N+2(e-1),x[n])$.
%$Y[k]$ is calculated as the element-wise vector product 
%of $X[k]$ and $H[k]$, $Y[k] \leftarrow H[k].*X[k]$
%\footnote{The element-wise vector product $(.*)$ of two vectors is defined as 
%$Y[i]=X[i]H[i]:i\in [0 \cdots N-1]$.}.
%Finally, the new {\tt work} function calculates the output values $y[n]$ with
%the inverse FFT, $y[n] \leftarrow IFFT(N+2(e-1),y[n])$.

%The convolution of $N+e-1$ input items and an impulse response $h[n]$ of length $e$
%results in $N+2(e-1)$ output values. However, both the first and last $e-1$ values 
%are not values that the original filter would have calculated. These starting and
%ending values correspond to the output of the filter when the two signals
%do not overlap completely during the calculation of the convolution sum.
%In our initial implementation, the {\tt work} function simply
%discarded the beginning and trailing $e-1$ elements, and then advanced 
%the input tape by $N$ items. The next $N+e-1$ values on the tape were 
%used to produce the next $N$ outputs.

%The transformed filters now exploit the well known fact that the 
%beginning and trailing values of $y[n]$ contain partial results 
%that can be harnessed to improve efficiency.
%The first $e-1$ items of $y[n]$ are part of the computation from 
%the previous invocation of {\tt work}, and the final $e-1$ are part of the 
%calculation in the next invocation.
%In the automatic replacement, the compiler creates a filter
%which first pushes $y[i]+p[i]$ for $0 \le i \le (e-1)-1$, where $p[n]$
%contains partial results from the previous invocation of {\tt work}. 
%Then the filter pushes $y[i]$ for $e-1 \le i \le N+(e-1)-1$.
%$p[n]$ is updated with final $e-1$ values of $y[n]$ such that $p[i]=y[i+(N+e-1)]$ 
%for $0 \le i \le N+2(e-1)-1$, and the input tape is then advanced by $N+e-1$ items.
%Overall, this leads to peek, pop and push rates of 
%$e'=N+(e-1)$, $o'=N+(e-1)$ and $u'=N+(e-1)$.

%We need one slight modification to get the initial conditions correct. 
%The first {\tt work} function invocation, {\tt initWork}, 
%needs to be slightly different from every other execution. 
%Initially, the partial results field doesn't contain any information about past calculations 
%so the first $e-1$ elements of $y[n]$ are completely ignored.
%The middle $N$ elements of $y[n]$ are pushed and the final $e-1$ elements are
%saved in $p[n]$ as normal. This implies that the peek, pop and
%push rates for {\tt initWork} are $e'_{i} = N+(e-1)$, $o'_{i}=N+(e-1)$ and 
%$u'_{i}=N$, respectively.
