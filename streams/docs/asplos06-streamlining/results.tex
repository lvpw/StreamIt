\section{Experimental Evaluation}

In this section we present an evaluation of the contributions of this
paper and compare to previous techniques for compiling stream programs
to multicore architectures.  This section will include an elaboration
of the following contributions and conclusions:

\begin{itemize}
\item Our technique for exploiting coarse-grained data parallelism
produces abundant parallelism and achieve a geometric mean performance
gain of 9.9x over a sequential, single-core baseline.
\item Coarse-grained software pipelining is a
an effective technique for extracting parallelism beyond task and data
parallelism, with an additional mean speedup of 1.45x for
our benchmarks with stateful computation and 1.13x across all of our
benchmarks. On its own, our software pipelining technique affords a
7.7x performance gain over a sequential single-core baseline.
\item The combination of the techniques presented in this paper
improves upon previous work that exploited a combination of task and
pipeline parallelism, with a mean speedup of 1.84x.
\end{itemize}

In the evaluation, speedup of configuration A over B is calculated as
the throughput for an average steady-state of A divided by B; the
initialization and prologue schedules, if present, are not
included. Figure \ref{fig:thruput} presents a table of throughput
speedups normalized to single-core for most configurations.
\cite{raw_isca} reported that sequential StreamIt executing on a
single Raw core outperformed hand-written C implementations executing
on a single core over a benchmark similar to ours. Furthermore, we
have increased the performance of sequential compilation since
\cite{raw_isca}.

\begin{figure*}[t]
\centering
\psfig{figure=benchchar.eps, width=6.5in}
\caption{Benchmark characteristics
\protect\label{fig:benchchar}}
\end{figure*}

\subsection{Benchmark Suite}
We evaluate our techniques using the benchmark suite given in Figure
\ref{fig:benchchar}.   The benchmark suite consists of 12 StreamIt
applications. MPEG2Decoder implements the block decoding and the
motion vector decoding of an MPEG-2 decoder, containing approximately
one-third of the computation of the entire MPEG-2 decoder.  Also, the
DCT benchmark implements a 16x16 IEEE reference DCT while the
MPEG2Decoder benchmark includes an 8x8 IEEE fast DCT as a component.
For additional information for MPEG2Decoder, Vocoder, and Radar,
please refer to \cite{ipdps2006},
\cite{seneff80}, and \cite{pca}, respectively. 

In the table, the measurements given in each column are obtained from
the stream graph as conceived by the programmer, before it is
transformed by our techniques.  The ``Filters'' columns gives the
total number of filters in the stream (including file input filters
and file output filters that are not mapped to cores).  The number of
filters that perform peeking is important because peeking filters
cannot be fused with upstream neighbors without introducing shared
state.  Thus, once a peeking filter is fused, it cannot be fissed. In
the table, the column labeled ``Shortest Path'' and ``Longest Path''
give the shortest path and the longest path from a source to a
sink. ``Comp / Comm'' gives the static estimate of the computation to
communication ratio of each benchmark for one steady-state
execution. This is calculated by totaling the computation estimates
across all filters and dividing by the number of items communicated
per steady-state. Notice that although the computation to
communication ratio is much larger than 1 across our benchmarks, we
demonstrate that inter-core synchronization is an important factor to
consider.

The benchmarks are sorted in ascending order of the final column,
``Stateful work''. This is percentage of the statically estimated work
performed per steady-state by all filters that have state divided by
the total work performed by all filters per steady-state.  Referring
to Figure \ref{fig:benchchar}, we see that three of our benchmarks
include stateful computation.  The stateful computation performed in
MPEGDecoder is insignificant and expressed as...
\textbf{Bill, can you talk about the statefull computation in
MPEG, radar, and vocoder}.


\begin{figure*}[t]
\centering
\psfig{figure=maingraph.eps, width=6.5in}
\caption{Task, Task + Data, and Task + Data + Software Pipeline
\protect\label{fig:main_comp}}
\end{figure*}

\subsection{Exploiting Coarse-Grained Data Parallelism}
To motivate the necessity of our parallelism extraction techniques let
us first consider the task parallel execution model.  This model
closely approximates a thread model of execution where the only form
of coarse-grained parallelism exploited is fork/join parallelism.  In
our implementation, the sole form of parallelism exploited in this
technique is the parallelism across the children of a splitjoin. The
first bar of Figure \ref{fig:main_comp} gives the throughput speedup
for the each of our benchmarks running in the task parallel model
executing on 16-core Raw normalized to sequential StreamIt executing
on a single core of Raw.  For the remainder of the presentation,
unless otherwise noted, we target all 16 cores of Raw.  The geometric
mean performance speedup for task parallel is 2.27x over sequential
performance. We can see that for most of our benchmarks, little
parallelism is exploited, notable exceptions are Radar,
ChannelVocoder, and FilterBank.  Each contains wide splitjoins of
load-balanced children.  In the case of BitonicSort, the task
parallelism is expressed at too fine a granularity for the
communication system.  Given that we are targetting a 16-core
processor, a mean speedup of 2.27x is inadequate.

\begin{figure}[t]
\centering
\psfig{figure=fine_data.eps, width=3.2in}
\caption{Fine-Grained Data Parallelism normalized to Single Core
\protect\label{fig:fine_data}}
\end{figure}
The StreamIt programming model facilitates relatively simple analysis
to determine opportunities for data parallelism.  But the granularity
of the transformations must account for the additional synchronization
incurred by data-parallelizing a filter.  If we attempt to exploit
data-parallelism a fine granularity, by simply replicating each
stateless filter across the cores of the architecture we run the risk
of overwhelming the communication substrate of the target
architecture.  To study this, we implemented a simple algorithm for
exposing data parallelism: replicate each filter by the number of
cores, mapping each to its own core.  We call this fine-grained data
parallelism. In Figure \ref{fig:fine_data}, we show this technique
normalized to single-core performance. Fine-grained data parallelism
achieves a mean speedup of 1.40x over sequential StreamIt. Note that
FilterBank is not included in Figure \ref{fig:fine_data} because the
size of the fine-grained data parallel stream graph stressed our
system. For four of our benchmarks, fine-grained duplication on 16
cores has lower throughput than single-core.

The second bar of Figure \ref{fig:main_comp} gives the speedup of
coarse-grained data parallelism over single-core StreamIt. The mean
speedup across our suite is 9.9x over single core and 4.36x over our
task parallel baseline.  BitonicSort, whose original granularity was
too fine, now achieves a 8.4x speedup over a single core. 6 of our 12
applications are stateless and non-peeking (BitonicSort, DCT, DES,
FFT, Serpent, and TDE) and thus fuse to one filter that is fissed 16
ways.  For these benchmarks the mean speedup is 11.1x over the
single core.  For DCT, the algorithm data-parallelizes the bottleneck
of the application (a single filter that performs more than 6x the
work of each of the other filters).  For DCT, coarse-grained data
parallelism achieves a 14.6x speedup over single-core, while
fine-grained achieves only 4.0x because it fisses at too fine a
granularity, ignoring synchronization.  Coarsening and then
parallelizing reduces the synchronization costs of data parallelizing.
For Radar and Vocoder, data parallelism is paralyzed by the
preponderance of stateful computation.

\subsection{Exploiting Coarse-Grained Software Pipeline Parallelism}

\begin{figure}[t]
\centering
\psfig{figure=softpipe_graph.eps, width=3.2in}
\caption{Task and Task + Software Pipeline
\protect\label{fig:softpipe_graph}}
\end{figure}
Our technique for coarse-grained software pipelining is effective for
exploiting coarse-grained pipelined parallelism (though it
under-performs when compared to coarse-grained data parallelism).
More importantly, combining software pipelining with our data
parallelism techniques, provides a cumulative performance gain, most
especially for applications with stateful computation.

Figure \ref{fig:softpipe_graph} considers coarse-grained software
pipelining and task parallelism normalized to single-core performance.
On average, software pipelining has a speedup of 7.7x over single core
(compare to 9.9 for data parallelism) and a speedup of 3.4x over task
parallelism. Software pipelining performs well when it can effectively
load-balance the packing of the dependence-free steady-state.  In the
case of Radar, TDE, FilterBank, and FFT, software pipelining achieves
comparable or better performance compared to data parallelism (see
Figure \ref{fig:thruput}).  For these applications, the workload is
not dominated by a single filter and the resultant schedules are
statically load-balanced across cores.  For the Radar application,
software pipelining achieves a 2.3x speedup over data parallelism and
task parallelism because there is little coarse-grained data
parallelism to exploit and it can more effectively schedule the
dependence-free steady-state.

However, when compared to data parallelism, software pipelining is
hampered by its inability to reduce the critical path when the
critical path contains stateless work (e.g., DCT, MPEGDecoder).  Also,
our data parallelism techniques tend to coarsen the stream graph more
than Selective Fusion, removing more synchronization.  For example, in
DES, the Selective Fusion Algorithm makes a greedy decision that it
cannot remove communication affecting the critical path workload.
Software pipelining performs poorly for this application when compared
to data parallelism, 6.9x versus 13.9x over single core, although it
calculates a load-balanced mapping.  Another consideration when
comparing software pipelining to data parallelism is that the software
pipelining techniques rely more heavily on the accuracy of the static
work estimation strategy, although it is difficult to quantify this
effect.

When we software pipeline the data-parallelized stream graph, we
achieve a 13\% mean speedup over data parallelism alone. The
cumulative effect is most prominent when the application in question
contains stateful computation (45\% over data parallelism).  For
example, the combined technique achieves a 69\% speedup over each
individual technique for Vocoder. For the 6 stateless benchmarks
without peeking, software pipelining enjoys a benefit versus data
parallelism because there exists small amounts of hardware pipeline
parallelism across steady-states.  For ChannelVocoder, FilterBank, and
FM, software pipelining further coarsens the stream graph without
affecting the critical path work (as estimated statically) and
performs inter-core communication in parallel.  Each reduces the
synchronization encountered on the critical path.

The combined technique depresses the performance of MPEG by 6\%
because the Selective Fusion component of the software pipeliner fuses
one step too far.  In most circumstances, fusion will help to reduce
inter-core synchronization by using the local memory of the core for
buffering. Consequently, the algorithm does not model the
communication costs of each fusion step. In the case of MPEG, it fuses
too far and adds synchronization. The combined technique also hurts
Radar as compared to only software pipelining because we fiss too
aggressively and create synchronization across the critical path.

%by fusing two elements of a splitjoin 
%that perform little work.  The combined filter communicates more data
%across the splitjoin than its sibling (who perform much more work).
%The critical path of the application is increased because the
%synchronization cost of splitting and joining increases. 

In Figure \ref{fig:thruput}, we report the compute utilization and the
MFLOPS performance (N/A for integer benchmarks) for each benchmark
employing the combination of our techniques, task plus data plus
software pipeline parallelism. Note that for our target architecture,
the maximum number of MFLOPS achievable is 7200.  The compute
utilization is calculated as the number of instructions issued on each
computer processor divided by the total number possible for a
steady-state.  The utilization accurately models pipeline hazards and
stalls of Raw's single issue, in-order processing cores.  We achieve
generally excellent compute utilization; in 7 cases the utilization is
60\% or greater.


\subsection{Comparison To Previous Work}

\begin{figure}[t]
\centering
\psfig{figure=vs_space_graph.eps, width=3.2in}
\caption{Task + Pipeline and Task + Data + Software Pipeline
\protect\label{fig:vs-space}}
\end{figure}

\begin{figure*}[t]
\centering
\psfig{figure=thruput.eps, width=6.5in}
\caption{Comparison and Task + Data + Software Pipeline Performance Results
\protect\label{fig:thruput}}
\end{figure*}
In Figure \ref{fig:vs-space} we show our combined technique normalized
to our previous work for compiling streaming applications to multicore
architectures.  This baseline configuration is a maturation of the
ideas presented in \cite{streamit-asplos} and implements a task plus
pipeline parallel execution model relying solely on on-chip buffering
and the on-chip static network for communication and
synchronization. Our new techniques achieve a mean speedup of 1.84x
over this baseline. \footnote{Please note that the version of
\cite{streamit-asplos} published in the conference proceedings 
contained incorrect MFLOPS measurements.}


For most of our benchmarks, the combined techniques presented in this
paper offer improved data parallelism, improved scheduling
flexibility, and reduced synchronization compared to our previous
work. This comparison demonstrates that combining our techniques is
important for generalization to stateful benchmarks.  For Radar, data
parallelism loses to this baseline by 19\%, while the combined
technique enjoys a 38\% speedup. For Vocoder, data parallelism is 18\%
slower, while the combined technique is 30\% faster.

Task plus pipeline parallelism performs exceptionally well for
applications that contain long pipelines that can be load-balanced,
including FFT, TDE, and Serpent.  For example, the stream graph for
Serpent is a pipeline of identical splitjoins that is fused down to a
balanced pipeline.  Task plus pipeline parallel incurs less
synchronization and consequently its compute utilization is higher
(64\% versus 57\%).  The combined approach fuses Serpent to a single
filter and then fisses it 16 ways, converting the pipeline parallelism
into data parallelism.  Fission communication using a splitjoin is
more expensive than the pipeline communication of task plus pipeline
parallelism.  However, although Raw's on-chip network is well-suited to
pipeline communication, most other contemporary multicore
architectures do not provide a low-latency, fine-grained on-chip
interconnect enabling optimized pipeline parallelism between cores.


%For
%example, the single data parallel bottleneck in DCT is fissed only
%two ways in the baseline before the remainder of the pipeline becomes
%the bottleneck.  The partitioner DES

%DCT: must be partitioned to 16 tiles, the bottleneck is fissed 2 ways,
%but the remaining stages must be fused to 4-way and they become the
%bottleneck, plus synchronization!

%For MPEGDecoder, the space partitioner must fuse down to 16 tiles removing task
%parallelism of the program, and the work is not evenly distributed
%across the application, resulting partitioning is not load-balanced,
%dominated by a single filter that does more than 2x the amount of work
%than the next largest filter.

%DES:somewhat complicated graph repeated 4 time between some filters,
%space partitioner is forced to partition these 4 containers (8 filters
%each) into 2 filters that are not load balanced.  Synchronization
%issues not alleviated when forced to fuse to pipeline.

%Also the way we simulate input differs between the two backends.  Task
%+ Pipeline attaches the input and output devices to the chip in the
%place of dram, and the input is continuously streamed onto the chip.
%The new backend fetches input and writes output from/to the DRAMs,
%generating a DRAM command.

