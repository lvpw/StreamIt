\section{Implementation} 

To facilitate evaluation of our techniques, we have implemented a
fully-automatic compiler that supports each of the execution models
discussed in this paper.  The compiler targets the Raw
microprocessor.  In this section we will first give a description of
the Raw architecture followed by implementation details of the 
execution models.

\subsection{The Raw Architecture}
\label{sec:raw}

%\begin{figure}
%\centering
%\psfig{figure=raw-diagram.eps,width=3in}
%\caption{Block diagram of the Raw architecture.
%\protect\label{fig:raw-diagram}}
%\end{figure}
The Raw microprocessor~\cite{raw10,raw} addresses the wire delay
problem~\cite{raw13} by providing direct instruction set architecture
(ISA) analogs to three underlying physical resources of the processor:
gates, wires and pins. The architecture exposes the gate resources as
a scalable 2-D array of identical, programmable cores, that are
connected to their immediate neighbors by four on-chip networks.
Values routed through the networks off of the side of the array appear
on the pins, and values placed on the pins by external devices
(wide-word A/Ds, DRAMS, etc.) appear on the networks.  Each of the
cores contains a compute processor, some memory and two types of
routers--one static, one dynamic--that control the flow of data over
the networks as well as into the compute processor.  The compute
processor interfaces to the network through a bypassed,
register-mapped interface~\cite{raw10} that allows instructions to use
the networks and the register files interchangeably.

Because we generate bulk DRAM transfers, we do not want these
optimizable accesses to become the bottleneck of the hardware
configuration.  So, we employ a simulation a CL2 PC 3500 DDR DRAM,
which provides enough bandwidth to saturate both directions of a Raw
port~\cite{raw_isca}.  Additionally, each chipset contains a streaming
memory controller that supports a number of simple streaming memory
requests. In our configuration, we attach 16 such DRAMs to each of the
16 logical ports of the chip.  The chipset receives request messages
over the dynamic network for bulk transfers to and from the DRAMs.
The transfers themselves can use either the static network or the
general dynamic network (the desired network is encoded in the
request).

The results in this paper were generated using btl, a cycle-accurate
simulator that models arrays of Raw cores identical to those in the
.15 micron 16-core Raw prototype ASIC chip, with a target clock rate
of 450 MHz. The core employs as compute processor an 8-stage, single
issue, in-order MIPS-style pipeline that has a 32 KB data cache, 32 KB
of instruction memory, and 64 KB of static router memory.  We
augmented the simulator to include a 2-way set associative hardware
instruction caching mechanism that is serviced over the dynamic
network, with resource contention modeled accordingly. 

%\subsection{Task + Pipeline Parallel Execution}
%\label{sec:space-imp}
%Talk about this as our previous work, not leveraging!!, reference the
%results sections that this appears in.  


%To compile for the task plus (hardware) pipeline parallel execution
%model, we leverage our previous work~\cite{streamit-asplos}.  In this
%model we require that the number of filters in the stream graph be
%less than or equal to the number of processing cores of the target
%architecture.  To achieve this, we repeatedly apply fusion and fission
%transformations as directed by a dynamic programming algorithm, termed
%the {\it partitioner}.  The algorithm fisses individual filters or
%fuses contiguous sections of the stream graph in the hope of creating
%a load balanced stream graph ready for execution.

%The resulting nodes of the stream graph are executed in a pipelined
%manner.  We calculate the mapping of nodes to Raw cores using
%simulated annealing, with at most one node mapped to each core.
%Communication bypasses the memory system as the nodes communicate over
%the Raw's static network.  Because the static network is blocking and
%has little buffering, the simulated annealing cost function attempts
%to model the synchronization costs of each layout configuration
%encountered during the search.

\subsection{StreamIt Compiler Infrastructure}
The techniques presented in this paper are evaluated in the context of
the StreamIt compiler infrastructure.  The system includes a
high-level stream IR with a host of graph transformations including
graph canonicalization, synchronization removal, refactoring, fusion,
and fission~\cite{streamit-asplos}.  Also included are domain specific
optimizations for linear filters (e.g., FIR, FFT, and
DCT)~\cite{lamb:pldi:2003}, state-space
analysis~\cite{agrawal:cases:2005}, and cache
optimizations~\cite{sermulins:lctes:2005}.  The infrastructure bundles
multiple architecture-specific backends.  We leverage StreamIt's
spatially-aware Raw backend for this work.

Previously, we described hardware and software pipelining as two
distinct techniques for exploiting pipeline parallelism.  The StreamIt
compiler has full support for each.  However it also implements a
hybrid approach where hardware pipelined units are scheduled in a
software pipelined loop.  While we were excited by the possibilities
of the hybrid approach, it does not provide a benefit on Raw due the
tight coupling between processors and the limited FIFO buffering of
the network. Although these are not a fundamental limits of the
architecture, they enforce fine-grained orchestration of communication
and computation that is a mismatch for our coarse-grained execution
model.

%Also, multiple filters from non-local sections of the stream graph
%can be assigned to each processing core.  Across the cores, filters
%execute in dataflow order, with each filter reading its input from and
%writing its output to shared memory. Each core executes a schedule of
%filters assigned to it and each filter executes on a single processing
%core for the entire execution. Each core executes independently until
%a split or join point is reached.

%We experimented with numerous techniques for forming {\it spatial}
%pipelines of filters mapped to different cores that communicate among
%themselves using only on-chip resources.  Our best techniques afforded
%no performance gains.  We have concluded that it is extremely
%difficult for a compiler operating at a coarse granularity to manage
%the fine-grained synchronization required to map stream communication
%to Raw's on-chip static network.  Additionally, the techniques
%presented in this paper are general since nearly all commercial
%multicores do not provide fine-grained, low-latency inter-core
%communication.

In the compiler, we elected to buffer all streaming data off-chip
because given the Raw configuration we are simulating and for the
regular bulk memory traffic we generate, it is more expensive to
stream data from a core's local data cache than to stream the data
from the streaming memory controllers. A load hit in the data cache
incurs a 3-cycle latency.  So although the networks are
register-mapped, two instructions must be performed to hide the
latency of the load, implying a maximum bandwidth of 1/2 word per
cycle, while each streaming memory controller has a load bandwidth of
1 word per cycle for unit-stride memory accesses.

Streaming computation requires that the target architecture provides
an efficient mechanism for implementing split and join (scatter and
gather) operations.  The StreamIt compiler programs the switch
processors to form a network to perform the splitting (including
duplication) and the joining of data streams.  Since Raw's DRAM ports
are banked, we must read all the data participating in the
reorganization from off-chip memory and write it back to off-chip
memory.  The disadvantages to this scheme are that all the data
required for joining must be available before the join can commence
and the compute processors of the cores involved are idle during the
reorganization.  Architectures that include decoupled DMA engines
(e.g., Cell) can overlap splitting/joining communication with useful
computation.

\begin{figure*}[t]
\centering
\psfig{figure=benchchar.eps, width=6.15in}
\caption{Benchmark descriptions and characteristics.
\protect\label{fig:benchchar}}
\vspace{-6pt}
\end{figure*}

\subsubsection{Baseline Scheduler}
Our scheduling techniques for software pipelining of the stream graph
were presented in Section \ref{sec:softpipe}.  To facilitate
evaluation of coarse-grained software pipelining, we implemented a
separate scheduling path for a non-software pipelined schedule that
executes the steady-state respecting the dataflow dependencies of the
stream graph.  This scheduling path ignores the presence of the
encompassing outer loop in the stream graph and is employed for the
task and task + data parallel configurations of the evaluation section.

This scheduling problem is equivalent to static scheduling of a
coarse-grained dataflow DAG to a multiprocessor which has been a
well-studied problem over the last 40 years (see~\cite{DAGSched} for a
good review).  We once again leverage simulated annealing as
randomized solutions to this static scheduling problem are superior to
other heuristics~\cite{kwok99fastest}.

Briefly, we generate an initial layout by assigning the filters of the
stream graph to processors in dataflow order, assigning a random
processing core to each filter. The simulated annealing perturbation
function randomly selects a new core for a filter, inserting the
filter at the correct slot in the core's schedule of filters based on
the dataflow dependencies of the graph.  The cost function of the
annealer uses our static work estimation to calculate the maximum
critical path length (measured in cycles) from a source to a sink in
the graph.  After the annealer is finished, we use the configuration
that achieved the minimum critical path length over the course of the
search.  Finally, we run a greedy assignment algorithm to assign the
buffers representing the channels of the stream graph to DRAM ports,
attempting to minimize synchronization costs.

\subsubsection{Optimizations}

For the results detailed in the next section we ran a host of
optimizations including function inlining, constant propagation,
constant folding, array scalarization, and loop unrolling (with a
factor of 4).  These optimizations are especially important for a
fused filter, as we can possibly unroll enough to scalarize
constituent splitter and joiner buffers, eliminating the shuffling
operations.  Finally, the two backends produce a mix of C and assembly
code that is compiled with GCC 3.4 at optimization level 3.
