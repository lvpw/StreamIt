\section{Related Work}
\label{sec:related}

Previous work in scheduling computation graphs to parallel targets has
focused on partitioning and scheduling techniques that exploit task
and pipeline parallelism~\cite{SDFSched, SDFSched2,
may87communicating, DAGSched, pipeline-sdf}.  Application of
loop-conscience transformations to coarse-grained dataflow graphs has
been investigated.  Unrolling (or ``unfolding'' in this domain) is
employed for SDF graphs to reduce the initiation interval but they
they do not consider real
architectures~\cite{unfolding,unfolding2}. Software pipelining
techniques have been employed for scheduling synchronous dataflow
graphs onto various embedded and DSP
targets~\cite{bakshi99,chatha-02}, but requires a programmer knowledge
of both the application and the architecture. None of these systems
exploit the combination of task, data, and pipeline parallelism.
Furthermore, these systems do not provide a robust end-to-end path for
application parallelization from a high-level, portable programming
language.

Previous work on instruction-level software pipelining has focused
mostly on scheduling machine instructions in a loop via modulo
scheduling~\cite{rau81, lam-softpipe}.  The algorithms devised must
account for tight resource constraints and complex instruction
dependences. Our software-pipelining problem is much less constrained,
enabling us to employ a simple greedy heuristic.  Furthermore, a
traditional modulo scheduling algorithm is not needed because we have
an implicit loop barrier at the end of each steady-state.  ILP
compilers for clustered VLIW architectures~\cite{Bulldog, Multiflow,
lee98spacetime, qian02} must partition instructions and assign them to
clusters as part of the instruction scheduling. Clustering is
analogous to our application of filter fusion in our software
pipelining algorithm. 

% MIKE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% BILL

The Imagine stream processor~\cite{rixner98bandwidthefficient}
supports a time-multiplexed execution model.  The architecture
contains 48 parallel ALU's organized into 6 VLIW clusters.  The
programming model requires the programmer to write computation filters
in Kernel-C and stitch them together using Stream-C.  Because the
execution unit is data-parallel, the compiler uses time multiplexing
to execute a single filter at a time across all of the parallel
clusters.  While this provides perfect load balancing and high
arithmetic utilization when there is abundant data parallelism, it
suffers when a filter has retained state or data-dependences between
iterations.  Moreover, architectures based solely on time-multiplexing
do not scale spatially, as there are global wires orchestrating the
parallel execution units.

- partitioning stuff
- SDF
