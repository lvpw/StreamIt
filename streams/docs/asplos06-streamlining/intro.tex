\section{Introduction}

%% Possibly add figure with stream graph that labels task, data, and pipeline parallelism

As centralized microprocessors are ceasing to scale effectively,
multicore architectures are becoming the industy standard.  For
example, the IBM/Toshiba/Sony Cell processor has 9 cores\cite{Cell-hpca}, Sun
Niagara has 8 cores\cite{Niagara}, RMI XLR732 has 8 cores\cite{RMI-web},
IBM/Microsoft Xenon has 3 cores, and most vendors are shipping
dual-core chips.  Cisco has described a next-generation network
processor containing 192 Tensilica Xtensa cores~\cite{etherton05ancs}.
%The Intel IXP 2800 network processor has 1 core and 16 micro-engines.
This trend has pushed the performance burden to the compiler, as
future application-level performance gains depend on effective
parallelization across the cores.  Unfortunately, traditional
programming models such as C, C++ and FORTRAN are ill-suited to
multicore architectures because they assume a single instruction
stream and a monolithic memory.  Extracting coarse-grained parallelism
suitable for multicore execution amounts to a heroic compiler analysis
that remains largely intractable.

The stream programming paradigm offers a promising approach for
exposing parallelism suitable for multicore architectures.  Stream
languages such as StreamIt\cite{streamitcc}, Brook\cite{brook04},
Baker\cite{Baker}, and Spidle\cite{spidle02} are motivated not only by
trends in computer architecture, but also by trends in the application
space, as network, image, voice, and multimedia programs are becoming
only more prevalent.  In the StreamIt Language, a program is represented
as a set of autonomous actors that communicate through FIFO data
channels (see Figure~\ref{fig:vocoder}).  During program execution,
actors fire repeatedly in a periodic schedule.  As each actor has a
separate program counter and an independent address space, all
dependences between actors are made explicit by the communication
channels.  Compilers can leverage this dependence information to
orchestrate parallel execution.

Despite the abundance of parallelism in stream programs, it is
nonetheless a challenging problem to obtain an efficient mapping to a
multicore architecture.  Often the gains from parallel execution can
be overshadowed by the costs of communication and synchronization.  In
addition, not all parallelism has equal benefits, as there is
sometimes a critical path that can only be reduced by running certain
actors in parallel.  Due to these concerns, it is critical to leverage
the right combination of task, data, and pipeline parallelism while
avoiding the hazards associated with each.

Task parallelism refers to pairs of actors that are on different
parallel branches of the original stream graph, as written by the
programmer.  That is, the output of each actor never reaches the input
of the other.  In stream programs, task parallelism reflects logical
parallelism in the underlying algorithm.  It is easy to exploit by
mapping each task to an independent processor and splitting or joining
the data stream at the endpoints (see Figure~\ref{fig:exemodel}b).
The hazards associated with task parallelism are the communication and
synchronization associated with the splits and joins.  Also, as the
granularity of task parallelism depends on the application (and the
programmer), it is not sufficient as the only source of parallelism.

Data parallelism refers to actors that have no dependences between one
execution and the next.  Such ``stateless'' actors\footnote{A
stateless actor may still have read-only state.}  offer unlimited data
parallelism, as different instances of the actor can be spread across
any number of computation units (see Figure~\ref{fig:exemodel}c).
However, while data parallelism is well-suited to vector machines, on
coarse-grained multicore architectures it can introduce excessive
communication overhead.  Previous data-parallel streaming
architectures have focused on designing a special memory hierarchy to
support this communication\cite{imagine-ieee}.  The other hazards of data
parallelism include increased buffering and latency, as well as the
inability to parallelize actors with state.

Pipeline parallelism applies to chains of producers and consumers that
are directly connected in the stream graph.  In our previous
work\cite{streamit-asplos}, we exploited pipeline parallelism by
mapping clusters of producers and consumers to different cores and
using an on-chip network for direct communication between actors (see
Figure~\ref{fig:exemodel}d).  Compared to data parallelism, this
approach offers reduced latency, reduced buffering, and good locality.
It does not introduce any extraneous communication, and it provides
the ability to execute any pair of stateful actors in parallel.
However, this form of pipelining introduces extra synchronization, as
producers and consumers must stay tightly coupled in their execution.
In addition, effective load balancing is critical, as the throughput
of the stream graph is equal to the minimum throughput across all of
the processors.

In this paper, we present two novel techniques for overcoming the
traditional limitations in exploiting task, data, and pipeline
parallelism on multicore architectures.  The first technique leverages
data parallelism, but avoids the communication overhead by first
increasing the granularity of the stream graph.  Using a program
analysis, we fuse actors in the graph as much as possible so long as
the result is stateless.  Each fused actor has a significantly higher
computation to communication ratio, and thus incurs significantly
reduced communication overhead in being duplicated across cores.  To
further reduce the communication costs, the technique also leverages
task parallelism: two task-parallel actors need only be split across
half of the cores in order to obtain high utilization.
%While this technique exploits data parallelism, it also relies on the
%producer-consumer relationships evident in stream programs for
%increasing the granularity of the graph.

The second technique leverages pipeline parallelism.  However, to
avoid the pitfall of synchronization, it employs software pipelining
techniques to execute actors from different iterations in parallel.
While software pipelining is traditionally applied at the instruction
level, we leverage powerful properties of the stream programming model
to apply the same technique at a coarse level of granularity.  This
effectively removes all dependences between actors scheduled in a
steady-state iteration of the stream graph, greatly increasing the
scheduling freedom.  Like hardware-based pipelining, software
pipelining allows stateful actors to execute in parallel.
% and avoids the communication overhead of data parallelism.  
However, it avoids the synchronization overhead because processors are
reading and writing into a buffer rather than directly communicating
with another processor.

Our techniques for exploiting coarse-grained data and pipeline
parallelism are complementary, as data-parallel actors can also be
scheduled using software pipelining.  As described in our results
section, either technique in isolation offers significant speedups,
Executing on 16 cores, coarse-grained pipeline parallelism achieves a
mean speedup of 7.7x over single core performance and 3.4x over a task
parallel baseline on 16 cores.  Coarse-grained data parallelism
achieves 9.9x and 4.4x, respectively. Combining the techniques yields
the most general results, as data parallelism offers good load
balancing for stateless actors while software pipelining enables
stateful actors to execute in parallel.  The mean speedup of the
combined technique executed on 16 cores is 11.2x over single core and
5.0x speedup over task parallelism on 16 cores.

%% Other Multicores (Trademarks?):
%% RMI XLR Family of Processors
%%   (http://razamicroelectronics.com/products/xlr.htm)
%% Cavium OCTEON Processors?  cavium.com
%% Rapport KC256 www.rapportincorporated.com
%% PicoChip's PicoArray

\begin{figure}[t]
\centering
\psfig{figure=vocoder.eps,width=3in}
\caption{Stream graph for a simplified subset of our Vocoder
benchmark.  Following a bank of sliding DFTs, the signal is converted
to polar coordinates.  Node {\tt B2} sends the magnitude component to
the left and the phase component to the right.  In this simplified
example, no magnitude adjustment is needed.\label{fig:vocoder}}
\end{figure}

\begin{figure*}[t]
\psfig{figure=time-proc-4-1.eps,width=7in}
\caption{Parallel execution models for stream programs.  Each block corresponds to a filter in the Vocoder example (Figure~\ref{fig:vocoder}).  The height of the block reflects the amount of work contained in the filter.\label{fig:exemodel}}
\end{figure*}
