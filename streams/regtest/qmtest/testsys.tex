\documentclass[11pt]{article}

\usepackage{booktabs}
% \textsf looks terrible in the postscript world, but quite nice in
% the Computer Modern world.  Using the pslatex package makes it
% look less bad, but still not great.  --dzm
%\usepackage{pslatex}
%\usepackage{palatino}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{streamit}

\title{A QMTest-Based Test System for StreamIt}
\author{David Maze\\\texttt{dmaze@cag.lcs.mit.edu}}

\begin{document}

\maketitle
\tableofcontents

\section{Motivation}

The StreamIt compiler is becoming an increasingly complex piece of
software, with several independent back-ends and different compiler
flows.  Our previous test system was based on the Java JUnit test
infrastructure, a system designed for adding light-weight unit tests
to existing Java classes; upon this, we retrofitted a system that
could run a single compiler flow by invoking the Java runtime several
times.  Since then, we have developed a script, \textsf{strc}, to run
the compiler, added standardized sets of options, and added at least
one potential alternate flow to the compiler; we also have a desire to
test the Java library ``compiler'' flow path.

The JUnit infrastructure has become increasingly dated.  Potential
compiler flags were implemented as a 32-bit integer bitfield; when the
flags used for the \textsf{strc} standard optimization sets were
added, 30 of the 32 bits were used.  The old test system could never
support the Java library path without significant coding, and it will
be difficult to add support for either frontend-to-Kopi conversion or
the cluster backend.  It also proves to be very difficult to run the
test system as a set of parallel jobs.

Other people have recommended using the
\href{http://www.qmtest.com/}{QMTest} infrastructure for more involved
system tests.  QMTest is written entirely in Python, but this makes it
easy to add more tests by writing simple Python classes.  Tests can
have dependencies on other tests, and can be run hierarchically; ``run
all of the FM radio tests'' is easy to do, ``run all of the RAW
tests'' is straightforward (but slightly less easy).  QMTest also has
built-in support for running tests in parallel, though customizing it
to our environment may require writing code.

\section{Implementation}

This incarnation of the test system is entirely in Python, with some
additional XML.

\subsection{Directory Layout}

The default setup for QMTest is to have a ``test
database'' consisting of a set of directories with embedded XML files;
a \textsf{QMTest} directory at the top level contains additional
control information.  The \textsf{build-qmtest.py} script reads a
control file, then builds a directory tree that QMTest will notice.
By default this builds a tree parallel to the StreamIt \textsf{apps}
directory named \textsf{apps.qms}, copying in files from the main tree
as needed.

The script searches for files named \textsf{benchmark.xml} and reads
those.  In particular, the relevant XML file must have one or more
\verb|<impl>|s for the ``StreamIt'' language; all files listed there
are copied into all of the child directories of the matching QMTest
directory.  Implementations are named by the ``name'' attribute of the
corresponding XML tag.
%, or as ``impl1'', ``impl2'', and so on.

The QMTest name for a particular test then consists of the path to the
XML file, the implementation name, the backend/optimization name, and
then the name of the actual test.  So, for example,
\textsf{apps.benchmarks.fm.impl1.raw4\_o2.run} would be the QMTest test
that runs the code produced by \texttt{strc -{}-raw 4 -O2} on the only
implementation in \textsf{apps/benchmarks/fm}.  The test components
are named \textsf{compile}, \textsf{run}, and \textsf{verify}.

\paragraph{Java library.}  Compilation for the Java library uses the
backend name \textsf{library}.  Since \textsf{strc} runs the compiled
program on its own, there is no \textsf{run} stage.

\paragraph{Uniprocessor backend.}  Compilation for the uniprocessor
backend uses the backend name \textsf{uni}, along with
\textsf{uni\_o1} and \textsf{uni\_o2} for optimized compilation.

\paragraph{RAW backend.}  Compilation for a 4x4 RAW chip uses the
backend name \textsf{raw4}, along with \textsf{raw4\_o1} and
\textsf{raw4\_o2} for optimized compilation.  Unlike in the old test
system, the compilation stage does not build the output of the
compiler; we let the execution stage handle this, since the RAW
makefile system will do this on its own.

In all cases, the directory structure matches the name of the test,
with the final component being a file in the target directory.  To be
recognized by QMTest, directory names end with \texttt{.qms}, and test
files with \texttt{.qmt}.

\subsection{Python Scripts}

As mentioned previously, \textsf{build-qmtest.py} generates the
directory layout for QMTest to use.  It does the work of scanning the
directory tree for \textsf{benchmark.xml} files, creating directories,
and creating XML test files.  The Python XML DOM infrastructure is
used to create the various files, which get written out.

These reference QMTest Test classes, which are by and large defined in
\textsf{streamit.py}.  There are three classes here, which describe the
three stages of the StreamIt tests.  Of particular note, the
``verify'' stage is implemented within a single Python function, where
before it was a set of three separate Perl scripts.  Each class
contains an \textsf{arguments} field, which corresponds exactly to the
set of \verb|<argument>| tags that may be declared in the XML test
files.

QMTest also depends on several files being in the \textsf{QMTest}
subdirectory of the top level of the directory tree.  These are the
files \textsf{classes.qmc} and \textsf{configuration}, along with the
\textsf{streamit.py} file which provides the test classes.

\section{Extensibility}

Extensibility was a key concern for this system.  It should prove much
easier to add to the system than to the original system.

\subsection{New Benchmarks}

In the old system, adding benchmarks involves modifying the Java
source for the regtest.  In this system, a new test (or a new
implementation) just involves adding or modifying a
\textsf{benchmark.xml} file; compilation or execution will always
happen, and verification will happen if there is a
\verb|<file class="output">| in the implementation.

\subsection{Fewer Benchmarks}

It is somewhat easy to run QMTest on a set of fewer benchmarks than
all of them.  You can create a test suite using the QMTest GUI, or
reverse-engineer the suite XML file format and programmatically create
a list of tests or suites.  A straightforward future extension will be
to add support for running all of the benchmarks for a particular
backend.  You can already use a truncated path, such as \textsf{qmtest
  run apps.benchmarks.fm} to run all implementations and all
configurations of the FM Radio benchmark.

\subsection{Different Compiler Flags}

The \textsf{regtest.xml} file controls some high-level behavior of the
system, and is read by \textsf{build-qmtest.py} to assemble the
directory tree.  To add or remove compiler flags, add or remove
\verb|<option>| tags from this file.

\subsection{Different Compiler Flows}

Any change in the compiler flow that is hidden within \textsf{strc} is
irrelevant to this test system.  For example, \textsf{strc} handles
the experimental \verb|--tokopi| option to not generate an
intermediate Java file; testing this would just involve adding the
compiler flag to the control file.

For more involved changes, you may need to write a QMTest object.  Add
it to \textsf{streamit.py}.  \textsf{build-qmtest.py} also needs to be
updated to add or create the relevant \texttt{.qmt} file.  For
example, you could use this technique to separately test whether our
generated RAW source code compiles by adding a stage in between the
``compile'' and ``run'' stages.  (This could also be accomplished by
modifying the \textsf{RunStrcTest} Python test class.)

\subsection{Adding Backends}

Adding a backend is probably the most involved change in the current
system.  This requires moderate changes in both \textsf{streamit.py}
and \textsf{build-qmtest.py}, along with adding the backend to
\textsf{regtest.xml}.  In particular, there are places which test the
backend flag for validity, and places which dispatch based on the
backend and assume it is one of the extant backends.

\subsection{Data Files}
The description of the structure of the .xml files is based on looking
at build-qmtest.py and determining what tags it uses.  If it does not
use a tag that is in an example file, then the tag is listed as
optional and shown in the sequence in which it appears on some
randomly-chosen example file.

\subsubsection{regtest.xml}
Format: \verb@regtest (test | option(CDATA) | whichtype(CDATA))+@

All \verb+<test root="DIRNAME">+ indicate directories that might have 
benchmarks.xml files in directory subtrees. (build-qmtest.py processes
this file and does not follow symbolic links in determining the
directory subtree.)

All \verb+<option target="BACKEND">COMMAND_LINE_OPTIONS</option>+
indicate back ends that will be used for all tests.

Current legal values for the target attribute are  ``uni'', ``raw4'', or 
``cluster''.

All \verb+<whichtype>TESTING_TYPE</whichtype>+ provide keywords indicating a 
subset of tests to be run.
If there are no \verb+<whichtype>+ elements, then all tests found in
{\tt benchmark.xml} files will be considered for running subject to
the constraints in the \verb+<impl>+ elements in the {\tt
benchmark.xml} files .
If there are \verb+<whichtype>+ elements, then only the tests with one
or more matching \verb+<whichtype>+ elements in their \verb+<impl>+
will be considered for running.


\subsubsection{benchmark.xml}
Format:\begin{verbatim}
benchmark 
  (name(CDATA)?, desc(CDATA)?, description(CDATA)?, reference(CDATA)?
   implementations 
     (impl
        (desc(CDATA)? | file(CDATA)+ | whichtype(CDATA)+ | make(CDATA)?)))
       \end{verbatim}%
where {\tt impl} has attributes
\begin{itemize}
\item {\tt regtest} optional.  If value is {\tt skip} then skip
  processing of {\tt file}s in this {\tt impl} 
\item {\tt dir} optional to indicate that {\tt file}s are in a subdirectory.
The value is the relative address of the subdirectory from the
directory containing the benchmark.xml
\item {\tt id} optional, used for naming test directories, for
  reporting.  If absent defaults to ``impl'' plus a sequence number.
\item {\tt lang} Language to use in testing.  build-qmtest.py filters
  out all {\tt impl}s where {\tt lang} $\neq$ {\tt StreamIT}.
\end{itemize}

{\tt make} names a make file, which is run before processing {\tt file}s.
Since the text is just processed by ``make -f ...'', you can put targets, 
switches, ... on this line as well.

{\tt file} has one attribute, {\tt class}.  If the value of {\tt class}
is "source" or if class is missing then the file name is of a source
file.  If {\tt class} is "output" then the file name is the is expected
output.  If there is no "output" file, then the source is compiled and
run, but any output from the run is considered to pass the test.

A {\tt whichtype} has a text field indicating that the test is to be run
if the text matches one of the whichtypes from {\tt benchmarks.xml}.
This is used to categorize tests for inclusion into various testing
scenarios: regression testing, testing before a CVS commit, ... 
If either {\tt benchmarks.xml} does not specify any {\tt whichtype}s
or the {\tt impl} does not specify any {\tt whichtype}s then the test
will be run.

\subsubsection{classes.qmc}
Points QM at some overridden or extended classes in {\tt streamit.py}.

\subsubsection{configuration}
Tells QM what sort of ``database'' of tests we are using:  We are curently
using a directory-structured database where the leaves contain {\tt  xml} 
files giving instructions to QM as well as containing the tests themselves.

\subsubsection{results.qmr}
QM results file in undocumented internal format (where both the
format, and the internal routines to access the elements of the file
may change between versions of QM).  This is the file produced by
``qmtest run''.

\subsubsection{results.xml}
This is a version of the QM results file as processed by ``qmtest report''.
the programs {\tt summarize_xml_results} and {\tt rt-results} run off
the contents of this file.
Unfortunately, there is no DTD for this file, and as of QM 2.3, there
was no guarantee from CodeSourcery that the file format would not
change.
However, we expect that the data that we process from the {\tt results.xml}
file will not change format.

\subsubsection{Directories}
{\tt run-reg-tests.py} builds the following directory structure:
A root directory with a name derived from the date and time that the
regression test was started.
This directory contains log files from pre- and post-processing
phases, and two subdirectories.

The ``rt'' subdirectory is where the files that describe the
regression test to the RT ticket-management system will reside after
testing. (There is an extension to RT to allow browsing these files.)

The ``streams'' subdirectory is a fresh {\tt cvs} checkout of the
entire StreamIt source tree.  This checkout includes some of the
programs used in regression testing.

In the ``streams'' subdirectory, regression testing builds two new
subdirectories and two files:  The files ``results.qmr'' and
``results.xml'' are described above.
The new subdirectories are ``QMTest'' and ``apps.qms''.

The ``streams/QMtest'' subdirectory contains the files needed to
tailor QM to our needs:  ``configuration'', ``classes.qmc'', and the
source and compiled version of ``streamit.py'' which contains extesion
classes adapting the QM test framework to out needs.

The ``streams/apps.qms'' subdirectory is the root of the test
database.  Conditer a path under ``apps.qms'' to be similar to the
path under ``streams/apps'' to a benchmark, except that ``.qms'' is
added to the name of each subdirectory.  There is a formatting
difference: QM requires that all letters are lowercase and that
hyphens and other odd characters in directory and file names be
removed.

At the leaf of a  ``streams/apps.qms'' directtory, find: 
\begin{itemize}
\item a {\tt .str} file to test
\item optionally, an output file to verify the results of running the 
compiled program,
\item {\tt compile.qmt}, {\tt run.qmt}, and {\tt verify.qmt}
containing instructions to QM in {\tt xml} format.
This includes the name of the class that performs the test, the test
name, any arguments needed to run the test, and any prerequisites
needed to order tests correctly.
\end{itemize}
{\tt build-qmtest.py} creates the ``apps.qms'' directory structure and
the {\tt .qmt} files based on {\tt impl} fielde in a {\tt benchmark.xml}
file and {\tt option}s in the {\tt regtest.xml} file.

\subsection{Programs and Control Flow}

\subsubsection{streamit.py}
Extends / overrides some QM methods, and contains a few utility routines.

The current version includes its own version of ``TimedExecutable''
since that was not supplied in QMTest 2.0 (It appears in later
versions).  This version of TimedExecutable does not work with
``process targets'':
\begin{verbatim}
qmtest create-target -a processes=4 p4 process_target.ProcessTarget
qmtest run
\end{verbatim}
will occasionally come up withe an EACCESS error (Error 13) when the
provess target performs some exec functions before TimedExecutable can
perform a setpgid.

The three Run methods: RunStrcTest, which compiles; RunProgramTest,
which executes; and CompareResultsTest, which verifies output
all currently require that a chdir is made to the proper test
directory.
This makes it impossible to use a thread target
\begin{verbatim}
qmtest run -j4
\end{verbatim}
since threads and chdir do not interact well.
A common symptom of attempting this is verification errors trying to
verify one test's output against another test's expected output file.

\subsubsection{run-reg-tests.py}
\begin{itemize}
\item check out clean directory:
 Under directory named in global variable {\tt regtest\_root} check
 out a fresh copy of streams from CVS, build the compiler (src), the C
 library (library/c) and the raw toolchain (misc/raw),
\item use streamitqm / run-reg-tests.py to convert from our .xml files to QM
  files,
\item run QM,  ({\tt qmtest run} followed by  {\tt qmtest report})
\item run summarize_xml_results to create summary report and mail it to all.
\item run rt-results to build files usable by RT for detailed
  reporting of the results,
\item and set the directory built by this run to be the ``latest'' results.
\end{itemize}

\subsubsection{streamitqm}
Shell script used to get from run-reg-tests.py to 
./regtest/qmtest/build-qmtest.py
plus some file handling.  run-reg-tests.py expects this file to be in 
\$STREAMIT\_HOME/regtest/qmtest/streamitqm.

\subsubsection{build-qmtest.py}
Executable, invoked with 1 parameter: the complete path and file name for the 
regtest.xml file.  streamitqm expects this file to be in './regtest/qmtest'.
Creates files {\tt compile.qmt}, {\tt run.qmt}, and {\tt verify.qmt}
for each combination of backend and options.

\subsubsection{summarize_xml_results}
Executable, invoked from run-reg-tests.py to produce a brief report based on
the current QMTest results.xml file, and differences from the ``latest''
QMTest results.xml file if one exists. The first parameter is the path to
the current QMTest results.xml file, the second parameter is the path to
the latest QMTeat results.xml file.

\subsubsection{rt-results}
Executable, invoked with 2 parameters: path to the results.xml file (defaults
to results.xml if not given), 
and path to place to put reports (defaults to '.' if not given).
Creates files 
{\tt Summary}, 
{\tt RegtestItem}, 
{\tt listing.html},
and a {\tt .html} file per test.

These files are used to show the latest QMtest results on the 
RT ticket tracking system. 
 The files are not very readable on their own.


\section{Running your own regression test}

Modified from Bill Thies' mail message of  Wed, 14 Sep 2005.
\begin{verbatim}
1. Edit $STREAMIT_HOME/regtest/qmtest/regtest.xml to test the
   directory tree ("test root") and options that you want.

   All benchmarks with a benchmark.xml file under the test root
   specified will be tested.

2. Do the following commands:

   cd $STREAMIT_HOME
   ./regtest/qmtest/streamitqm clean   # remove evidence of previous regtest
   ./regtest/qmtest/streamitqm setup   # create directories and xml for regtest
   setenv QMTEST_CLASS_PATH $STREAMIT_HOME/QMTest  # tell QM where files are
   qmtest run                          # run regression test

This will start a regtest running with your local copy of the source
tree and outputting the results to the screen.  The applications and
compile options being tested are stored under $STREAMIT_HOME/apps.qms.

When done, you can see a summary report similar to that from the
nightly regression test using the commands:
   qmtest -o ./results.xml ./results.qmr 
   ./regtest/qmtest/summarize_xml_results ./results.xml
however, for a substantial test, you will have to wait for a rather
slow XML parser to do its work.  It may be easier to just capture the
results of "qmtest run" to a file.
\end{verbatim}
\end{document}
