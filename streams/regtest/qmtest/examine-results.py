#!/usr/uns/bin/python
#
# examine-results.py: get interesting results from a QMTest results file
# David Maze <dmaze@cag.lcs.mit.edu>
# $Id: examine-results.py,v 1.4 2005-10-05 23:39:25 dimock Exp $

import os
import os.path

# First thing we need to do is set up some magic to find the QMTest
# classes.  Erk.
qm_home = '/usr/uns'
os.environ['QM_HOME'] = qm_home
os.environ['QM_PATH'] = '%s/bin/qmtest' % qm_home
os.environ['QM_BUILD'] = '0'
execfile(os.path.join(qm_home, 'lib/qm/qm', 'setup_path.py'))

# Now, be a normal script from here on in.
import qm.test.base
from   qm.test.result import Result

def main():
    result_file = 'results.qmr'
    if len(sys.argv) > 1: result_file = sys.argv[1]
    current = get_classified_results(result_file)
    print_counts(current)
    if len(sys.argv) > 2:
        latest_file = sys.argv[2]
        latest = get_classified_results(latest_file)
        print_deltas(current, latest)
    detailed_results(current)

def get_classified_results(filename):
    """Read in a QMTest results file, and classify it.

    Reads in a results file, typically named 'results.qmr', as
    generated by a QMTest run.

    'filename' -- Name of the file to read.

    returns -- A mapping from benchmark name to disposition, as
    returned by 'classify_results()'."""

    f = open(filename, 'r')
    results = qm.test.base.load_results(f)
    f.close()
    return classify_results(results)

def classify_results(results):
    """Classify a listing of QMTest results for StreamIt.

    Running a particular StreamIt benchmark can have several different
    outcomes, depending on the target backend and what information is
    available.  On a particular test, compilation can fail
    ('compile-failed'); if it succeeds, and the backend is not the
    Java libary backend, execution can fail as well ('run-failed').
    If both of these pass, a reference output may be missing
    ('not-verified'), or the output may disagree ('verify-failed') or
    agree ('passed') with it.

    'results' -- A list of 'qm.test.result.Result' objects.

    returns -- A mapping from string test name prefix to one of the
    parenthesized strings in the description above."""

    # How can things divide up?  A couple of ways:
    # -- compile failed
    # -- compile succeeded, run failed
    # -- compile succeeded, run succeeded or is absent, verify failed
    # -- compile/run succeeded, verify is absent
    # -- compile/run succeeded, verify succeeded
    #
    # Start by breaking up the list of results by test name.
    resname = {}
    for r in results:
        label = r.GetId()
        parts = label.split('.')
        first = '.'.join(parts[:-1])
        last = parts[-1]
        if first not in resname: resname[first] = {}
        resname[first][last] = r

    # Now go through that list.
    disposition = {}
    for k in resname.keys():
        if resname[k]['compile'].GetOutcome() != Result.PASS:
            thedisp = 'compile-failed'
        elif 'run' in resname[k] and \
             resname[k]['run'].GetOutcome() != Result.PASS:
            thedisp = 'run-failed'
        elif 'verify' not in resname[k]:
            thedisp = 'not-verified'
        elif resname[k]['verify'].GetOutcome() != Result.PASS:
            thedisp = 'verify-failed'
        else:
            thedisp = 'passed'
        disposition[k] = thedisp

    return disposition

def print_counts(disposition):
    """Print a message about the total number of successes and failures."""
    # Get some counts:
    fails = []
    sum = 0
    for k in ['compile-failed', 'run-failed', 'verify-failed']:
        fails.append(len(filter(lambda v: v == k, disposition.values())))
        sum = sum + fails[-1]
    fails.insert(0, sum)

    succeeds = []
    sum = 0
    for k in ['passed', 'not-verified']:
        succeeds.append(len(filter(lambda v: v == k, disposition.values())))
        sum = sum + succeeds[-1]
    succeeds.insert(0, sum)

    print "%4d failures  (%d compile, %d execute, %d verify)" % tuple(fails)
    print "%4d successes (%d passed, %d not verified -- no output to compare to)" % tuple(succeeds)

def detailed_results(disposition):
    """Print detailed results."""

    for (k, t) in \
        [('compile-failed',
          "For the following benchmarks, COMPILATION failed:"),
         ('run-failed', "For the following benchmarks, EXECUTION failed:"),
         ('verify-failed',
          "For the following benchmarks, VERIFICATION failed:"),
         ('not-verified',
          "The following benchmarks executed, but can NOT be VERIFIED because there is no output to compare to:"),
         ('passed', "The following benchmarks PASSED:")]:
        if k in disposition.values():
            print
            print t
            l = [bench for bench, disp in disposition.items() if disp == k]
            l.sort()
            for b in l:
                print "  " + b

def compare_tags(a, b):
    """Compare two regtest status tags.

    returns -- 1 if 'a > b', that is, if a was more successful; -1 if
    b was more successful; 0 if a and b are equal."""

    # Deal deterministically with invalid inputs, for no good reason.
    if a == b: return 0
    if b == 'compile-failed':
        return 1
    if b == 'run-failed':
        if a == 'compile-failed': return -1
        return 1
    if b == 'verify-failed':
        if a == 'compile-failed': return -1
        if a == 'run-failed': return -1
        return 1
    if b == 'not-verified':
        if a == 'compile-failed': return -1
        if a == 'run-failed': return -1
        if a == 'verify-failed': return -1
        return 1
    if b == 'passed':
        return -1

def print_deltas(current, latest):
    """Print changes between two regtest outputs."""

    # Figure out what got better and what got worse.
    born = [k for k in current.keys() if k not in latest]
    died = [k for k in latest.keys() if k not in current]
    aggregate = []
    for k in current.keys():
        if k in latest:
            aggregate.append((k, current[k], latest[k]))
    better = [k for k, c, l in aggregate if compare_tags(c, l) > 0]
    worse = [k for k, c, l in aggregate if compare_tags(c, l) < 0]
    unchanged = [k for k, c, l in aggregate if compare_tags(c, l) == 0]

    # It's possible that nothing at all changed.
    if born == [] and died == [] and better == [] and worse == []:
        print
        print "No changes since last test run."

    if born != []:
        print
        print "NEW tests since last run:"
        born.sort()
        for k in born:
            print "  " + k

    if died != []:
        print
        print "Tests NOT RUN in this run, but in last run:"
        died.sort()
        for k in died:
            print "  " + k

    if better != []:
        print
        print "Tests IMPROVED in this run over last run:"
        better.sort()
        for k in better:
            print "  %s (%s -> %s)" % (k, latest[k], current[k])

    if worse != []:
        print
        print "Tests WORSENED in this run since last run:"
        worse.sort()
        for k in worse:
            print "  %s (%s -> %s)" % (k, latest[k], current[k])

if __name__ == "__main__":
    main()
