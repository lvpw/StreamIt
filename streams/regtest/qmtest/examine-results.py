# OBSOLETE: used internal represntation of QM 2.0.3 results file.
# Neither the internal representation, nor the internal interface to the
#  representation remained stable between versions.
# replaced by sequence of 'qmtest report' and 'summarize_xml_results'
#
#!/usr/uns/bin/python
#
# examine-results.py: get interesting results from a QMTest results file
# David Maze <dmaze@cag.lcs.mit.edu>
# $Id: examine-results.py,v 1.5 2005-10-20 19:41:58 dimock Exp $

import os
import os.path
import sys

#
# Finding qm modules for recent versions of QM:
# (luckily for latest version, was also put in
# /usr/uns/lib/python2.2/site-packages which is on the usual python path)
# 

# #(childin, childout) = os.popen2('which qmtest')
# #qmtest_fullpath = childout.read().replace('\n','')
# #childin.close(); childout.close()     # leaves [which <defunct>] until we exit
# #qmtest_fullpath = os.path.realpath(qmtest_fullpath)
# #  os.environ['QM_PATH'] = qmtest_fullpath
# #qm_home = qmtest_fullpath.replace('/bin/qmtest','')
# #  os.environ['QM_HOME'] = qm_home
# #  os.environ['QM_BUILD'] = '0'
# #  py_ver_2 = sys.version_info[0:2]
# #  python_subdir = 'python' + str(py_ver_2[0]) + '.' + str(py_ver_2[1])
# #  qm_path = os.path.join(qm_home, 'lib', python_subdir, 'site-packages', 'qm')
# #  sys.path.insert(0, qm_path)
# import qm
# qm.prefix = '/home/linux/encap/qm-2.3'

#
# Finding qm modules for early versions of QM (through 2.1? 2.0?)
#
qm_home = '/usr/uns/encap/qm-2.0.3'
os.environ['QM_HOME'] = qm_home
os.environ['QM_PATH'] = '%s/bin/qmtest' % qm_home
os.environ['QM_BUILD'] = '0'
execfile(os.path.join(qm_home, 'lib/qm/qm', 'setup_path.py'))

# Now, be a normal script from here on in.

import qm.test.base
from   qm.test.result import Result

# expects to be given one or two results files as command line arguments.
# if one results file: this is the current results
# if two results files: the second is from the previous regression test, used
#  to produce deltas.
# if no command line arguments, expects processes file ./results.qmr
#
# With more recent versions of QM, needs to pass a test database to the
# qm routines.  The test database is assumed to be the directory
# containing the results file.
#
def main():
    result_file = 'results.qmr'
    if len(sys.argv) > 1: result_file = sys.argv[1]
    current = get_classified_results(result_file)
    print_counts(current)
    if len(sys.argv) > 2:
        latest_file = sys.argv[2]
        latest = get_classified_results(latest_file)
        print_deltas(current, latest)
    detailed_results(current)

def get_classified_results(filename):
    """Read in a QMTest results file, and classify it.

    Reads in a results file, typically named 'results.qmr', as
    generated by a QMTest run.

    'filename' -- Name of the file to read.

    returns -- A mapping from benchmark name to disposition, as
    returned by 'classify_results()'."""

    f = open(filename, 'r')
#   QM 2.0 version
    results = qm.test.base.load_results(f)
#   QM 2.3 version
#    db = os.path.split(os.path.realpath(filename))[0]
#    qm.common.RcConfiguration.Load(qm.common.RcConfiguration(), None)
#    sys.stderr.write("qm.common.RcConfiguration.Load done\n")
#    results = qm.test.base.load_results(f,qm.test.database.load_database(db))
#    sys.stderr.write("qm.test.base.load_results done\n")
    f.close()
    
    return classify_results(results)

def classify_results(results):
    """Classify a listing of QMTest results for StreamIt.

    Running a particular StreamIt benchmark can have several different
    outcomes, depending on the target backend and what information is
    available.  On a particular test, compilation can fail
    ('compile-failed'); if it succeeds, and the backend is not the
    Java libary backend, execution can fail as well ('run-failed').
    If both of these pass, a reference output may be missing
    ('not-verified'), or the output may disagree ('verify-failed') or
    agree ('passed') with it.

    'results' -- A list of 'qm.test.result.Result' objects.

    returns -- A mapping from string test name prefix to one of the
    parenthesized strings in the description above."""

    # How can things divide up?  A couple of ways:
    # -- compile failed
    # -- compile succeeded, run failed
    # -- compile succeeded, run succeeded or is absent, verify failed
    # -- compile/run succeeded, verify is absent
    # -- compile/run succeeded, verify succeeded
    #
    # Start by breaking up the list of results by test name.
    resname = {}
#    sys.stderr.write("about to look at results:\n")
#    sys.stderr.write("results:\n")
#    sys.stderr.write(str(results))
#    sys.stderr.write("\n")
#
# Under QM 2.3 crashes here: Segmentation fault
#
#    sys.stderr.write(str(results.GetResult()))
#    sys.stderr.write("\n")
    for r in results:
        label = r.GetId()
        parts = label.split('.')
        first = '.'.join(parts[:-1])
        last = parts[-1]
        if first not in resname: resname[first] = {}
        resname[first][last] = r

    # Now go through that list.
    disposition = {}
    for k in resname.keys():
        if resname[k]['compile'].GetOutcome() != Result.PASS:
            thedisp = 'compile-failed'
        elif 'run' in resname[k] and \
             resname[k]['run'].GetOutcome() != Result.PASS:
            thedisp = 'run-failed'
        elif 'verify' not in resname[k]:
            thedisp = 'not-verified'
        elif resname[k]['verify'].GetOutcome() != Result.PASS:
            thedisp = 'verify-failed'
        else:
            thedisp = 'passed'
        disposition[k] = thedisp
    return disposition

def print_counts(disposition):
    """Print a message about the total number of successes and failures."""
    # Get some counts:
    fails = []
    sum = 0
    for k in ['compile-failed', 'run-failed', 'verify-failed']:
        fails.append(len(filter(lambda v: v == k, disposition.values())))
        sum = sum + fails[-1]
    fails.insert(0, sum)

    succeeds = []
    sum = 0
    for k in ['passed', 'not-verified']:
        succeeds.append(len(filter(lambda v: v == k, disposition.values())))
        sum = sum + succeeds[-1]
    succeeds.insert(0, sum)

    print "%4d failures  (%d compile, %d execute, %d verify)" % tuple(fails)
    print "%4d successes (%d passed, %d not verified -- no output to compare to)" % tuple(succeeds)

def detailed_results(disposition):
    """Print detailed results."""

    for (k, t) in \
        [('compile-failed',
          "For the following benchmarks, COMPILATION failed:"),
         ('run-failed', "For the following benchmarks, EXECUTION failed:"),
         ('verify-failed',
          "For the following benchmarks, VERIFICATION failed:"),
         ('not-verified',
          "The following benchmarks executed, but can NOT be VERIFIED because there is no output to compare to:"),
         ('passed', "The following benchmarks PASSED:")]:
        if k in disposition.values():
            print
            print t
            l = [bench for bench, disp in disposition.items() if disp == k]
            l.sort()
            for b in l:
                print "  " + b

def compare_tags(a, b):
    """Compare two regtest status tags.

    returns -- 1 if 'a > b', that is, if a was more successful; -1 if
    b was more successful; 0 if a and b are equal."""

    # Deal deterministically with invalid inputs, for no good reason.
    if a == b: return 0
    if b == 'compile-failed':
        return 1
    if b == 'run-failed':
        if a == 'compile-failed': return -1
        return 1
    if b == 'verify-failed':
        if a == 'compile-failed': return -1
        if a == 'run-failed': return -1
        return 1
    if b == 'not-verified':
        if a == 'compile-failed': return -1
        if a == 'run-failed': return -1
        if a == 'verify-failed': return -1
        return 1
    if b == 'passed':
        return -1

def print_deltas(current, latest):
    """Print changes between two regtest outputs."""

    # Figure out what got better and what got worse.
    born = [k for k in current.keys() if k not in latest]
    died = [k for k in latest.keys() if k not in current]
    aggregate = []
    for k in current.keys():
        if k in latest:
            aggregate.append((k, current[k], latest[k]))
    better = [k for k, c, l in aggregate if compare_tags(c, l) > 0]
    worse = [k for k, c, l in aggregate if compare_tags(c, l) < 0]
    unchanged = [k for k, c, l in aggregate if compare_tags(c, l) == 0]

    # It's possible that nothing at all changed.
    if born == [] and died == [] and better == [] and worse == []:
        print
        print "No changes since last test run."

    if born != []:
        print
        print "NEW tests since last run:"
        born.sort()
        for k in born:
            print "  " + k

    if died != []:
        print
        print "Tests NOT RUN in this run, but in last run:"
        died.sort()
        for k in died:
            print "  " + k

    if better != []:
        print
        print "Tests IMPROVED in this run over last run:"
        better.sort()
        for k in better:
            print "  %s (%s -> %s)" % (k, latest[k], current[k])

    if worse != []:
        print
        print "Tests WORSENED in this run since last run:"
        worse.sort()
        for k in worse:
            print "  %s (%s -> %s)" % (k, latest[k], current[k])

if __name__ == "__main__":
    main()
